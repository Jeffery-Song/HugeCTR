2022-12-12 08:04:47.582238: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.589546: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.597060: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.601745: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.607933: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.619711: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.627200: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.639621: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.690348: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.695445: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.698643: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.699638: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.700580: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.701654: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.702761: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.704068: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.705776: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.706718: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.706875: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.708572: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.708619: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.710215: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.710280: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.711643: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.711886: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.713268: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.713575: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.714767: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.715409: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.716189: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.717175: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.718430: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.718448: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.720385: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.721355: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.722566: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.724190: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.724760: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.725546: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.726452: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.727081: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.728022: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.728599: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.729589: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.730635: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.731631: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.732606: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.733661: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.734342: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 08:04:47.738952: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.740418: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.741508: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.742502: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.743582: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.744862: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.745022: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.746951: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.747294: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.747946: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.748736: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.749341: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.750066: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.752089: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.753998: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.756172: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.757235: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.758099: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.765195: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.769683: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.770757: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.771039: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.773060: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.773580: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.773786: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.773985: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.776278: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.777066: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.777115: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.777417: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.779006: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.779112: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.779671: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.779824: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.780395: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.781791: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.782031: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.782442: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.782679: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.783365: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.796683: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.800897: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.801540: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.801760: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.802429: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.803218: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.803368: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.804262: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.805254: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.805393: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.806512: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.806887: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.807114: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.808016: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.808938: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.821374: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.843407: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.843534: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.843974: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.844387: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.845252: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.845602: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.847411: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.847539: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.848159: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.848687: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.849498: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.850129: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.852290: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.853164: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.853307: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.853901: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.855156: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.856658: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.857239: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.858233: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.858468: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.859882: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.860882: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.861382: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.861744: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.863407: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.864098: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.864796: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.865109: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.866512: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.866875: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.867683: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.868083: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.869661: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.869879: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.870734: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.871332: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.872402: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.872519: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.873339: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.874448: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.875359: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.875479: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.876535: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.877608: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.878183: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.878360: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.879284: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.880643: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.881345: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.882101: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.882144: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.883257: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.883837: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.884991: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.885208: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.886143: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.886951: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.887595: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.887635: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.888167: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.889447: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.890365: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.891003: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.891115: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.891854: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.892789: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.893899: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.894861: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.895479: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.895561: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.896718: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.897181: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.898048: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.898656: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.898833: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.900272: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.900593: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.900858: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 08:04:47.901379: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.901471: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.902251: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.902350: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.903830: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.904127: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.905394: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.905569: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.906235: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.906454: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.907884: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.908328: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.909666: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.910033: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.910316: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.911391: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.911710: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.912108: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.913489: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.914134: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.914157: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.914311: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 08:04:47.915488: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.915618: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.915746: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.917390: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.918479: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.919547: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.919608: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.920108: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.921274: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.921949: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 08:04:47.922231: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.923459: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.923734: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.924373: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.925028: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.927489: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.927851: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.928685: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.929353: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.930014: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 08:04:47.931646: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.931942: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.932408: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.932753: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.933625: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.935952: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.936299: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.936864: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.937778: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.939979: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.940322: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.940350: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.940925: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.942344: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.944583: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.944905: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.945080: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.947012: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.949404: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.950102: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.951166: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.951740: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.954242: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.955965: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.956588: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.958911: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.960587: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.961208: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.964144: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.994427: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:47.994836: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:48.010575: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:48.011349: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:48.012908: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:48.015154: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:48.016232: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:48.017895: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:48.047843: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:48.048441: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:48.051536: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:48.065415: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:48.067387: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:48.071390: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:48.073859: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:48.076662: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:48.078261: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 08:04:48.088917: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:48.091622: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:48.124133: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:48.124952: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:48.126656: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:48.129974: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:48.132771: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:48.167220: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:48.167798: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 08:04:48.176361: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 08:04:48.178310: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:48.187015: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:48.211166: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:48.211283: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:48.217591: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:48.217766: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:49.176334: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:49.178119: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:49.178660: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:49.180094: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 08:04:49.180159: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 08:04:49.198286: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:49.198917: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:49.199448: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:49.200036: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:49.200965: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:49.201570: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 08:04:49.245179: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 08:04:49.245387: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 08:04:49.287809: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-12 08:04:49.369794: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:49.370627: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:49.371178: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:49.371639: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 08:04:49.371696: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 08:04:49.389762: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:49.390422: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:49.390937: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:49.391553: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:49.392116: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:49.392596: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 08:04:49.416469: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:49.417256: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:49.417803: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:49.418270: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 08:04:49.418326: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 08:04:49.419195: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:49.419849: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:49.420386: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:49.420845: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 08:04:49.420898: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 08:04:49.430415: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:49.431048: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:49.431806: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:49.432300: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 08:04:49.432357: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 08:04:49.435979: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:49.436596: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:49.437110: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:49.437671: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:49.438190: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:49.438654: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 08:04:49.439272: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:49.439850: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:49.440361: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:49.441148: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:49.441667: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:49.442138: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 08:04:49.450258: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:49.450925: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:49.452056: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:49.452677: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:49.453201: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:49.453663: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 08:04:49.468996: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 08:04:49.469200: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 08:04:49.471440: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-12 08:04:49.473591: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:49.474234: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:49.475195: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:49.475809: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 08:04:49.475864: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 08:04:49.480369: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 08:04:49.480540: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 08:04:49.482446: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
2022-12-12 08:04:49.493360: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:49.493967: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:49.494228: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:49.494514: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 08:04:49.494674: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 08:04:49.494721: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:49.495372: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:49.495635: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 08:04:49.495692: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 08:04:49.496324: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:49.496924: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:49.496991: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
2022-12-12 08:04:49.497444: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:49.497917: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 08:04:49.505088: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:49.505758: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:49.506308: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:49.506770: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 08:04:49.506821: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 08:04:49.514998: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:49.515631: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:49.516145: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:49.516707: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:49.517225: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:49.517689: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 08:04:49.523272: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 08:04:49.523433: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 08:04:49.524163: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:49.524805: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:49.525260: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
2022-12-12 08:04:49.525321: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:49.525902: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:49.526414: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 08:04:49.526877: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 08:04:49.538515: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 08:04:49.538685: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 08:04:49.540569: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
2022-12-12 08:04:49.559013: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 08:04:49.559201: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 08:04:49.560157: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
2022-12-12 08:04:49.567046: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 08:04:49.567250: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 08:04:49.568977: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
[HCTR][08:04:50.825][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][08:04:50.825][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][08:04:50.825][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][08:04:50.826][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][08:04:50.826][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][08:04:50.826][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][08:04:50.854][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][08:04:50.854][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:01,  1.56s/it]warmup run: 1it [00:01,  1.53s/it]warmup run: 1it [00:01,  1.55s/it]warmup run: 98it [00:01, 81.90it/s]warmup run: 98it [00:01, 83.38it/s]warmup run: 99it [00:01, 83.38it/s]warmup run: 1it [00:01,  1.54s/it]warmup run: 197it [00:01, 179.01it/s]warmup run: 196it [00:01, 180.72it/s]warmup run: 1it [00:01,  1.50s/it]warmup run: 1it [00:01,  1.48s/it]warmup run: 198it [00:01, 181.01it/s]warmup run: 95it [00:01, 80.47it/s]warmup run: 296it [00:01, 286.33it/s]warmup run: 1it [00:01,  1.49s/it]warmup run: 295it [00:01, 289.10it/s]warmup run: 1it [00:01,  1.50s/it]warmup run: 99it [00:01, 85.87it/s]warmup run: 99it [00:01, 86.72it/s]warmup run: 296it [00:01, 287.28it/s]warmup run: 190it [00:01, 174.45it/s]warmup run: 396it [00:01, 399.42it/s]warmup run: 95it [00:01, 82.79it/s]warmup run: 393it [00:01, 399.70it/s]warmup run: 98it [00:01, 85.16it/s]warmup run: 199it [00:01, 186.82it/s]warmup run: 198it [00:01, 187.46it/s]warmup run: 395it [00:01, 399.53it/s]warmup run: 285it [00:01, 278.22it/s]warmup run: 496it [00:02, 509.62it/s]warmup run: 195it [00:01, 184.60it/s]warmup run: 491it [00:02, 507.40it/s]warmup run: 197it [00:01, 185.23it/s]warmup run: 299it [00:01, 297.49it/s]warmup run: 297it [00:01, 297.65it/s]warmup run: 495it [00:02, 510.91it/s]warmup run: 380it [00:01, 385.57it/s]warmup run: 597it [00:02, 613.55it/s]warmup run: 295it [00:01, 296.15it/s]warmup run: 590it [00:02, 607.83it/s]warmup run: 295it [00:01, 293.58it/s]warmup run: 400it [00:01, 412.97it/s]warmup run: 396it [00:01, 410.69it/s]warmup run: 597it [00:02, 617.04it/s]warmup run: 473it [00:02, 486.95it/s]warmup run: 697it [00:02, 701.92it/s]warmup run: 395it [00:01, 410.90it/s]warmup run: 688it [00:02, 693.30it/s]warmup run: 394it [00:01, 406.64it/s]warmup run: 500it [00:02, 523.58it/s]warmup run: 495it [00:01, 520.19it/s]warmup run: 699it [00:02, 708.40it/s]warmup run: 568it [00:02, 584.13it/s]warmup run: 798it [00:02, 776.03it/s]warmup run: 495it [00:01, 521.73it/s]warmup run: 784it [00:02, 754.15it/s]warmup run: 493it [00:01, 516.28it/s]warmup run: 601it [00:02, 626.72it/s]warmup run: 595it [00:02, 621.71it/s]warmup run: 800it [00:02, 781.13it/s]warmup run: 664it [00:02, 669.54it/s]warmup run: 898it [00:02, 832.36it/s]warmup run: 596it [00:02, 624.51it/s]warmup run: 879it [00:02, 802.92it/s]warmup run: 595it [00:02, 621.93it/s]warmup run: 703it [00:02, 716.63it/s]warmup run: 695it [00:02, 708.56it/s]warmup run: 901it [00:02, 840.25it/s]warmup run: 759it [00:02, 738.87it/s]warmup run: 999it [00:02, 879.83it/s]warmup run: 696it [00:02, 710.34it/s]warmup run: 975it [00:02, 843.26it/s]warmup run: 695it [00:02, 709.47it/s]warmup run: 804it [00:02, 788.72it/s]warmup run: 795it [00:02, 780.02it/s]warmup run: 1002it [00:02, 884.36it/s]warmup run: 854it [00:02, 792.75it/s]warmup run: 1100it [00:02, 915.65it/s]warmup run: 795it [00:02, 779.39it/s]warmup run: 1072it [00:02, 877.56it/s]warmup run: 796it [00:02, 783.36it/s]warmup run: 905it [00:02, 845.17it/s]warmup run: 894it [00:02, 834.17it/s]warmup run: 1103it [00:02, 917.61it/s]warmup run: 949it [00:02, 835.25it/s]warmup run: 1201it [00:02, 941.71it/s]warmup run: 896it [00:02, 839.49it/s]warmup run: 1169it [00:02, 902.34it/s]warmup run: 897it [00:02, 841.71it/s]warmup run: 1006it [00:02, 888.21it/s]warmup run: 996it [00:02, 882.86it/s]warmup run: 1205it [00:02, 945.00it/s]warmup run: 1046it [00:02, 871.21it/s]warmup run: 1301it [00:02, 958.29it/s]warmup run: 997it [00:02, 885.41it/s]warmup run: 1266it [00:02, 919.99it/s]warmup run: 997it [00:02, 883.72it/s]warmup run: 1107it [00:02, 920.98it/s]warmup run: 1098it [00:02, 918.87it/s]warmup run: 1306it [00:02, 953.52it/s]warmup run: 1143it [00:02, 897.88it/s]warmup run: 1402it [00:02, 971.94it/s]warmup run: 1097it [00:02, 915.07it/s]warmup run: 1365it [00:02, 937.54it/s]warmup run: 1097it [00:02, 914.80it/s]warmup run: 1207it [00:02, 942.83it/s]warmup run: 1199it [00:02, 943.61it/s]warmup run: 1407it [00:02, 969.58it/s]warmup run: 1242it [00:02, 922.52it/s]warmup run: 1503it [00:03, 978.50it/s]warmup run: 1198it [00:02, 941.66it/s]warmup run: 1463it [00:03, 947.68it/s]warmup run: 1197it [00:02, 936.71it/s]warmup run: 1307it [00:02, 948.61it/s]warmup run: 1299it [00:02, 958.55it/s]warmup run: 1507it [00:03, 976.74it/s]warmup run: 1343it [00:02, 947.29it/s]warmup run: 1604it [00:03, 986.60it/s]warmup run: 1298it [00:02, 958.03it/s]warmup run: 1560it [00:03, 953.79it/s]warmup run: 1297it [00:02, 952.33it/s]warmup run: 1406it [00:02, 954.70it/s]warmup run: 1400it [00:02, 971.14it/s]warmup run: 1607it [00:03, 978.99it/s]warmup run: 1443it [00:03, 962.47it/s]warmup run: 1705it [00:03, 989.62it/s]warmup run: 1399it [00:02, 972.84it/s]warmup run: 1657it [00:03, 954.17it/s]warmup run: 1398it [00:02, 967.62it/s]warmup run: 1505it [00:03, 957.94it/s]warmup run: 1501it [00:02, 981.55it/s]warmup run: 1707it [00:03, 981.93it/s]warmup run: 1544it [00:03, 975.07it/s]warmup run: 1806it [00:03, 993.05it/s]warmup run: 1500it [00:02, 983.06it/s]warmup run: 1498it [00:03, 976.44it/s]warmup run: 1754it [00:03, 955.14it/s]warmup run: 1603it [00:03, 962.47it/s]warmup run: 1603it [00:03, 991.12it/s]warmup run: 1807it [00:03, 986.66it/s]warmup run: 1644it [00:03, 981.64it/s]warmup run: 1907it [00:03, 994.85it/s]warmup run: 1601it [00:03, 988.32it/s]warmup run: 1598it [00:03, 982.05it/s]warmup run: 1851it [00:03, 950.91it/s]warmup run: 1701it [00:03, 963.34it/s]warmup run: 1704it [00:03, 995.83it/s]warmup run: 1907it [00:03, 989.25it/s]warmup run: 1745it [00:03, 989.41it/s]warmup run: 2010it [00:03, 1003.08it/s]warmup run: 1702it [00:03, 991.58it/s]warmup run: 1698it [00:03, 986.02it/s]warmup run: 1949it [00:03, 957.80it/s]warmup run: 1802it [00:03, 975.24it/s]warmup run: 1806it [00:03, 1001.43it/s]warmup run: 2008it [00:03, 993.91it/s]warmup run: 1845it [00:03, 992.54it/s]warmup run: 2131it [00:03, 1062.78it/s]warmup run: 1803it [00:03, 988.58it/s]warmup run: 1799it [00:03, 992.06it/s]warmup run: 2056it [00:03, 990.00it/s]warmup run: 1902it [00:03, 980.75it/s]warmup run: 1908it [00:03, 1005.80it/s]warmup run: 2124it [00:03, 1042.40it/s]warmup run: 1946it [00:03, 997.61it/s]warmup run: 2253it [00:03, 1107.16it/s]warmup run: 1903it [00:03, 991.56it/s]warmup run: 1900it [00:03, 994.87it/s]warmup run: 2176it [00:03, 1050.68it/s]warmup run: 2002it [00:03, 983.50it/s]warmup run: 2013it [00:03, 1016.70it/s]warmup run: 2232it [00:03, 1051.03it/s]warmup run: 2054it [00:03, 1019.67it/s]warmup run: 2375it [00:03, 1138.47it/s]warmup run: 2004it [00:03, 996.09it/s]warmup run: 2000it [00:03, 995.98it/s]warmup run: 2296it [00:03, 1094.32it/s]warmup run: 2122it [00:03, 1045.48it/s]warmup run: 2135it [00:03, 1075.50it/s]warmup run: 2172it [00:03, 1066.14it/s]warmup run: 2338it [00:03, 1021.79it/s]warmup run: 2497it [00:03, 1162.14it/s]warmup run: 2123it [00:03, 1052.99it/s]warmup run: 2120it [00:03, 1055.88it/s]warmup run: 2416it [00:03, 1125.68it/s]warmup run: 2242it [00:03, 1090.43it/s]warmup run: 2257it [00:03, 1116.41it/s]warmup run: 2290it [00:03, 1099.31it/s]warmup run: 2457it [00:03, 1068.44it/s]warmup run: 2619it [00:04, 1178.91it/s]warmup run: 2242it [00:03, 1093.79it/s]warmup run: 2241it [00:03, 1099.78it/s]warmup run: 2536it [00:04, 1147.41it/s]warmup run: 2362it [00:03, 1122.46it/s]warmup run: 2379it [00:03, 1144.61it/s]warmup run: 2408it [00:03, 1122.01it/s]warmup run: 2576it [00:04, 1101.93it/s]warmup run: 2742it [00:04, 1191.27it/s]warmup run: 2361it [00:03, 1121.80it/s]warmup run: 2362it [00:03, 1131.27it/s]warmup run: 2657it [00:04, 1163.71it/s]warmup run: 2483it [00:03, 1145.71it/s]warmup run: 2500it [00:03, 1161.90it/s]warmup run: 2526it [00:04, 1137.55it/s]warmup run: 2697it [00:04, 1133.50it/s]warmup run: 2863it [00:04, 1195.27it/s]warmup run: 2480it [00:03, 1142.08it/s]warmup run: 2483it [00:03, 1152.64it/s]warmup run: 2776it [00:04, 1171.07it/s]warmup run: 2603it [00:04, 1160.64it/s]warmup run: 2620it [00:03, 1172.92it/s]warmup run: 2644it [00:04, 1149.15it/s]warmup run: 2817it [00:04, 1151.87it/s]warmup run: 2985it [00:04, 1201.63it/s]warmup run: 2599it [00:04, 1156.20it/s]warmup run: 2603it [00:04, 1166.03it/s]warmup run: 3000it [00:04, 682.43it/s] warmup run: 2896it [00:04, 1179.41it/s]warmup run: 2723it [00:04, 1171.58it/s]warmup run: 2740it [00:04, 1179.49it/s]warmup run: 2762it [00:04, 1155.50it/s]warmup run: 2938it [00:04, 1168.32it/s]warmup run: 2718it [00:04, 1165.25it/s]warmup run: 3000it [00:04, 674.87it/s] warmup run: 2723it [00:04, 1175.87it/s]warmup run: 2843it [00:04, 1179.77it/s]warmup run: 3000it [00:04, 675.65it/s] warmup run: 2858it [00:04, 1177.97it/s]warmup run: 2881it [00:04, 1163.29it/s]warmup run: 2836it [00:04, 1167.42it/s]warmup run: 2842it [00:04, 1178.98it/s]warmup run: 2965it [00:04, 1190.51it/s]warmup run: 3000it [00:04, 688.67it/s] warmup run: 2976it [00:04, 1175.36it/s]warmup run: 2998it [00:04, 1158.64it/s]warmup run: 2955it [00:04, 1174.10it/s]warmup run: 3000it [00:04, 672.56it/s] warmup run: 3000it [00:04, 693.68it/s] warmup run: 2961it [00:04, 1181.75it/s]warmup run: 3000it [00:04, 690.03it/s] warmup run: 3000it [00:04, 689.76it/s] 



warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   5%|         | 164/3000 [00:00<00:01, 1635.40it/s]warmup should be done:   5%|         | 163/3000 [00:00<00:01, 1626.54it/s]warmup should be done:   5%|         | 154/3000 [00:00<00:01, 1536.46it/s]warmup should be done:   5%|         | 162/3000 [00:00<00:01, 1616.10it/s]warmup should be done:   6%|         | 165/3000 [00:00<00:01, 1644.26it/s]warmup should be done:   5%|         | 153/3000 [00:00<00:01, 1525.83it/s]warmup should be done:   5%|         | 162/3000 [00:00<00:01, 1610.63it/s]warmup should be done:   4%|         | 109/3000 [00:00<00:02, 1083.47it/s]warmup should be done:  11%|         | 326/3000 [00:00<00:01, 1628.75it/s]warmup should be done:  11%|         | 329/3000 [00:00<00:01, 1639.79it/s]warmup should be done:  11%|         | 317/3000 [00:00<00:01, 1590.90it/s]warmup should be done:  11%|         | 327/3000 [00:00<00:01, 1629.05it/s]warmup should be done:  11%|         | 332/3000 [00:00<00:01, 1654.59it/s]warmup should be done:  10%|         | 308/3000 [00:00<00:01, 1531.56it/s]warmup should be done:   8%|         | 253/3000 [00:00<00:02, 1289.62it/s]warmup should be done:  11%|         | 324/3000 [00:00<00:01, 1604.00it/s]warmup should be done:  16%|        | 489/3000 [00:00<00:01, 1627.81it/s]warmup should be done:  16%|        | 480/3000 [00:00<00:01, 1607.54it/s]warmup should be done:  16%|        | 493/3000 [00:00<00:01, 1634.48it/s]warmup should be done:  17%|        | 498/3000 [00:00<00:01, 1652.77it/s]warmup should be done:  14%|        | 417/3000 [00:00<00:01, 1445.15it/s]warmup should be done:  15%|        | 462/3000 [00:00<00:01, 1525.27it/s]warmup should be done:  16%|        | 490/3000 [00:00<00:01, 1601.80it/s]warmup should be done:  16%|        | 485/3000 [00:00<00:01, 1570.27it/s]warmup should be done:  22%|       | 652/3000 [00:00<00:01, 1627.76it/s]warmup should be done:  21%|       | 643/3000 [00:00<00:01, 1615.92it/s]warmup should be done:  22%|       | 657/3000 [00:00<00:01, 1633.80it/s]warmup should be done:  19%|        | 578/3000 [00:00<00:01, 1508.87it/s]warmup should be done:  22%|       | 664/3000 [00:00<00:01, 1651.67it/s]warmup should be done:  20%|        | 615/3000 [00:00<00:01, 1519.68it/s]warmup should be done:  22%|       | 651/3000 [00:00<00:01, 1602.76it/s]warmup should be done:  21%|       | 643/3000 [00:00<00:01, 1571.30it/s]warmup should be done:  27%|       | 806/3000 [00:00<00:01, 1619.02it/s]warmup should be done:  27%|       | 815/3000 [00:00<00:01, 1621.71it/s]warmup should be done:  27%|       | 821/3000 [00:00<00:01, 1632.79it/s]warmup should be done:  25%|       | 742/3000 [00:00<00:01, 1552.81it/s]warmup should be done:  28%|       | 830/3000 [00:00<00:01, 1649.78it/s]warmup should be done:  26%|       | 767/3000 [00:00<00:01, 1518.81it/s]warmup should be done:  27%|       | 812/3000 [00:00<00:01, 1602.86it/s]warmup should be done:  27%|       | 806/3000 [00:00<00:01, 1591.49it/s]warmup should be done:  32%|      | 969/3000 [00:00<00:01, 1619.76it/s]warmup should be done:  33%|      | 985/3000 [00:00<00:01, 1630.61it/s]warmup should be done:  30%|       | 906/3000 [00:00<00:01, 1579.36it/s]warmup should be done:  33%|      | 995/3000 [00:00<00:01, 1647.17it/s]warmup should be done:  31%|       | 919/3000 [00:00<00:01, 1515.61it/s]warmup should be done:  33%|      | 978/3000 [00:00<00:01, 1608.99it/s]warmup should be done:  32%|      | 973/3000 [00:00<00:01, 1592.02it/s]warmup should be done:  32%|      | 972/3000 [00:00<00:01, 1611.62it/s]warmup should be done:  38%|      | 1131/3000 [00:00<00:01, 1617.67it/s]warmup should be done:  36%|      | 1069/3000 [00:00<00:01, 1593.22it/s]warmup should be done:  38%|      | 1149/3000 [00:00<00:01, 1626.32it/s]warmup should be done:  38%|      | 1139/3000 [00:00<00:01, 1609.02it/s]warmup should be done:  39%|      | 1160/3000 [00:00<00:01, 1635.86it/s]warmup should be done:  36%|      | 1071/3000 [00:00<00:01, 1510.63it/s]warmup should be done:  38%|      | 1134/3000 [00:00<00:01, 1595.08it/s]warmup should be done:  38%|      | 1137/3000 [00:00<00:01, 1621.31it/s]warmup should be done:  43%|     | 1293/3000 [00:00<00:01, 1618.12it/s]warmup should be done:  41%|      | 1233/3000 [00:00<00:01, 1606.45it/s]warmup should be done:  44%|     | 1312/3000 [00:00<00:01, 1625.48it/s]warmup should be done:  43%|     | 1302/3000 [00:00<00:01, 1612.99it/s]warmup should be done:  44%|     | 1324/3000 [00:00<00:01, 1634.48it/s]warmup should be done:  41%|      | 1223/3000 [00:00<00:01, 1510.40it/s]warmup should be done:  43%|     | 1295/3000 [00:00<00:01, 1598.76it/s]warmup should be done:  43%|     | 1303/3000 [00:00<00:01, 1630.50it/s]warmup should be done:  49%|     | 1456/3000 [00:00<00:00, 1619.12it/s]warmup should be done:  47%|     | 1397/3000 [00:00<00:00, 1614.73it/s]warmup should be done:  49%|     | 1475/3000 [00:00<00:00, 1625.10it/s]warmup should be done:  50%|     | 1489/3000 [00:00<00:00, 1636.18it/s]warmup should be done:  46%|     | 1375/3000 [00:00<00:01, 1510.76it/s]warmup should be done:  49%|     | 1457/3000 [00:00<00:00, 1602.29it/s]warmup should be done:  49%|     | 1469/3000 [00:00<00:00, 1637.26it/s]warmup should be done:  49%|     | 1464/3000 [00:00<00:00, 1586.73it/s]warmup should be done:  54%|    | 1618/3000 [00:01<00:00, 1619.19it/s]warmup should be done:  52%|    | 1560/3000 [00:01<00:00, 1616.87it/s]warmup should be done:  55%|    | 1638/3000 [00:01<00:00, 1622.70it/s]warmup should be done:  55%|    | 1653/3000 [00:01<00:00, 1632.45it/s]warmup should be done:  54%|    | 1618/3000 [00:01<00:00, 1604.42it/s]warmup should be done:  55%|    | 1635/3000 [00:01<00:00, 1642.01it/s]warmup should be done:  51%|     | 1527/3000 [00:01<00:00, 1496.20it/s]warmup should be done:  54%|    | 1625/3000 [00:01<00:00, 1592.93it/s]warmup should be done:  59%|    | 1780/3000 [00:01<00:00, 1619.09it/s]warmup should be done:  57%|    | 1723/3000 [00:01<00:00, 1618.13it/s]warmup should be done:  60%|    | 1801/3000 [00:01<00:00, 1622.22it/s]warmup should be done:  61%|    | 1817/3000 [00:01<00:00, 1627.32it/s]warmup should be done:  59%|    | 1779/3000 [00:01<00:00, 1605.41it/s]warmup should be done:  56%|    | 1691/3000 [00:01<00:00, 1537.40it/s]warmup should be done:  60%|    | 1786/3000 [00:01<00:00, 1597.62it/s]warmup should be done:  60%|    | 1800/3000 [00:01<00:00, 1625.11it/s]warmup should be done:  65%|   | 1942/3000 [00:01<00:00, 1618.79it/s]warmup should be done:  63%|   | 1885/3000 [00:01<00:00, 1613.49it/s]warmup should be done:  65%|   | 1964/3000 [00:01<00:00, 1620.87it/s]warmup should be done:  65%|   | 1941/3000 [00:01<00:00, 1607.30it/s]warmup should be done:  66%|   | 1980/3000 [00:01<00:00, 1624.19it/s]warmup should be done:  62%|   | 1856/3000 [00:01<00:00, 1569.57it/s]warmup should be done:  65%|   | 1946/3000 [00:01<00:00, 1595.59it/s]warmup should be done:  66%|   | 1965/3000 [00:01<00:00, 1632.52it/s]warmup should be done:  70%|   | 2104/3000 [00:01<00:00, 1618.70it/s]warmup should be done:  71%|   | 2127/3000 [00:01<00:00, 1619.32it/s]warmup should be done:  68%|   | 2047/3000 [00:01<00:00, 1606.48it/s]warmup should be done:  70%|   | 2102/3000 [00:01<00:00, 1606.31it/s]warmup should be done:  71%|  | 2143/3000 [00:01<00:00, 1621.02it/s]warmup should be done:  70%|   | 2107/3000 [00:01<00:00, 1599.23it/s]warmup should be done:  67%|   | 2021/3000 [00:01<00:00, 1591.73it/s]warmup should be done:  71%|   | 2129/3000 [00:01<00:00, 1633.71it/s]warmup should be done:  76%|  | 2267/3000 [00:01<00:00, 1619.49it/s]warmup should be done:  76%|  | 2289/3000 [00:01<00:00, 1618.12it/s]warmup should be done:  74%|  | 2208/3000 [00:01<00:00, 1601.88it/s]warmup should be done:  75%|  | 2264/3000 [00:01<00:00, 1607.60it/s]warmup should be done:  76%|  | 2268/3000 [00:01<00:00, 1601.33it/s]warmup should be done:  73%|  | 2186/3000 [00:01<00:00, 1607.47it/s]warmup should be done:  77%|  | 2306/3000 [00:01<00:00, 1615.46it/s]warmup should be done:  76%|  | 2294/3000 [00:01<00:00, 1638.56it/s]warmup should be done:  81%|  | 2429/3000 [00:01<00:00, 1618.13it/s]warmup should be done:  82%| | 2451/3000 [00:01<00:00, 1617.01it/s]warmup should be done:  79%|  | 2369/3000 [00:01<00:00, 1596.29it/s]warmup should be done:  81%|  | 2425/3000 [00:01<00:00, 1605.97it/s]warmup should be done:  81%|  | 2429/3000 [00:01<00:00, 1601.90it/s]warmup should be done:  78%|  | 2350/3000 [00:01<00:00, 1614.72it/s]warmup should be done:  82%| | 2468/3000 [00:01<00:00, 1610.02it/s]warmup should be done:  82%| | 2458/3000 [00:01<00:00, 1637.84it/s]warmup should be done:  86%| | 2592/3000 [00:01<00:00, 1618.90it/s]warmup should be done:  87%| | 2614/3000 [00:01<00:00, 1617.88it/s]warmup should be done:  86%| | 2586/3000 [00:01<00:00, 1606.16it/s]warmup should be done:  84%| | 2529/3000 [00:01<00:00, 1594.60it/s]warmup should be done:  86%| | 2590/3000 [00:01<00:00, 1603.38it/s]warmup should be done:  84%| | 2515/3000 [00:01<00:00, 1623.72it/s]warmup should be done:  87%| | 2624/3000 [00:01<00:00, 1642.01it/s]warmup should be done:  88%| | 2630/3000 [00:01<00:00, 1600.94it/s]warmup should be done:  92%|| 2755/3000 [00:01<00:00, 1620.30it/s]warmup should be done:  93%|| 2776/3000 [00:01<00:00, 1617.66it/s]warmup should be done:  92%|| 2747/3000 [00:01<00:00, 1606.08it/s]warmup should be done:  90%| | 2689/3000 [00:01<00:00, 1593.94it/s]warmup should be done:  92%|| 2751/3000 [00:01<00:00, 1604.66it/s]warmup should be done:  89%| | 2680/3000 [00:01<00:00, 1630.12it/s]warmup should be done:  93%|| 2790/3000 [00:01<00:00, 1644.84it/s]warmup should be done:  93%|| 2795/3000 [00:01<00:00, 1612.82it/s]warmup should be done:  97%|| 2918/3000 [00:01<00:00, 1622.60it/s]warmup should be done:  98%|| 2938/3000 [00:01<00:00, 1609.86it/s]warmup should be done:  97%|| 2910/3000 [00:01<00:00, 1611.23it/s]warmup should be done:  95%|| 2849/3000 [00:01<00:00, 1595.72it/s]warmup should be done:  97%|| 2915/3000 [00:01<00:00, 1615.00it/s]warmup should be done:  95%|| 2846/3000 [00:01<00:00, 1636.93it/s]warmup should be done:  99%|| 2957/3000 [00:01<00:00, 1649.56it/s]warmup should be done:  99%|| 2961/3000 [00:01<00:00, 1625.74it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1629.00it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1628.13it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1620.96it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1616.68it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1608.54it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1605.85it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1573.83it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1572.75it/s]






warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|         | 168/3000 [00:00<00:01, 1679.27it/s]warmup should be done:   6%|         | 169/3000 [00:00<00:01, 1686.70it/s]warmup should be done:   6%|         | 165/3000 [00:00<00:01, 1648.53it/s]warmup should be done:   5%|         | 164/3000 [00:00<00:01, 1637.23it/s]warmup should be done:   5%|         | 161/3000 [00:00<00:01, 1606.90it/s]warmup should be done:   6%|         | 165/3000 [00:00<00:01, 1646.67it/s]warmup should be done:   6%|         | 169/3000 [00:00<00:01, 1682.41it/s]warmup should be done:   6%|         | 167/3000 [00:00<00:01, 1661.63it/s]warmup should be done:  11%|         | 336/3000 [00:00<00:01, 1677.21it/s]warmup should be done:  11%|         | 334/3000 [00:00<00:01, 1671.46it/s]warmup should be done:  11%|         | 324/3000 [00:00<00:01, 1616.39it/s]warmup should be done:  11%|        | 338/3000 [00:00<00:01, 1682.49it/s]warmup should be done:  11%|         | 334/3000 [00:00<00:01, 1661.52it/s]warmup should be done:  11%|         | 328/3000 [00:00<00:01, 1629.77it/s]warmup should be done:  11%|         | 330/3000 [00:00<00:01, 1635.43it/s]warmup should be done:  11%|        | 338/3000 [00:00<00:01, 1673.30it/s]warmup should be done:  17%|        | 504/3000 [00:00<00:01, 1681.02it/s]warmup should be done:  17%|        | 505/3000 [00:00<00:01, 1678.79it/s]warmup should be done:  16%|        | 487/3000 [00:00<00:01, 1620.62it/s]warmup should be done:  17%|        | 507/3000 [00:00<00:01, 1684.02it/s]warmup should be done:  16%|        | 492/3000 [00:00<00:01, 1633.69it/s]warmup should be done:  17%|        | 501/3000 [00:00<00:01, 1662.24it/s]warmup should be done:  16%|        | 495/3000 [00:00<00:01, 1642.00it/s]warmup should be done:  17%|        | 506/3000 [00:00<00:01, 1643.92it/s]warmup should be done:  22%|       | 674/3000 [00:00<00:01, 1681.88it/s]warmup should be done:  22%|       | 675/3000 [00:00<00:01, 1689.00it/s]warmup should be done:  22%|       | 650/3000 [00:00<00:01, 1620.08it/s]warmup should be done:  23%|       | 677/3000 [00:00<00:01, 1687.47it/s]warmup should be done:  22%|       | 657/3000 [00:00<00:01, 1637.52it/s]warmup should be done:  22%|       | 661/3000 [00:00<00:01, 1648.31it/s]warmup should be done:  22%|       | 669/3000 [00:00<00:01, 1665.69it/s]warmup should be done:  22%|       | 672/3000 [00:00<00:01, 1648.58it/s]warmup should be done:  28%|       | 843/3000 [00:00<00:01, 1684.40it/s]warmup should be done:  28%|       | 846/3000 [00:00<00:01, 1693.70it/s]warmup should be done:  28%|       | 847/3000 [00:00<00:01, 1690.56it/s]warmup should be done:  27%|       | 813/3000 [00:00<00:01, 1622.22it/s]warmup should be done:  28%|       | 827/3000 [00:00<00:01, 1652.04it/s]warmup should be done:  27%|       | 822/3000 [00:00<00:01, 1639.86it/s]warmup should be done:  28%|       | 837/3000 [00:00<00:01, 1667.97it/s]warmup should be done:  28%|       | 839/3000 [00:00<00:01, 1653.29it/s]warmup should be done:  34%|      | 1012/3000 [00:00<00:01, 1682.50it/s]warmup should be done:  33%|      | 976/3000 [00:00<00:01, 1624.60it/s]warmup should be done:  33%|      | 993/3000 [00:00<00:01, 1652.24it/s]warmup should be done:  33%|      | 986/3000 [00:00<00:01, 1637.34it/s]warmup should be done:  33%|      | 1004/3000 [00:00<00:01, 1667.13it/s]warmup should be done:  34%|      | 1017/3000 [00:00<00:01, 1688.48it/s]warmup should be done:  34%|      | 1016/3000 [00:00<00:01, 1671.24it/s]warmup should be done:  34%|      | 1005/3000 [00:00<00:01, 1635.79it/s]warmup should be done:  38%|      | 1139/3000 [00:00<00:01, 1624.40it/s]warmup should be done:  38%|      | 1152/3000 [00:00<00:01, 1641.79it/s]warmup should be done:  40%|      | 1186/3000 [00:00<00:01, 1685.53it/s]warmup should be done:  39%|      | 1171/3000 [00:00<00:01, 1664.61it/s]warmup should be done:  39%|      | 1159/3000 [00:00<00:01, 1648.36it/s]warmup should be done:  39%|      | 1181/3000 [00:00<00:01, 1672.96it/s]warmup should be done:  40%|      | 1185/3000 [00:00<00:01, 1676.22it/s]warmup should be done:  39%|      | 1169/3000 [00:00<00:01, 1636.91it/s]warmup should be done:  43%|     | 1302/3000 [00:00<00:01, 1620.06it/s]warmup should be done:  45%|     | 1338/3000 [00:00<00:00, 1665.84it/s]warmup should be done:  44%|     | 1317/3000 [00:00<00:01, 1641.71it/s]warmup should be done:  45%|     | 1356/3000 [00:00<00:00, 1688.28it/s]warmup should be done:  44%|     | 1325/3000 [00:00<00:01, 1649.13it/s]warmup should be done:  45%|     | 1349/3000 [00:00<00:00, 1668.75it/s]warmup should be done:  45%|     | 1356/3000 [00:00<00:00, 1684.92it/s]warmup should be done:  44%|     | 1335/3000 [00:00<00:01, 1641.86it/s]warmup should be done:  49%|     | 1466/3000 [00:00<00:00, 1626.18it/s]warmup should be done:  49%|     | 1483/3000 [00:00<00:00, 1647.26it/s]warmup should be done:  50%|     | 1505/3000 [00:00<00:00, 1666.69it/s]warmup should be done:  51%|     | 1526/3000 [00:00<00:00, 1689.12it/s]warmup should be done:  51%|     | 1516/3000 [00:00<00:00, 1667.59it/s]warmup should be done:  51%|     | 1525/3000 [00:00<00:00, 1686.28it/s]warmup should be done:  50%|     | 1490/3000 [00:00<00:00, 1623.97it/s]warmup should be done:  50%|     | 1500/3000 [00:00<00:00, 1644.02it/s]warmup should be done:  54%|    | 1631/3000 [00:01<00:00, 1630.82it/s]warmup should be done:  55%|    | 1650/3000 [00:01<00:00, 1651.80it/s]warmup should be done:  56%|    | 1673/3000 [00:01<00:00, 1667.97it/s]warmup should be done:  57%|    | 1696/3000 [00:01<00:00, 1690.14it/s]warmup should be done:  56%|    | 1683/3000 [00:01<00:00, 1667.61it/s]warmup should be done:  56%|    | 1694/3000 [00:01<00:00, 1678.96it/s]warmup should be done:  55%|    | 1656/3000 [00:01<00:00, 1634.54it/s]warmup should be done:  56%|    | 1666/3000 [00:01<00:00, 1648.75it/s]warmup should be done:  60%|    | 1795/3000 [00:01<00:00, 1632.92it/s]warmup should be done:  61%|    | 1816/3000 [00:01<00:00, 1653.73it/s]warmup should be done:  61%|   | 1840/3000 [00:01<00:00, 1667.98it/s]warmup should be done:  62%|   | 1866/3000 [00:01<00:00, 1690.83it/s]warmup should be done:  62%|   | 1850/3000 [00:01<00:00, 1664.75it/s]warmup should be done:  62%|   | 1862/3000 [00:01<00:00, 1676.73it/s]warmup should be done:  61%|    | 1823/3000 [00:01<00:00, 1642.79it/s]warmup should be done:  61%|    | 1832/3000 [00:01<00:00, 1651.79it/s]warmup should be done:  65%|   | 1959/3000 [00:01<00:00, 1634.34it/s]warmup should be done:  67%|   | 2007/3000 [00:01<00:00, 1666.57it/s]warmup should be done:  66%|   | 1982/3000 [00:01<00:00, 1652.11it/s]warmup should be done:  68%|   | 2036/3000 [00:01<00:00, 1691.33it/s]warmup should be done:  67%|   | 2017/3000 [00:01<00:00, 1661.26it/s]warmup should be done:  68%|   | 2030/3000 [00:01<00:00, 1676.72it/s]warmup should be done:  66%|   | 1989/3000 [00:01<00:00, 1645.92it/s]warmup should be done:  67%|   | 1998/3000 [00:01<00:00, 1650.67it/s]warmup should be done:  71%|   | 2125/3000 [00:01<00:00, 1641.61it/s]warmup should be done:  72%|  | 2174/3000 [00:01<00:00, 1665.68it/s]warmup should be done:  72%|  | 2148/3000 [00:01<00:00, 1652.23it/s]warmup should be done:  74%|  | 2206/3000 [00:01<00:00, 1690.37it/s]warmup should be done:  73%|  | 2184/3000 [00:01<00:00, 1660.38it/s]warmup should be done:  73%|  | 2198/3000 [00:01<00:00, 1676.07it/s]warmup should be done:  72%|  | 2154/3000 [00:01<00:00, 1647.06it/s]warmup should be done:  72%|  | 2164/3000 [00:01<00:00, 1649.95it/s]warmup should be done:  76%|  | 2291/3000 [00:01<00:00, 1644.94it/s]warmup should be done:  78%|  | 2341/3000 [00:01<00:00, 1665.55it/s]warmup should be done:  77%|  | 2314/3000 [00:01<00:00, 1652.53it/s]warmup should be done:  79%|  | 2376/3000 [00:01<00:00, 1689.40it/s]warmup should be done:  78%|  | 2351/3000 [00:01<00:00, 1659.16it/s]warmup should be done:  79%|  | 2366/3000 [00:01<00:00, 1676.02it/s]warmup should be done:  77%|  | 2321/3000 [00:01<00:00, 1650.95it/s]warmup should be done:  78%|  | 2330/3000 [00:01<00:00, 1652.21it/s]warmup should be done:  82%| | 2457/3000 [00:01<00:00, 1647.70it/s]warmup should be done:  83%| | 2481/3000 [00:01<00:00, 1656.78it/s]warmup should be done:  84%| | 2508/3000 [00:01<00:00, 1663.55it/s]warmup should be done:  85%| | 2546/3000 [00:01<00:00, 1690.84it/s]warmup should be done:  84%| | 2517/3000 [00:01<00:00, 1657.62it/s]warmup should be done:  85%| | 2537/3000 [00:01<00:00, 1683.72it/s]warmup should be done:  83%| | 2488/3000 [00:01<00:00, 1654.59it/s]warmup should be done:  83%| | 2497/3000 [00:01<00:00, 1655.14it/s]warmup should be done:  87%| | 2623/3000 [00:01<00:00, 1649.45it/s]warmup should be done:  88%| | 2648/3000 [00:01<00:00, 1659.95it/s]warmup should be done:  89%| | 2675/3000 [00:01<00:00, 1663.56it/s]warmup should be done:  91%| | 2716/3000 [00:01<00:00, 1689.25it/s]warmup should be done:  89%| | 2683/3000 [00:01<00:00, 1655.33it/s]warmup should be done:  90%| | 2708/3000 [00:01<00:00, 1689.21it/s]warmup should be done:  88%| | 2654/3000 [00:01<00:00, 1655.92it/s]warmup should be done:  89%| | 2663/3000 [00:01<00:00, 1652.74it/s]warmup should be done:  93%|| 2789/3000 [00:01<00:00, 1651.98it/s]warmup should be done:  94%|| 2816/3000 [00:01<00:00, 1664.37it/s]warmup should be done:  95%|| 2842/3000 [00:01<00:00, 1664.07it/s]warmup should be done:  96%|| 2885/3000 [00:01<00:00, 1688.00it/s]warmup should be done:  95%|| 2849/3000 [00:01<00:00, 1652.24it/s]warmup should be done:  96%|| 2878/3000 [00:01<00:00, 1691.25it/s]warmup should be done:  94%|| 2820/3000 [00:01<00:00, 1655.66it/s]warmup should be done:  94%|| 2829/3000 [00:01<00:00, 1651.87it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1688.60it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1682.86it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1665.64it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1664.75it/s]warmup should be done:  98%|| 2955/3000 [00:01<00:00, 1654.24it/s]warmup should be done: 100%|| 2985/3000 [00:01<00:00, 1669.83it/s]warmup should be done: 100%|| 2987/3000 [00:01<00:00, 1657.79it/s]warmup should be done: 100%|| 2995/3000 [00:01<00:00, 1653.72it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1651.91it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1650.15it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1648.38it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1636.51it/s]
WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fe782ecd0d0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fe782edc280>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fe782ecd160>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fe782f9ae80>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fe782f9dd30>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fe782f9b730>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fe782ece1f0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fe782ecc1c0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
2022-12-12 08:06:20.908423: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fe2bb02d350 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 08:06:20.908509: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 08:06:20.918033: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 08:06:21.087471: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fe2b3030fb0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 08:06:21.087534: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 08:06:21.097153: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 08:06:21.423422: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fe2ae82fdb0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 08:06:21.423486: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 08:06:21.433821: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 08:06:21.501000: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fe2be830420 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 08:06:21.501062: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 08:06:21.511125: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 08:06:21.800008: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fe2ba830460 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 08:06:21.800077: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 08:06:21.800098: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fe2b70299c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 08:06:21.800168: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 08:06:21.808238: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 08:06:21.808849: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 08:06:21.844541: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fe2a7035670 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 08:06:21.844604: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 08:06:21.845244: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fe2b2f920c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 08:06:21.845307: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 08:06:21.853232: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 08:06:21.854667: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 08:06:28.183370: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 08:06:28.428841: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 08:06:28.453057: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 08:06:28.493252: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 08:06:28.546201: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 08:06:28.742200: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 08:06:28.743448: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 08:06:28.839756: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][08:07:17.585][ERROR][RK0][tid #140612439361280]: replica 5 reaches 1000, calling init pre replica
[HCTR][08:07:17.585][ERROR][RK0][tid #140612439361280]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][08:07:17.593][ERROR][RK0][tid #140612439361280]: coll ps creation done
[HCTR][08:07:17.593][ERROR][RK0][tid #140612439361280]: replica 5 waits for coll ps creation barrier
[HCTR][08:07:17.790][ERROR][RK0][main]: replica 7 reaches 1000, calling init pre replica
[HCTR][08:07:17.790][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][08:07:17.795][ERROR][RK0][main]: coll ps creation done
[HCTR][08:07:17.795][ERROR][RK0][main]: replica 7 waits for coll ps creation barrier
[HCTR][08:07:17.895][ERROR][RK0][main]: replica 4 reaches 1000, calling init pre replica
[HCTR][08:07:17.895][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][08:07:17.904][ERROR][RK0][main]: coll ps creation done
[HCTR][08:07:17.904][ERROR][RK0][main]: replica 4 waits for coll ps creation barrier
[HCTR][08:07:17.951][ERROR][RK0][tid #140612170925824]: replica 1 reaches 1000, calling init pre replica
[HCTR][08:07:17.951][ERROR][RK0][tid #140612170925824]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][08:07:17.958][ERROR][RK0][tid #140612170925824]: coll ps creation done
[HCTR][08:07:17.958][ERROR][RK0][tid #140612170925824]: replica 1 waits for coll ps creation barrier
[HCTR][08:07:17.998][ERROR][RK0][tid #140612103816960]: replica 3 reaches 1000, calling init pre replica
[HCTR][08:07:17.998][ERROR][RK0][tid #140612103816960]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][08:07:18.003][ERROR][RK0][tid #140612103816960]: coll ps creation done
[HCTR][08:07:18.003][ERROR][RK0][tid #140612103816960]: replica 3 waits for coll ps creation barrier
[HCTR][08:07:18.058][ERROR][RK0][tid #140612187711232]: replica 6 reaches 1000, calling init pre replica
[HCTR][08:07:18.059][ERROR][RK0][tid #140612187711232]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][08:07:18.063][ERROR][RK0][tid #140612187711232]: coll ps creation done
[HCTR][08:07:18.063][ERROR][RK0][tid #140612187711232]: replica 6 waits for coll ps creation barrier
[HCTR][08:07:18.093][ERROR][RK0][tid #140612649080576]: replica 0 reaches 1000, calling init pre replica
[HCTR][08:07:18.094][ERROR][RK0][tid #140612649080576]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][08:07:18.101][ERROR][RK0][tid #140612649080576]: coll ps creation done
[HCTR][08:07:18.101][ERROR][RK0][tid #140612649080576]: replica 0 waits for coll ps creation barrier
[HCTR][08:07:18.247][ERROR][RK0][main]: replica 2 reaches 1000, calling init pre replica
[HCTR][08:07:18.247][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][08:07:18.252][ERROR][RK0][main]: coll ps creation done
[HCTR][08:07:18.252][ERROR][RK0][main]: replica 2 waits for coll ps creation barrier
[HCTR][08:07:18.252][ERROR][RK0][tid #140612649080576]: replica 0 preparing frequency
[HCTR][08:07:19.120][ERROR][RK0][tid #140612649080576]: replica 0 preparing frequency done
[HCTR][08:07:19.169][ERROR][RK0][tid #140612649080576]: replica 0 calling init per replica
[HCTR][08:07:19.169][ERROR][RK0][main]: replica 4 calling init per replica
[HCTR][08:07:19.169][ERROR][RK0][tid #140612103816960]: replica 3 calling init per replica
[HCTR][08:07:19.169][ERROR][RK0][main]: replica 2 calling init per replica
[HCTR][08:07:19.169][ERROR][RK0][tid #140612170925824]: replica 1 calling init per replica
[HCTR][08:07:19.169][ERROR][RK0][main]: replica 7 calling init per replica
[HCTR][08:07:19.169][ERROR][RK0][tid #140612187711232]: replica 6 calling init per replica
[HCTR][08:07:19.169][ERROR][RK0][tid #140612439361280]: replica 5 calling init per replica
[HCTR][08:07:19.169][ERROR][RK0][tid #140612649080576]: Calling build_v2
[HCTR][08:07:19.169][ERROR][RK0][main]: Calling build_v2
[HCTR][08:07:19.169][ERROR][RK0][tid #140612103816960]: Calling build_v2
[HCTR][08:07:19.169][ERROR][RK0][main]: Calling build_v2
[HCTR][08:07:19.169][ERROR][RK0][tid #140612170925824]: Calling build_v2
[HCTR][08:07:19.169][ERROR][RK0][main]: Calling build_v2
[HCTR][08:07:19.169][ERROR][RK0][tid #140612187711232]: Calling build_v2
[HCTR][08:07:19.169][ERROR][RK0][tid #140612649080576]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][08:07:19.169][ERROR][RK0][tid #140612439361280]: Calling build_v2
[HCTR][08:07:19.169][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][08:07:19.169][ERROR][RK0][tid #140612103816960]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][08:07:19.169][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][08:07:19.169][ERROR][RK0][tid #140612170925824]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][08:07:19.169][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][08:07:19.169][ERROR][RK0][tid #140612187711232]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][08:07:19.169][ERROR][RK0][tid #140612439361280]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[2022-12-12 08:07:19.173867: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] [v100x8, slow pcie
2022-12-12 08:07:19[.2022-12-12 08:07:19173910.: 173942E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc :[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc178:] 196v100x8, slow pcie] 2022-12-12 08:07:19
assigning 0 to cpu.
173955: [E2022-12-12 08:07:19 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc174000:[: 178E]  v100x8, slow pcie/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
:196] assigning 0 to cpu
[[2022-12-12 08:07:192022-12-12 08:07:19..2022-12-12 08:07:19174044174047.: : 174004EE:   [E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[ ::2022-12-12 08:07:19/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 08:07:19196212.:[.] ] 174090178174054assigning 0 to cpubuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8: 2022-12-12 08:07:19] v100x8, slow pcie: [

E.
E 2022-12-12 08:07:19174095[ [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[.: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[2022-12-12 08:07:19:2022-12-12 08:07:192022-12-12 08:07:19174137E:2022-12-12 08:07:19.212..:  178.174206] 174208174184E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] 174224: build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8: :  :v100x8, slow pcie: E
EE/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc178
E   :] [ /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc178v100x8, slow pcie2022-12-12 08:07:19/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc::2022-12-12 08:07:19:] 
.:213196.178v100x8, slow pcie174356212] [] 174364] 
: ] remote time is 8.684212022-12-12 08:07:19assigning 0 to cpu: v100x8, slow pcieEbuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8
[.
E
 
2022-12-12 08:07:19[174424 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.[2022-12-12 08:07:19[[: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:1744692022-12-12 08:07:19.2022-12-12 08:07:192022-12-12 08:07:19E:213: .174499.. 196] E174516: 174524174526/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] remote time is 8.68421 : E: : :assigning 0 to cpu
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE EE196
: [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc  ] 196/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 08:07:19:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccassigning 0 to cpu] [:.214::
assigning 0 to cpu2022-12-12 08:07:19196174668] 212213
.] : cpu time is 97.0588] ] 174703assigning 0 to cpuE
build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8remote time is 8.68421: [
 

E2022-12-12 08:07:19/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[ .[[:2022-12-12 08:07:19/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc1747832022-12-12 08:07:19[2022-12-12 08:07:19214.:: .2022-12-12 08:07:19.] 174813212E174820.174821cpu time is 97.0588: ]  : 174835: 
Ebuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE: E 
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc212/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[:] :/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:2022-12-12 08:07:19212build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8214:213.] 
] 212] 174974build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8cpu time is 97.0588] remote time is 8.68421[: 

build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8
2022-12-12 08:07:19E
[. [2022-12-12 08:07:19175055/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 08:07:19[.: :.2022-12-12 08:07:19175091E213175099.:  E] : 175109/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc remote time is 8.68421E: :/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
 E213:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc [] 213:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 08:07:19remote time is 8.68421] 214:.
remote time is 8.68421] 213175214
cpu time is 97.0588[] : 
2022-12-12 08:07:19[remote time is 8.68421E.2022-12-12 08:07:19
 175266./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 175281[:E: 2022-12-12 08:07:19214 E.] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 175313cpu time is 97.0588:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 
214:E] 214 cpu time is 97.0588] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
cpu time is 97.0588:
214] cpu time is 97.0588
[2022-12-12 08:08:36.432306: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-12 08:08:36.472362: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
[2022-12-12 08:08:36.472437: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:919] num_cached_nodes = 30000001
[2022-12-12 08:08:36.585492: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-12 08:08:36.585584: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-12 08:08:36.585618: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-12 08:08:36.585649: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-12 08:08:36.586065: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 08:08:36.586960: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 08:08:36.587635: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 08:08:36.600798: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 5 solved
[2022-12-12 08:08:36.600866: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 5 initing device 5
[[2022-12-12 08:08:36[2022-12-12 08:08:36.2022-12-12 08:08:36.601221[.601226: 2022-12-12 08:08:36601254: E.: E 601255E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:202 :[202] /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc19802022-12-12 08:08:36] :6 solved] .3 solved202
eager alloc mem 381.47 MB601303
] 
[[: 1 solved2022-12-12 08:08:362022-12-12 08:08:36E
.. 601402601412[/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc: : 2022-12-12 08:08:36:EE.202  601442] /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc: 2 solved::E
205205 ] ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc[worker 0 thread 6 initing device 6worker 0 thread 3 initing device 3:2022-12-12 08:08:36

205.] 601534worker 0 thread 1 initing device 1: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 2 initing device 2
[[2022-12-12 08:08:362022-12-12 08:08:36..601916601921: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2022-12-12 08:08:36:1980.1980] 601948] eager alloc mem 381.47 MB: eager alloc mem 381.47 MB
E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB[
2022-12-12 08:08:36.601999: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[[2022-12-12 08:08:362022-12-12 08:08:36..603368603380: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::202202] ] 7 solved4 solved

[[2022-12-12 08:08:362022-12-12 08:08:36..603503603506: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::205205] ] worker 0 thread 7 initing device 7worker 0 thread 4 initing device 4

[[2022-12-12 08:08:362022-12-12 08:08:36..603962603962: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 381.47 MBeager alloc mem 381.47 MB

[2022-12-12 08:08:36.604134: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 08:08:36.606456: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 08:08:36.606505: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 08:08:36.606573: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 08:08:36.606626: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 08:08:36.608261: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 08:08:36.608755: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 08:08:36.608791: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 08:08:36.610694: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 08:08:36.610787: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 08:08:36.610836: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 08:08:36.610951: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 08:08:36.612435: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 08:08:36.612496: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 08:08:36.668811: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.00 Bytes
[2022-12-12 08:08:36.669193: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-12 08:08:36.674703: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 08:08:36.674802: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-12 08:08:36.674850: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 08:08:36.675707: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 114.82 MB
[2022-12-12 08:08:36.676463: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:08:36.677594: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:08:36.677682: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 08:08:36.678356: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 08:08:36.678397: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 14.31 GB
[2022-12-12 08:08:36.691181: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.00 Bytes
[2022-12-12 08:08:36.691512: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-12 08:08:36.692329: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.00 Bytes
[2022-12-12 08:08:36.692404: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.00 Bytes
[2022-12-12 08:08:36.692632: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-12 08:08:36.692693: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-12 08:08:36.694344: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.00 Bytes
[2022-12-12 08:08:36.694630: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-12 08:08:36.694991: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.00 Bytes
[2022-12-12 08:08:36.695295: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[[2022-12-12 08:08:362022-12-12 08:08:36..698555698555: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 2.00 Byteseager alloc mem 2.00 Bytes

[2022-12-12 08:08:36.698890: [E2022-12-12 08:08:36 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu698898:: 1980E]  eager alloc mem 1024.00 Bytes/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
:1980] eager alloc mem 1024.00 Bytes
[2022-12-12 08:08:36.743149: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 08:08:36.743238: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2[
2022-12-12 08:08:36.743241: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024[
2022-12-12 08:08:36.743290: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 08:08:36[.2022-12-12 08:08:36743315.: 743304E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638:] 638eager release cuda mem 2] 
eager release cuda mem 1024
[[2022-12-12 08:08:36[2022-12-12 08:08:36.2022-12-12 08:08:36.743386.743372: 743390: E: E E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638:638] 638] eager release cuda mem 400000000] eager release cuda mem 1024
eager release cuda mem 2
[
2022-12-12 08:08:36.743445: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-12 08:08:36:.638[743483] 2022-12-12 08:08:36: eager release cuda mem 1024.E
743494 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE: 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] :[eager release cuda mem 26382022-12-12 08:08:36
] .eager release cuda mem 400000000[743532
2022-12-12 08:08:36: [.E2022-12-12 08:08:36743530 .: /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc743565E::  638E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc]  [:eager release cuda mem 2/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-12 08:08:36638
:.] 638743617eager release cuda mem 1024[] : 
2022-12-12 08:08:36eager release cuda mem 400000000E.
 743690[/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: 2022-12-12 08:08:36:E.638 743724] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: eager release cuda mem 1024:E
638 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 400000000:
638[] 2022-12-12 08:08:36eager release cuda mem 2.
743815: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-12 08:08:36.743846: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000[
2022-12-12 08:08:36.743868: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 08:08:36.744278: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 114.82 MB
[2022-12-12 08:08:36.745418: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 114.82 MB
[2022-12-12 08:08:36.746090: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 114.82 MB
[2022-12-12 08:08:36.747173: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 114.82 MB
[2022-12-12 08:08:36.747844: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 114.82 MB
[2022-12-12 08:08:36.748360: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 114.82 MB
[2022-12-12 08:08:36.748868: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 114.82 MB
[2022-12-12 08:08:36.749779: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:08:36.750404: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:08:36.750519: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:08:36.750765: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:08:36.750810: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:08:36.750862: E[ 2022-12-12 08:08:36/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.:750866638: ] Eeager release cuda mem 625663[ 
2022-12-12 08:08:36/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.:7509041980: ] Eeager alloc mem 611.00 KB 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:08:36.750992: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 08:08:36.751487: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:08:36.751570: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 08:08:36.751610: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:08:36.751664: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 08:08:36.751694: E[ 2022-12-12 08:08:36/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.:7517041980: ] Eeager alloc mem 25.25 KB 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 14.31 GB
[2022-12-12 08:08:36.751821: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:08:36.751891: E[ 2022-12-12 08:08:36/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.:751905638: ] Eeager release cuda mem 625663 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 08:08:36.751989: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 08:08:36[.2022-12-12 08:08:36752030.: 752036E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638:] 638eager release cuda mem 625663] 
eager release cuda mem 625663
[[2022-12-12 08:08:362022-12-12 08:08:36..752144752146: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 25.25 KBeager alloc mem 25.25 KB

[2022-12-12 08:08:36.752239: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 08:08:36.752281: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 14.31 GB
[2022-12-12 08:08:36.752374: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 08:08:36.752413: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 14.31 GB
[2022-12-12 08:08:36.752598: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 08:08:36.752638: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] [eager alloc mem 14.31 GB2022-12-12 08:08:36
.752656: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 08:08:36.752698: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 14.31 GB
[2022-12-12 08:08:36[.2022-12-12 08:08:36752844.: 752851E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638:] 638eager release cuda mem 25855] 
eager release cuda mem 25855
[2022-12-12 08:08:36.[7529092022-12-12 08:08:36: .E752914 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: 1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] :eager alloc mem 14.31 GB1980
] eager alloc mem 14.31 GB
[[[[[[[[2022-12-12 08:08:402022-12-12 08:08:402022-12-12 08:08:402022-12-12 08:08:402022-12-12 08:08:402022-12-12 08:08:402022-12-12 08:08:402022-12-12 08:08:40........ 37995 37993 37994 37993 37993 37998 37995 37993: : : : : : : : EEEEEEEE        /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::::::19801980198019801980198019801980] ] eager alloc mem 611.00 KB] ] ] ] ] ] eager alloc mem 611.00 KB
eager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KB






[2022-12-12 08:08:40. 39178: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[[2022-12-12 08:08:40[:2022-12-12 08:08:402022-12-12 08:08:40[.[2022-12-12 08:08:40[638..2022-12-12 08:08:40 391922022-12-12 08:08:40.2022-12-12 08:08:40]  39202 39192.: . 39199.eager release cuda mem 625663: :  39210E 39227:  39212
EE:  : E:   E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc ::/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638[638:] :638:] 2022-12-12 08:08:40] 638eager release cuda mem 625663638] 638eager release cuda mem 625663.eager release cuda mem 625663] 
] eager release cuda mem 625663] 
 39360
eager release cuda mem 625663eager release cuda mem 625663
eager release cuda mem 625663: 


E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:08:40. 39492[: 2022-12-12 08:08:40E.[  395042022-12-12 08:08:40/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[: .:2022-12-12 08:08:40E[ 39512[1980. [2022-12-12 08:08:40: 2022-12-12 08:08:40]  39520/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-12 08:08:40.E.eager alloc mem 611.00 KB: :. 39530  39532
E1980 39537: /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:  ] : E:E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 611.00 KBE 1980 :
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:eager alloc mem 611.00 KB:] :1980
1980eager alloc mem 611.00 KB1980] ] 
] eager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KB


[2022-12-12 08:08:40. 40229: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:08:40. 40300: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:08:40. 40332: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:08:40. 40387: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638[] 2022-12-12 08:08:40eager release cuda mem 625663.
 40406: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:08:40. 40440: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638[] 2022-12-12 08:08:40eager release cuda mem 625663[.
2022-12-12 08:08:40 40458[.: [2022-12-12 08:08:40 40464E2022-12-12 08:08:40[.:  .2022-12-12 08:08:40 40473E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc 40478.:  :[:  40484E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu6382022-12-12 08:08:40E:  :] . E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc1980eager release cuda mem 625663 40520/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :] 
: :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638eager alloc mem 611.00 KBE638:] 
 ] 638eager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager release cuda mem 625663] 
:[
eager release cuda mem 62566319802022-12-12 08:08:40
] .eager alloc mem 611.00 KB 40648
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:08:40. 40702[: 2022-12-12 08:08:40E.  40709/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: :[E19802022-12-12 08:08:40 ] ./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 611.00 KB 40725:
: 1980E]  eager alloc mem 611.00 KB/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
:1980] eager alloc mem 611.00 KB
[2022-12-12 08:08:40. 41050: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:08:40. 41118: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:08:40. 41167: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:08:40. 41237: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:08:40. 41357: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:08:40. 41417[: 2022-12-12 08:08:40E[. 2022-12-12 08:08:40 41426/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.: : 41431E638:  ] E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager release cuda mem 625663 :
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc1980:] 638eager alloc mem 611.00 KB] 
eager release cuda mem 625663
[2022-12-12 08:08:40. 41501: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:[6382022-12-12 08:08:40] .eager release cuda mem 625663[ 41518
2022-12-12 08:08:40: [.E2022-12-12 08:08:40[ 41528 .2022-12-12 08:08:40: /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu 41537.E::  41541[ 1980E: 2022-12-12 08:08:40/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc]  E.:eager alloc mem 611.00 KB/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu  41581638
:/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: ] 1980:Eeager release cuda mem 625663] 638 
eager alloc mem 611.00 KB] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
eager release cuda mem 625663:
1980] eager alloc mem 611.00 KB
[2022-12-12 08:08:40. 41720: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:[19802022-12-12 08:08:40] .eager alloc mem 611.00 KB 41748
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:08:40. 41869: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:08:40. 41936: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:08:40. 41984: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:08:40. 42058: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:08:40. 42215: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:08:40. 42282: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:08:40. 42364: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:08:40. 42418: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-12 08:08:40:[.6382022-12-12 08:08:40 42432] .: eager release cuda mem 625663 42436E
:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc1980:] 638eager alloc mem 611.00 KB] 
eager release cuda mem 625663
[2022-12-12 08:08:40[.2022-12-12 08:08:40 42507.:  42510E:  E[/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu [2022-12-12 08:08:40:/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-12 08:08:40.1980:. 42529] 638 42535: eager alloc mem 611.00 KB] : E
eager release cuda mem 625663E 
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::6381980] ] eager release cuda mem 625663eager alloc mem 611.00 KB

[2022-12-12 08:08:40. 42660: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:08:40. 42684: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638[] 2022-12-12 08:08:40eager release cuda mem 625663.
 42705: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:08:40. 42759: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:08:40. 42806: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:08:40. 42873: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:08:40. 43029: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:08:40. 43095: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:08:40. 43215: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:08:40. 43285: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:08:40. 43327: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[[2022-12-12 08:08:402022-12-12 08:08:40.. 43392 43395: : [EE2022-12-12 08:08:40  ./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu 43410::: 6381980E] ]  eager release cuda mem 625663eager alloc mem 611.00 KB/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc

:[6382022-12-12 08:08:40] .eager release cuda mem 625663 43458
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] [eager release cuda mem 6256632022-12-12 08:08:40[
.2022-12-12 08:08:40 43505.[:  435102022-12-12 08:08:40E: . E[ 43537/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu 2022-12-12 08:08:40: :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.E1980: 43570 ] 638[: /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 611.00 KB] 2022-12-12 08:08:40E:
eager release cuda mem 625663. 1980
 43619/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] : :eager alloc mem 611.00 KBE1980
 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager alloc mem 611.00 KB:
[6382022-12-12 08:08:40] .eager release cuda mem 625663 43733
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:08:40. 43810: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:08:40. 43848: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:08:40. 43913: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:08:40. 44033: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:08:40. 44100: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:08:40. 44192: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:08:40. 44258: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 08:08:40. 44392: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:08:40. 44448: E[ 2022-12-12 08:08:40/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.: 44460638[: ] 2022-12-12 08:08:40Eeager release cuda mem 625663. 
 44472/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[: :2022-12-12 08:08:40E1980. ]  44509/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager alloc mem 611.00 KB[: :[
2022-12-12 08:08:40E6382022-12-12 08:08:40. ] . 44553/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 625663 44558: :
: E638E ]  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:
[:1980[2022-12-12 08:08:40638] [2022-12-12 08:08:40.] eager alloc mem 611.00 KB2022-12-12 08:08:40. 44660eager release cuda mem 625663
. 44671: 
 44684: E: E E[ /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc 2022-12-12 08:08:40/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.:638: 447511980] 638: ] eager release cuda mem 625663] Eeager alloc mem 611.00 KB
[eager release cuda mem 120400004 
2022-12-12 08:08:40
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[.:2022-12-12 08:08:40 44846638.: ]  44871Eeager release cuda mem 120400004:  
E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638:] 638eager release cuda mem 625663] 
eager release cuda mem 120400004
[2022-12-12 08:08:40. 44948: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 120400004
[2022-12-12 08:08:40. 45010: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:08:40. 45047: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 120400004
[2022-12-12 08:08:40. 45305: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:08:40. 45342: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 120400004
[2022-12-12 08:08:40. 45445: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:08:40. 45489: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 120400004
[2022-12-12 08:08:40. 45603: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 08:08:40. 45644: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 120400004
[2022-12-12 08:08:40. 45870: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 30000001 / 100000000 nodes ( 30.00 %~30.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 69999999 / 100000000 nodes ( 70.00 %) | 14.31 GB | 3.44388 secs 
[2022-12-12 08:08:40. 46145: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 30000001 / 100000000 nodes ( 30.00 %~30.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 69999999 / 100000000 nodes ( 70.00 %) | 14.31 GB | 3.44219 secs 
[2022-12-12 08:08:40. 46500: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 30000001 / 100000000 nodes ( 30.00 %~30.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 69999999 / 100000000 nodes ( 70.00 %) | 14.31 GB | 3.44526 secs 
[2022-12-12 08:08:40. 46716: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 30000001 / 100000000 nodes ( 30.00 %~30.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 69999999 / 100000000 nodes ( 70.00 %) | 14.31 GB | 3.44477 secs 
[2022-12-12 08:08:40. 47088: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 30000001 / 100000000 nodes ( 30.00 %~30.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 69999999 / 100000000 nodes ( 70.00 %) | 14.31 GB | 3.44517 secs 
[2022-12-12 08:08:40. 47504: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 30000001 / 100000000 nodes ( 30.00 %~30.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 69999999 / 100000000 nodes ( 70.00 %) | 14.31 GB | 3.44559 secs 
[2022-12-12 08:08:40. 47709: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 30000001 / 100000000 nodes ( 30.00 %~30.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 69999999 / 100000000 nodes ( 70.00 %) | 14.31 GB | 3.46165 secs 
[2022-12-12 08:08:40. 47921: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 30000001 / 100000000 nodes ( 30.00 %~30.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 69999999 / 100000000 nodes ( 70.00 %) | 14.31 GB | 3.44397 secs 
[HCTR][08:08:40.048][ERROR][RK0][tid #140612187711232]: replica 6 calling init per replica done, doing barrier
[HCTR][08:08:40.048][ERROR][RK0][main]: replica 2 calling init per replica done, doing barrier
[HCTR][08:08:40.048][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier
[HCTR][08:08:40.048][ERROR][RK0][tid #140612103816960]: replica 3 calling init per replica done, doing barrier
[HCTR][08:08:40.048][ERROR][RK0][tid #140612649080576]: replica 0 calling init per replica done, doing barrier
[HCTR][08:08:40.048][ERROR][RK0][tid #140612170925824]: replica 1 calling init per replica done, doing barrier
[HCTR][08:08:40.048][ERROR][RK0][tid #140612439361280]: replica 5 calling init per replica done, doing barrier
[HCTR][08:08:40.048][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier
[HCTR][08:08:40.048][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier done
[HCTR][08:08:40.048][ERROR][RK0][tid #140612649080576]: replica 0 calling init per replica done, doing barrier done
[HCTR][08:08:40.048][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier done
[HCTR][08:08:40.048][ERROR][RK0][tid #140612187711232]: replica 6 calling init per replica done, doing barrier done
[HCTR][08:08:40.048][ERROR][RK0][tid #140612103816960]: replica 3 calling init per replica done, doing barrier done
[HCTR][08:08:40.048][ERROR][RK0][tid #140612170925824]: replica 1 calling init per replica done, doing barrier done
[HCTR][08:08:40.048][ERROR][RK0][main]: init per replica done
[HCTR][08:08:40.048][ERROR][RK0][main]: replica 2 calling init per replica done, doing barrier done
[HCTR][08:08:40.048][ERROR][RK0][tid #140612439361280]: replica 5 calling init per replica done, doing barrier done
[HCTR][08:08:40.048][ERROR][RK0][main]: init per replica done
[HCTR][08:08:40.048][ERROR][RK0][tid #140612187711232]: init per replica done
[HCTR][08:08:40.048][ERROR][RK0][tid #140612103816960]: init per replica done
[HCTR][08:08:40.048][ERROR][RK0][tid #140612170925824]: init per replica done
[HCTR][08:08:40.048][ERROR][RK0][main]: init per replica done
[HCTR][08:08:40.048][ERROR][RK0][tid #140612439361280]: init per replica done
[HCTR][08:08:40.050][ERROR][RK0][tid #140612649080576]: init per replica done
