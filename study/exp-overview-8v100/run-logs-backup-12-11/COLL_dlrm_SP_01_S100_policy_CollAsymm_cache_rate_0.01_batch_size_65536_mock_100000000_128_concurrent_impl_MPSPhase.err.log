2022-12-11 20:39:07.065431: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.073165: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.081914: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.086667: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.099352: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.111768: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.120025: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.124498: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.172546: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.177231: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.180385: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.181468: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.182466: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.183478: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.184449: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.185420: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.186382: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.187518: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.188502: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.189635: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.191172: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.192044: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.192319: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.193661: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.193676: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.195031: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.195157: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.196593: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.197142: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.197755: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.198831: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.199237: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.200415: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.200982: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.201851: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.202610: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.203211: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.204437: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.205398: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.206378: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.211527: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 20:39:07.214620: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.215626: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.216088: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.217517: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.217669: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.219017: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.219270: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.220872: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.221277: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.221898: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.223420: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.223628: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.223859: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.224231: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.226236: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.226508: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.226891: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.226902: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.229344: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.229709: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.230094: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.230566: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.232195: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.233104: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.233558: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.235514: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.236002: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.237494: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.238026: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.239484: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.240148: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.241852: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.242595: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.244900: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.244948: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.247062: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.247067: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.247294: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.248368: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.249095: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.249279: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.251721: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.253453: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.253700: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.254418: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.255464: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.255640: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.256403: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.277412: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.278427: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.288016: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.290694: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.291018: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.291849: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.292209: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.293242: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.293529: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.293898: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.295513: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.296079: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.296572: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.297518: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.298533: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.298637: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.298859: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.300017: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.301349: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.301524: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.302164: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.303075: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.304079: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.304194: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.306181: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.306352: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.306660: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.307658: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.307945: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.308015: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.309793: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.310975: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.311551: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.311796: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.311957: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.313678: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.314562: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.315369: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.315522: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.315688: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.317234: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.317631: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.318608: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.318816: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.318992: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.320398: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.321040: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.321717: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.321900: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.322076: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.323781: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.324244: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.324850: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.325093: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.325253: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.326887: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.327461: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.328222: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.328408: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.328722: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.330213: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.330628: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.331513: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.331777: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.331806: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.333363: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.334044: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.334687: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.335028: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.335165: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.337131: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.337379: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.337755: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.338037: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.338123: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.340435: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.340830: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.340880: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.341168: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.341261: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.343595: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.343952: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.344096: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.344331: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.344373: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.346654: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.347446: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.347994: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.348350: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.348381: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.349681: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.350144: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.350845: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.351048: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.351442: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.351587: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.353382: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.353724: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.354030: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.354673: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.354823: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.355523: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.355562: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.357241: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.357643: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.358198: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.359074: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.359100: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.359681: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.359711: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.361345: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.361948: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.362261: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.363240: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.363284: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.363922: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.364042: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.365766: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.366617: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.367447: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.367521: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.367864: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.368037: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.369358: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.370396: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.370564: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 20:39:07.371117: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.371336: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.371664: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.372557: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.372850: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.373874: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.374856: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.374920: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.375199: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.376508: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.376692: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.377531: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.378502: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.380048: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.380651: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.380761: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.382106: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 20:39:07.382631: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 20:39:07.382686: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.383012: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.383058: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 20:39:07.383326: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.384617: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 20:39:07.385226: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.385370: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.385551: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.387591: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.387596: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.389959: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.389984: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.392504: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.392601: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.392646: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.393115: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.393411: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.395409: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.396066: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.396192: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.396276: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.397182: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.397692: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.399913: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.400250: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.400334: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.400563: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.401248: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.401952: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.404223: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.406029: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.433203: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.437509: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.438270: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.442196: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.443321: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.446749: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.449130: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.452905: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.453778: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.467647: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.469461: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.473613: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.474501: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.478091: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.478845: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.510772: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.513538: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 20:39:07.519009: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 20:39:07.524150: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.529255: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.532824: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.534685: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.538778: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:07.569145: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:08.471066: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:08.471692: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:08.472229: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:08.473031: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 20:39:08.473088: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-11 20:39:08.490968: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:08.491977: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:08.492489: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:08.493076: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:08.494385: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:08.494870: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-11 20:39:08.540305: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:39:08.540520: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:39:08.599839: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-11 20:39:08.761498: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:08.762127: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:08.762645: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:08.763117: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 20:39:08.763179: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-11 20:39:08.774924: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:08.775162: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:08.775997: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:08.776516: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:08.777068: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:08.777437: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:08.778365: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 20:39:08.778421: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-11 20:39:08.778518: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 20:39:08.778564: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-11 20:39:08.781001: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:08.781847: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:08.782353: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:08.782927: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:08.783452: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:08.784497: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-11 20:39:08.788413: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:08.789008: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:08.789518: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:08.789986: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 20:39:08.790032: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-11 20:39:08.795820: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:08.795820: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:08.797007: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:08.797031: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:08.798002: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:08.798057: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:08.799309: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:08.799340: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:08.800849: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:08.800878: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:08.801823: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-11 20:39:08.801862: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-11 20:39:08.807288: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:08.807906: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:08.808414: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:08.809008: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:08.809533: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:08.810035: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-11 20:39:08.810055: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:08.810625: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:08.811168: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:08.811634: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 20:39:08.811673: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-11 20:39:08.828683: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:08.829331: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:08.829844: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:08.830413: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:08.831169: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:08.831644: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-11 20:39:08.834843: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:08.835465: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:08.836001: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:08.836467: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 20:39:08.836513: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-11 20:39:08.838614: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:08.839229: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:08.839756: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:08.840254: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 20:39:08.840303: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-11 20:39:08.847641: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:39:08.847859: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:39:08.849599: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
2022-12-11 20:39:08.853977: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:08.854613: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:08.855144: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:08.855746: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:08.855808: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:39:08.856008: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:39:08.856302: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:08.856777: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-11 20:39:08.857593: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:08.858042: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
2022-12-11 20:39:08.858238: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:08.858749: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:08.859339: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:08.859874: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 20:39:08.860335: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-11 20:39:08.878575: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:39:08.878803: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:39:08.880550: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
2022-12-11 20:39:08.881704: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:39:08.881899: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:39:08.883653: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-11 20:39:08.900708: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:39:08.900899: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:39:08.902465: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
2022-12-11 20:39:08.904273: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:39:08.904421: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:39:08.906120: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
2022-12-11 20:39:08.907457: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:39:08.907604: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 20:39:08.909970: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
[HCTR][20:39:10.162][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][20:39:10.173][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][20:39:10.173][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][20:39:10.173][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][20:39:10.173][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][20:39:10.173][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][20:39:10.210][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][20:39:10.211][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:01,  1.54s/it]warmup run: 1it [00:01,  1.53s/it]warmup run: 1it [00:01,  1.51s/it]warmup run: 1it [00:01,  1.56s/it]warmup run: 1it [00:01,  1.55s/it]warmup run: 94it [00:01, 79.66it/s]warmup run: 1it [00:01,  1.51s/it]warmup run: 1it [00:01,  1.50s/it]warmup run: 95it [00:01, 80.90it/s]warmup run: 100it [00:01, 86.08it/s]warmup run: 98it [00:01, 82.13it/s]warmup run: 93it [00:01, 78.04it/s]warmup run: 187it [00:01, 171.66it/s]warmup run: 99it [00:01, 85.82it/s]warmup run: 101it [00:01, 86.73it/s]warmup run: 187it [00:01, 172.01it/s]warmup run: 200it [00:01, 186.43it/s]warmup run: 195it [00:01, 177.24it/s]warmup run: 190it [00:01, 173.75it/s]warmup run: 280it [00:01, 272.96it/s]warmup run: 199it [00:01, 186.69it/s]warmup run: 202it [00:01, 187.70it/s]warmup run: 1it [00:01,  1.52s/it]warmup run: 278it [00:01, 271.11it/s]warmup run: 300it [00:01, 296.69it/s]warmup run: 294it [00:01, 285.07it/s]warmup run: 288it [00:01, 280.92it/s]warmup run: 373it [00:01, 378.06it/s]warmup run: 297it [00:01, 294.64it/s]warmup run: 303it [00:01, 298.72it/s]warmup run: 93it [00:01, 79.40it/s]warmup run: 369it [00:01, 373.58it/s]warmup run: 401it [00:01, 411.99it/s]warmup run: 395it [00:01, 400.17it/s]warmup run: 387it [00:01, 393.57it/s]warmup run: 466it [00:02, 480.61it/s]warmup run: 394it [00:01, 404.17it/s]warmup run: 405it [00:01, 414.85it/s]warmup run: 186it [00:01, 172.01it/s]warmup run: 463it [00:02, 478.50it/s]warmup run: 503it [00:02, 525.56it/s]warmup run: 493it [00:02, 506.92it/s]warmup run: 485it [00:02, 501.92it/s]warmup run: 560it [00:02, 576.01it/s]warmup run: 492it [00:02, 512.72it/s]warmup run: 507it [00:02, 527.55it/s]warmup run: 279it [00:01, 273.93it/s]warmup run: 559it [00:02, 579.61it/s]warmup run: 606it [00:02, 631.73it/s]warmup run: 596it [00:02, 615.78it/s]warmup run: 584it [00:02, 603.54it/s]warmup run: 655it [00:02, 661.72it/s]warmup run: 594it [00:02, 619.27it/s]warmup run: 610it [00:02, 632.65it/s]warmup run: 373it [00:01, 380.79it/s]warmup run: 657it [00:02, 671.83it/s]warmup run: 710it [00:02, 725.15it/s]warmup run: 699it [00:02, 709.49it/s]warmup run: 684it [00:02, 693.15it/s]warmup run: 750it [00:02, 732.02it/s]warmup run: 696it [00:02, 711.63it/s]warmup run: 713it [00:02, 723.20it/s]warmup run: 467it [00:02, 484.93it/s]warmup run: 755it [00:02, 748.17it/s]warmup run: 813it [00:02, 800.05it/s]warmup run: 802it [00:02, 787.69it/s]warmup run: 783it [00:02, 766.34it/s]warmup run: 844it [00:02, 785.36it/s]warmup run: 798it [00:02, 788.21it/s]warmup run: 815it [00:02, 796.27it/s]warmup run: 561it [00:02, 580.07it/s]warmup run: 854it [00:02, 809.71it/s]warmup run: 915it [00:02, 856.24it/s]warmup run: 905it [00:02, 850.03it/s]warmup run: 882it [00:02, 822.40it/s]warmup run: 939it [00:02, 827.81it/s]warmup run: 897it [00:02, 837.04it/s]warmup run: 917it [00:02, 854.14it/s]warmup run: 656it [00:02, 665.17it/s]warmup run: 953it [00:02, 856.38it/s]warmup run: 1016it [00:02, 897.25it/s]warmup run: 1006it [00:02, 892.58it/s]warmup run: 980it [00:02, 863.36it/s]warmup run: 1033it [00:02, 858.95it/s]warmup run: 1018it [00:02, 894.74it/s]warmup run: 752it [00:02, 737.17it/s]warmup run: 996it [00:02, 837.88it/s]warmup run: 1050it [00:02, 885.33it/s]warmup run: 1117it [00:02, 928.74it/s]warmup run: 1107it [00:02, 922.35it/s]warmup run: 1078it [00:02, 893.44it/s]warmup run: 1127it [00:02, 881.47it/s]warmup run: 1119it [00:02, 923.21it/s]warmup run: 848it [00:02, 794.57it/s]warmup run: 1098it [00:02, 885.04it/s]warmup run: 1148it [00:02, 909.70it/s]warmup run: 1218it [00:02, 940.91it/s]warmup run: 1208it [00:02, 943.39it/s]warmup run: 1176it [00:02, 901.22it/s]warmup run: 1224it [00:02, 904.63it/s]warmup run: 1220it [00:02, 940.20it/s]warmup run: 1201it [00:02, 925.09it/s]warmup run: 942it [00:02, 774.96it/s]warmup run: 1248it [00:02, 934.04it/s]warmup run: 1318it [00:02, 947.93it/s]warmup run: 1309it [00:02, 959.98it/s]warmup run: 1274it [00:02, 921.55it/s]warmup run: 1321it [00:02, 920.97it/s]warmup run: 1320it [00:02, 957.19it/s]warmup run: 1306it [00:02, 958.17it/s]warmup run: 1349it [00:02, 954.53it/s]warmup run: 1030it [00:02, 759.15it/s]warmup run: 1417it [00:02, 949.20it/s]warmup run: 1410it [00:02, 974.42it/s]warmup run: 1371it [00:02, 931.38it/s]warmup run: 1420it [00:03, 939.14it/s]warmup run: 1420it [00:02, 962.67it/s]warmup run: 1411it [00:02, 982.27it/s]warmup run: 1450it [00:03, 970.63it/s]warmup run: 1130it [00:02, 821.29it/s]warmup run: 1515it [00:03, 957.06it/s]warmup run: 1511it [00:03, 982.65it/s]warmup run: 1467it [00:03, 930.84it/s]warmup run: 1523it [00:03, 965.87it/s]warmup run: 1520it [00:03, 968.18it/s]warmup run: 1516it [00:03, 1001.91it/s]warmup run: 1551it [00:03, 979.91it/s]warmup run: 1233it [00:02, 876.72it/s]warmup run: 1613it [00:03, 962.15it/s]warmup run: 1612it [00:03, 988.84it/s]warmup run: 1563it [00:03, 924.30it/s]warmup run: 1626it [00:03, 983.92it/s]warmup run: 1619it [00:03, 971.50it/s]warmup run: 1620it [00:03, 1011.57it/s]warmup run: 1651it [00:03, 982.40it/s]warmup run: 1336it [00:02, 917.63it/s]warmup run: 1712it [00:03, 967.73it/s]warmup run: 1713it [00:03, 990.15it/s]warmup run: 1657it [00:03, 922.55it/s]warmup run: 1726it [00:03, 974.06it/s]warmup run: 1718it [00:03, 973.26it/s]warmup run: 1725it [00:03, 1022.79it/s]warmup run: 1751it [00:03, 984.93it/s]warmup run: 1439it [00:03, 948.23it/s]warmup run: 1811it [00:03, 973.89it/s]warmup run: 1814it [00:03, 993.80it/s]warmup run: 1755it [00:03, 938.40it/s]warmup run: 1825it [00:03, 965.80it/s]warmup run: 1817it [00:03, 959.81it/s]warmup run: 1829it [00:03, 1025.23it/s]warmup run: 1851it [00:03, 987.26it/s]warmup run: 1542it [00:03, 969.90it/s]warmup run: 1910it [00:03, 977.36it/s]warmup run: 1915it [00:03, 994.52it/s]warmup run: 1851it [00:03, 942.60it/s]warmup run: 1923it [00:03, 965.48it/s]warmup run: 1914it [00:03, 955.22it/s]warmup run: 1933it [00:03, 1024.01it/s]warmup run: 1951it [00:03, 988.93it/s]warmup run: 1643it [00:03, 980.95it/s]warmup run: 2012it [00:03, 988.01it/s]warmup run: 2018it [00:03, 1003.25it/s]warmup run: 1947it [00:03, 946.02it/s]warmup run: 2026it [00:03, 983.15it/s]warmup run: 2015it [00:03, 970.79it/s]warmup run: 2041it [00:03, 1039.74it/s]warmup run: 2062it [00:03, 1022.76it/s]warmup run: 1743it [00:03, 984.94it/s]warmup run: 2130it [00:03, 1044.82it/s]warmup run: 2137it [00:03, 1056.66it/s]warmup run: 2053it [00:03, 979.38it/s]warmup run: 2146it [00:03, 1045.78it/s]warmup run: 2134it [00:03, 1033.35it/s]warmup run: 2161it [00:03, 1086.53it/s]warmup run: 2184it [00:03, 1081.00it/s]warmup run: 1844it [00:03, 990.55it/s]warmup run: 2249it [00:03, 1086.79it/s]warmup run: 2257it [00:03, 1097.04it/s]warmup run: 2173it [00:03, 1042.63it/s]warmup run: 2264it [00:03, 1084.36it/s]warmup run: 2253it [00:03, 1077.53it/s]warmup run: 2281it [00:03, 1119.96it/s]warmup run: 2306it [00:03, 1121.07it/s]warmup run: 1944it [00:03, 991.79it/s]warmup run: 2369it [00:03, 1117.93it/s]warmup run: 2376it [00:03, 1123.99it/s]warmup run: 2292it [00:03, 1086.04it/s]warmup run: 2382it [00:03, 1111.04it/s]warmup run: 2372it [00:03, 1110.20it/s]warmup run: 2401it [00:03, 1143.53it/s]warmup run: 2428it [00:03, 1149.87it/s]warmup run: 2053it [00:03, 1019.28it/s]warmup run: 2481it [00:03, 1109.51it/s]warmup run: 2496it [00:03, 1144.75it/s]warmup run: 2412it [00:03, 1118.87it/s]warmup run: 2500it [00:04, 1131.44it/s]warmup run: 2491it [00:03, 1133.42it/s]warmup run: 2522it [00:03, 1160.79it/s]warmup run: 2550it [00:04, 1170.11it/s]warmup run: 2174it [00:03, 1075.45it/s]warmup run: 2600it [00:04, 1133.22it/s]warmup run: 2616it [00:04, 1158.66it/s]warmup run: 2532it [00:04, 1141.81it/s]warmup run: 2617it [00:04, 1142.86it/s]warmup run: 2610it [00:04, 1149.79it/s]warmup run: 2642it [00:04, 1170.42it/s]warmup run: 2671it [00:04, 1180.38it/s]warmup run: 2295it [00:03, 1115.50it/s]warmup run: 2718it [00:04, 1146.25it/s]warmup run: 2736it [00:04, 1168.86it/s]warmup run: 2651it [00:04, 1153.62it/s]warmup run: 2733it [00:04, 1147.54it/s]warmup run: 2728it [00:04, 1156.63it/s]warmup run: 2763it [00:04, 1179.82it/s]warmup run: 2794it [00:04, 1192.94it/s]warmup run: 2417it [00:03, 1144.41it/s]warmup run: 2838it [00:04, 1159.60it/s]warmup run: 2854it [00:04, 1170.98it/s]warmup run: 2771it [00:04, 1164.86it/s]warmup run: 2851it [00:04, 1154.98it/s]warmup run: 2847it [00:04, 1163.75it/s]warmup run: 2883it [00:04, 1184.60it/s]warmup run: 2917it [00:04, 1201.92it/s]warmup run: 2540it [00:04, 1167.84it/s]warmup run: 2958it [00:04, 1169.15it/s]warmup run: 2973it [00:04, 1176.56it/s]warmup run: 2891it [00:04, 1174.28it/s]warmup run: 3000it [00:04, 681.24it/s] warmup run: 2969it [00:04, 1159.52it/s]warmup run: 3000it [00:04, 685.11it/s] warmup run: 2966it [00:04, 1170.28it/s]warmup run: 3000it [00:04, 691.82it/s] warmup run: 3000it [00:04, 678.43it/s] warmup run: 3000it [00:04, 667.33it/s] warmup run: 3000it [00:04, 684.74it/s] warmup run: 2658it [00:04, 1171.19it/s]warmup run: 3000it [00:04, 668.81it/s] warmup run: 2779it [00:04, 1182.72it/s]warmup run: 2902it [00:04, 1194.58it/s]warmup run: 3000it [00:04, 670.38it/s] 

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]


warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1659.91it/s]warmup should be done:   5%|▌         | 161/3000 [00:00<00:01, 1609.00it/s]warmup should be done:   5%|▌         | 163/3000 [00:00<00:01, 1628.47it/s]warmup should be done:   5%|▌         | 161/3000 [00:00<00:01, 1609.62it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1648.58it/s]warmup should be done:   5%|▌         | 163/3000 [00:00<00:01, 1624.39it/s]warmup should be done:   5%|▌         | 163/3000 [00:00<00:01, 1621.25it/s]warmup should be done:   5%|▌         | 163/3000 [00:00<00:01, 1622.22it/s]warmup should be done:  11%|█         | 325/3000 [00:00<00:01, 1625.83it/s]warmup should be done:  11%|█         | 328/3000 [00:00<00:01, 1639.18it/s]warmup should be done:  11%|█         | 332/3000 [00:00<00:01, 1654.02it/s]warmup should be done:  11%|█         | 324/3000 [00:00<00:01, 1616.23it/s]warmup should be done:  11%|█         | 327/3000 [00:00<00:01, 1632.13it/s]warmup should be done:  11%|█         | 334/3000 [00:00<00:01, 1667.40it/s]warmup should be done:  11%|█         | 328/3000 [00:00<00:01, 1634.86it/s]warmup should be done:  11%|█         | 328/3000 [00:00<00:01, 1634.26it/s]warmup should be done:  17%|█▋        | 501/3000 [00:00<00:01, 1668.26it/s]warmup should be done:  16%|█▌        | 487/3000 [00:00<00:01, 1620.05it/s]warmup should be done:  17%|█▋        | 498/3000 [00:00<00:01, 1652.42it/s]warmup should be done:  16%|█▋        | 492/3000 [00:00<00:01, 1633.16it/s]warmup should be done:  16%|█▋        | 492/3000 [00:00<00:01, 1635.54it/s]warmup should be done:  16%|█▋        | 491/3000 [00:00<00:01, 1631.43it/s]warmup should be done:  16%|█▋        | 492/3000 [00:00<00:01, 1634.15it/s]warmup should be done:  16%|█▋        | 488/3000 [00:00<00:01, 1585.72it/s]warmup should be done:  22%|██▏       | 668/3000 [00:00<00:01, 1668.38it/s]warmup should be done:  22%|██▏       | 664/3000 [00:00<00:01, 1653.03it/s]warmup should be done:  22%|██▏       | 650/3000 [00:00<00:01, 1621.35it/s]warmup should be done:  22%|██▏       | 656/3000 [00:00<00:01, 1631.40it/s]warmup should be done:  22%|██▏       | 655/3000 [00:00<00:01, 1629.73it/s]warmup should be done:  22%|██▏       | 656/3000 [00:00<00:01, 1619.69it/s]warmup should be done:  22%|██▏       | 656/3000 [00:00<00:01, 1603.14it/s]warmup should be done:  22%|██▏       | 647/3000 [00:00<00:01, 1580.55it/s]warmup should be done:  28%|██▊       | 835/3000 [00:00<00:01, 1665.99it/s]warmup should be done:  28%|██▊       | 830/3000 [00:00<00:01, 1652.09it/s]warmup should be done:  27%|██▋       | 813/3000 [00:00<00:01, 1618.44it/s]warmup should be done:  27%|██▋       | 820/3000 [00:00<00:01, 1629.93it/s]warmup should be done:  27%|██▋       | 818/3000 [00:00<00:01, 1626.62it/s]warmup should be done:  27%|██▋       | 819/3000 [00:00<00:01, 1609.96it/s]warmup should be done:  27%|██▋       | 806/3000 [00:00<00:01, 1580.67it/s]warmup should be done:  27%|██▋       | 818/3000 [00:00<00:01, 1594.02it/s]warmup should be done:  33%|███▎      | 1002/3000 [00:00<00:01, 1662.17it/s]warmup should be done:  32%|███▎      | 975/3000 [00:00<00:01, 1614.65it/s]warmup should be done:  33%|███▎      | 981/3000 [00:00<00:01, 1621.94it/s]warmup should be done:  33%|███▎      | 983/3000 [00:00<00:01, 1621.93it/s]warmup should be done:  33%|███▎      | 996/3000 [00:00<00:01, 1635.31it/s]warmup should be done:  32%|███▏      | 965/3000 [00:00<00:01, 1582.98it/s]warmup should be done:  33%|███▎      | 982/3000 [00:00<00:01, 1606.34it/s]warmup should be done:  33%|███▎      | 981/3000 [00:00<00:01, 1597.27it/s]warmup should be done:  39%|███▉      | 1169/3000 [00:00<00:01, 1663.10it/s]warmup should be done:  38%|███▊      | 1137/3000 [00:00<00:01, 1615.23it/s]warmup should be done:  38%|███▊      | 1144/3000 [00:00<00:01, 1621.47it/s]warmup should be done:  38%|███▊      | 1146/3000 [00:00<00:01, 1620.43it/s]warmup should be done:  39%|███▊      | 1161/3000 [00:00<00:01, 1638.55it/s]warmup should be done:  38%|███▊      | 1125/3000 [00:00<00:01, 1588.12it/s]warmup should be done:  38%|███▊      | 1148/3000 [00:00<00:01, 1621.37it/s]warmup should be done:  38%|███▊      | 1142/3000 [00:00<00:01, 1599.38it/s]warmup should be done:  45%|████▍     | 1336/3000 [00:00<00:01, 1662.59it/s]warmup should be done:  43%|████▎     | 1299/3000 [00:00<00:01, 1615.12it/s]warmup should be done:  44%|████▎     | 1307/3000 [00:00<00:01, 1619.69it/s]warmup should be done:  44%|████▎     | 1309/3000 [00:00<00:01, 1619.27it/s]warmup should be done:  44%|████▍     | 1326/3000 [00:00<00:01, 1641.53it/s]warmup should be done:  43%|████▎     | 1286/3000 [00:00<00:01, 1592.80it/s]warmup should be done:  44%|████▍     | 1314/3000 [00:00<00:01, 1632.24it/s]warmup should be done:  43%|████▎     | 1303/3000 [00:00<00:01, 1601.28it/s]warmup should be done:  50%|█████     | 1503/3000 [00:00<00:00, 1662.53it/s]warmup should be done:  49%|████▊     | 1461/3000 [00:00<00:00, 1614.05it/s]warmup should be done:  49%|████▉     | 1469/3000 [00:00<00:00, 1618.42it/s]warmup should be done:  50%|████▉     | 1491/3000 [00:00<00:00, 1643.11it/s]warmup should be done:  49%|████▉     | 1472/3000 [00:00<00:00, 1619.93it/s]warmup should be done:  48%|████▊     | 1447/3000 [00:00<00:00, 1596.27it/s]warmup should be done:  49%|████▉     | 1480/3000 [00:00<00:00, 1639.94it/s]warmup should be done:  49%|████▉     | 1466/3000 [00:00<00:00, 1607.64it/s]warmup should be done:  56%|█████▌    | 1670/3000 [00:01<00:00, 1663.32it/s]warmup should be done:  54%|█████▍    | 1623/3000 [00:01<00:00, 1614.31it/s]warmup should be done:  54%|█████▍    | 1631/3000 [00:01<00:00, 1617.68it/s]warmup should be done:  55%|█████▌    | 1656/3000 [00:01<00:00, 1643.74it/s]warmup should be done:  55%|█████▍    | 1636/3000 [00:01<00:00, 1624.48it/s]warmup should be done:  54%|█████▎    | 1608/3000 [00:01<00:00, 1598.45it/s]warmup should be done:  55%|█████▍    | 1646/3000 [00:01<00:00, 1645.06it/s]warmup should be done:  54%|█████▍    | 1629/3000 [00:01<00:00, 1611.68it/s]warmup should be done:  61%|██████    | 1837/3000 [00:01<00:00, 1662.62it/s]warmup should be done:  60%|█████▉    | 1785/3000 [00:01<00:00, 1615.51it/s]warmup should be done:  60%|█████▉    | 1793/3000 [00:01<00:00, 1617.24it/s]warmup should be done:  61%|██████    | 1821/3000 [00:01<00:00, 1644.23it/s]warmup should be done:  60%|██████    | 1800/3000 [00:01<00:00, 1627.39it/s]warmup should be done:  59%|█████▉    | 1769/3000 [00:01<00:00, 1600.36it/s]warmup should be done:  60%|██████    | 1812/3000 [00:01<00:00, 1648.70it/s]warmup should be done:  60%|█████▉    | 1792/3000 [00:01<00:00, 1614.52it/s]warmup should be done:  67%|██████▋   | 2004/3000 [00:01<00:00, 1663.29it/s]warmup should be done:  65%|██████▍   | 1947/3000 [00:01<00:00, 1615.23it/s]warmup should be done:  65%|██████▌   | 1956/3000 [00:01<00:00, 1618.14it/s]warmup should be done:  66%|██████▌   | 1986/3000 [00:01<00:00, 1643.41it/s]warmup should be done:  65%|██████▌   | 1964/3000 [00:01<00:00, 1629.39it/s]warmup should be done:  64%|██████▍   | 1930/3000 [00:01<00:00, 1601.08it/s]warmup should be done:  66%|██████▌   | 1978/3000 [00:01<00:00, 1651.05it/s]warmup should be done:  65%|██████▌   | 1954/3000 [00:01<00:00, 1615.98it/s]warmup should be done:  72%|███████▏  | 2171/3000 [00:01<00:00, 1662.48it/s]warmup should be done:  70%|███████   | 2109/3000 [00:01<00:00, 1615.49it/s]warmup should be done:  71%|███████   | 2119/3000 [00:01<00:00, 1618.77it/s]warmup should be done:  72%|███████▏  | 2151/3000 [00:01<00:00, 1643.03it/s]warmup should be done:  71%|███████   | 2127/3000 [00:01<00:00, 1619.18it/s]warmup should be done:  70%|██████▉   | 2091/3000 [00:01<00:00, 1601.16it/s]warmup should be done:  71%|███████▏  | 2144/3000 [00:01<00:00, 1652.10it/s]warmup should be done:  71%|███████   | 2117/3000 [00:01<00:00, 1618.20it/s]warmup should be done:  78%|███████▊  | 2338/3000 [00:01<00:00, 1659.81it/s]warmup should be done:  76%|███████▌  | 2271/3000 [00:01<00:00, 1612.24it/s]warmup should be done:  76%|███████▌  | 2281/3000 [00:01<00:00, 1613.68it/s]warmup should be done:  77%|███████▋  | 2316/3000 [00:01<00:00, 1641.46it/s]warmup should be done:  76%|███████▋  | 2290/3000 [00:01<00:00, 1622.17it/s]warmup should be done:  75%|███████▌  | 2252/3000 [00:01<00:00, 1599.96it/s]warmup should be done:  77%|███████▋  | 2310/3000 [00:01<00:00, 1650.71it/s]warmup should be done:  76%|███████▌  | 2279/3000 [00:01<00:00, 1618.70it/s]warmup should be done:  81%|████████  | 2433/3000 [00:01<00:00, 1612.41it/s]warmup should be done:  84%|████████▎ | 2505/3000 [00:01<00:00, 1660.11it/s]warmup should be done:  83%|████████▎ | 2481/3000 [00:01<00:00, 1643.64it/s]warmup should be done:  81%|████████▏ | 2443/3000 [00:01<00:00, 1613.30it/s]warmup should be done:  82%|████████▏ | 2455/3000 [00:01<00:00, 1628.75it/s]warmup should be done:  80%|████████  | 2413/3000 [00:01<00:00, 1601.54it/s]warmup should be done:  83%|████████▎ | 2476/3000 [00:01<00:00, 1650.93it/s]warmup should be done:  81%|████████▏ | 2442/3000 [00:01<00:00, 1621.77it/s]warmup should be done:  86%|████████▋ | 2595/3000 [00:01<00:00, 1612.21it/s]warmup should be done:  89%|████████▉ | 2672/3000 [00:01<00:00, 1659.44it/s]warmup should be done:  88%|████████▊ | 2647/3000 [00:01<00:00, 1645.86it/s]warmup should be done:  87%|████████▋ | 2605/3000 [00:01<00:00, 1612.17it/s]warmup should be done:  87%|████████▋ | 2621/3000 [00:01<00:00, 1635.41it/s]warmup should be done:  86%|████████▌ | 2574/3000 [00:01<00:00, 1603.36it/s]warmup should be done:  88%|████████▊ | 2642/3000 [00:01<00:00, 1652.96it/s]warmup should be done:  87%|████████▋ | 2606/3000 [00:01<00:00, 1626.73it/s]warmup should be done:  92%|█████████▏| 2757/3000 [00:01<00:00, 1612.47it/s]warmup should be done:  95%|█████████▍| 2839/3000 [00:01<00:00, 1661.70it/s]warmup should be done:  92%|█████████▏| 2767/3000 [00:01<00:00, 1612.96it/s]warmup should be done:  94%|█████████▎| 2812/3000 [00:01<00:00, 1639.64it/s]warmup should be done:  93%|█████████▎| 2787/3000 [00:01<00:00, 1641.20it/s]warmup should be done:  91%|█████████ | 2736/3000 [00:01<00:00, 1605.75it/s]warmup should be done:  94%|█████████▎| 2809/3000 [00:01<00:00, 1655.47it/s]warmup should be done:  92%|█████████▏| 2770/3000 [00:01<00:00, 1629.68it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1663.45it/s]warmup should be done:  97%|█████████▋| 2921/3000 [00:01<00:00, 1620.69it/s]warmup should be done:  98%|█████████▊| 2931/3000 [00:01<00:00, 1618.58it/s]warmup should be done:  98%|█████████▊| 2955/3000 [00:01<00:00, 1650.35it/s]warmup should be done:  99%|█████████▉| 2976/3000 [00:01<00:00, 1636.50it/s]warmup should be done:  97%|█████████▋| 2899/3000 [00:01<00:00, 1611.63it/s]warmup should be done:  99%|█████████▉| 2977/3000 [00:01<00:00, 1661.24it/s]warmup should be done:  98%|█████████▊| 2936/3000 [00:01<00:00, 1637.38it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1642.63it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1642.00it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1631.91it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1619.48it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1619.25it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1616.55it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1599.78it/s]






warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|▌         | 169/3000 [00:00<00:01, 1689.60it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1659.32it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1658.60it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1654.49it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1652.69it/s]warmup should be done:   6%|▌         | 171/3000 [00:00<00:01, 1701.58it/s]warmup should be done:   6%|▌         | 171/3000 [00:00<00:01, 1701.22it/s]warmup should be done:   6%|▌         | 169/3000 [00:00<00:01, 1680.45it/s]warmup should be done:  11%|█         | 332/3000 [00:00<00:01, 1657.50it/s]warmup should be done:  11%|█         | 333/3000 [00:00<00:01, 1661.96it/s]warmup should be done:  11%|█▏        | 340/3000 [00:00<00:01, 1695.90it/s]warmup should be done:  11%|█▏        | 342/3000 [00:00<00:01, 1705.24it/s]warmup should be done:  11%|█         | 332/3000 [00:00<00:01, 1651.78it/s]warmup should be done:  11%|█         | 333/3000 [00:00<00:01, 1657.25it/s]warmup should be done:  11%|█▏        | 338/3000 [00:00<00:01, 1681.81it/s]warmup should be done:  11%|█▏        | 343/3000 [00:00<00:01, 1707.30it/s]warmup should be done:  17%|█▋        | 511/3000 [00:00<00:01, 1701.22it/s]warmup should be done:  17%|█▋        | 501/3000 [00:00<00:01, 1668.36it/s]warmup should be done:  17%|█▋        | 514/3000 [00:00<00:01, 1710.38it/s]warmup should be done:  17%|█▋        | 500/3000 [00:00<00:01, 1662.35it/s]warmup should be done:  17%|█▋        | 498/3000 [00:00<00:01, 1653.64it/s]warmup should be done:  17%|█▋        | 498/3000 [00:00<00:01, 1652.16it/s]warmup should be done:  17%|█▋        | 507/3000 [00:00<00:01, 1683.35it/s]warmup should be done:  17%|█▋        | 514/3000 [00:00<00:01, 1685.54it/s]warmup should be done:  22%|██▏       | 669/3000 [00:00<00:01, 1671.88it/s]warmup should be done:  23%|██▎       | 682/3000 [00:00<00:01, 1701.32it/s]warmup should be done:  23%|██▎       | 686/3000 [00:00<00:01, 1709.81it/s]warmup should be done:  23%|██▎       | 676/3000 [00:00<00:01, 1684.92it/s]warmup should be done:  22%|██▏       | 665/3000 [00:00<00:01, 1657.40it/s]warmup should be done:  22%|██▏       | 668/3000 [00:00<00:01, 1665.97it/s]warmup should be done:  22%|██▏       | 664/3000 [00:00<00:01, 1650.73it/s]warmup should be done:  23%|██▎       | 684/3000 [00:00<00:01, 1690.03it/s]warmup should be done:  28%|██▊       | 845/3000 [00:00<00:01, 1686.54it/s]warmup should be done:  28%|██▊       | 835/3000 [00:00<00:01, 1667.20it/s]warmup should be done:  28%|██▊       | 831/3000 [00:00<00:01, 1657.78it/s]warmup should be done:  28%|██▊       | 837/3000 [00:00<00:01, 1669.98it/s]warmup should be done:  29%|██▊       | 857/3000 [00:00<00:01, 1707.90it/s]warmup should be done:  28%|██▊       | 853/3000 [00:00<00:01, 1699.54it/s]warmup should be done:  28%|██▊       | 830/3000 [00:00<00:01, 1644.20it/s]warmup should be done:  28%|██▊       | 854/3000 [00:00<00:01, 1690.78it/s]warmup should be done:  34%|███▍      | 1028/3000 [00:00<00:01, 1708.04it/s]warmup should be done:  34%|███▍      | 1014/3000 [00:00<00:01, 1685.42it/s]warmup should be done:  33%|███▎      | 1004/3000 [00:00<00:01, 1668.46it/s]warmup should be done:  33%|███▎      | 1003/3000 [00:00<00:01, 1668.80it/s]warmup should be done:  34%|███▍      | 1023/3000 [00:00<00:01, 1698.73it/s]warmup should be done:  33%|███▎      | 997/3000 [00:00<00:01, 1655.13it/s]warmup should be done:  33%|███▎      | 995/3000 [00:00<00:01, 1641.25it/s]warmup should be done:  34%|███▍      | 1024/3000 [00:00<00:01, 1690.21it/s]warmup should be done:  39%|███▉      | 1171/3000 [00:00<00:01, 1667.72it/s]warmup should be done:  40%|███▉      | 1199/3000 [00:00<00:01, 1706.19it/s]warmup should be done:  39%|███▉      | 1163/3000 [00:00<00:01, 1654.53it/s]warmup should be done:  39%|███▉      | 1171/3000 [00:00<00:01, 1669.32it/s]warmup should be done:  40%|███▉      | 1193/3000 [00:00<00:01, 1694.65it/s]warmup should be done:  39%|███▉      | 1183/3000 [00:00<00:01, 1680.51it/s]warmup should be done:  39%|███▊      | 1160/3000 [00:00<00:01, 1640.81it/s]warmup should be done:  40%|███▉      | 1194/3000 [00:00<00:01, 1688.10it/s]warmup should be done:  46%|████▌     | 1370/3000 [00:00<00:00, 1706.56it/s]warmup should be done:  45%|████▍     | 1339/3000 [00:00<00:00, 1669.08it/s]warmup should be done:  45%|████▍     | 1339/3000 [00:00<00:00, 1669.76it/s]warmup should be done:  45%|████▌     | 1352/3000 [00:00<00:00, 1682.51it/s]warmup should be done:  44%|████▍     | 1329/3000 [00:00<00:01, 1652.48it/s]warmup should be done:  45%|████▌     | 1363/3000 [00:00<00:00, 1693.68it/s]warmup should be done:  45%|████▌     | 1363/3000 [00:00<00:00, 1688.15it/s]warmup should be done:  44%|████▍     | 1325/3000 [00:00<00:01, 1636.42it/s]warmup should be done:  51%|█████▏    | 1541/3000 [00:00<00:00, 1706.36it/s]warmup should be done:  50%|█████     | 1507/3000 [00:00<00:00, 1670.30it/s]warmup should be done:  50%|█████     | 1506/3000 [00:00<00:00, 1669.80it/s]warmup should be done:  50%|████▉     | 1495/3000 [00:00<00:00, 1653.57it/s]warmup should be done:  51%|█████     | 1521/3000 [00:00<00:00, 1682.38it/s]warmup should be done:  51%|█████     | 1533/3000 [00:00<00:00, 1687.09it/s]warmup should be done:  51%|█████     | 1535/3000 [00:00<00:00, 1697.01it/s]warmup should be done:  50%|████▉     | 1490/3000 [00:00<00:00, 1638.06it/s]warmup should be done:  57%|█████▋    | 1712/3000 [00:01<00:00, 1706.14it/s]warmup should be done:  56%|█████▌    | 1675/3000 [00:01<00:00, 1672.90it/s]warmup should be done:  55%|█████▌    | 1661/3000 [00:01<00:00, 1655.33it/s]warmup should be done:  56%|█████▋    | 1690/3000 [00:01<00:00, 1682.70it/s]warmup should be done:  56%|█████▌    | 1673/3000 [00:01<00:00, 1660.15it/s]warmup should be done:  57%|█████▋    | 1702/3000 [00:01<00:00, 1682.86it/s]warmup should be done:  57%|█████▋    | 1707/3000 [00:01<00:00, 1703.74it/s]warmup should be done:  55%|█████▌    | 1655/3000 [00:01<00:00, 1640.63it/s]warmup should be done:  63%|██████▎   | 1883/3000 [00:01<00:00, 1706.95it/s]warmup should be done:  61%|██████▏   | 1843/3000 [00:01<00:00, 1673.60it/s]warmup should be done:  61%|██████    | 1828/3000 [00:01<00:00, 1656.71it/s]warmup should be done:  62%|██████▏   | 1859/3000 [00:01<00:00, 1682.41it/s]warmup should be done:  61%|██████▏   | 1842/3000 [00:01<00:00, 1666.43it/s]warmup should be done:  63%|██████▎   | 1879/3000 [00:01<00:00, 1708.54it/s]warmup should be done:  62%|██████▏   | 1871/3000 [00:01<00:00, 1681.32it/s]warmup should be done:  61%|██████    | 1820/3000 [00:01<00:00, 1640.62it/s]warmup should be done:  68%|██████▊   | 2054/3000 [00:01<00:00, 1706.04it/s]warmup should be done:  67%|██████▋   | 2011/3000 [00:01<00:00, 1673.03it/s]warmup should be done:  66%|██████▋   | 1994/3000 [00:01<00:00, 1653.65it/s]warmup should be done:  68%|██████▊   | 2028/3000 [00:01<00:00, 1681.21it/s]warmup should be done:  67%|██████▋   | 2012/3000 [00:01<00:00, 1673.76it/s]warmup should be done:  68%|██████▊   | 2052/3000 [00:01<00:00, 1712.78it/s]warmup should be done:  68%|██████▊   | 2040/3000 [00:01<00:00, 1680.14it/s]warmup should be done:  66%|██████▌   | 1985/3000 [00:01<00:00, 1638.16it/s]warmup should be done:  74%|███████▍  | 2225/3000 [00:01<00:00, 1705.29it/s]warmup should be done:  73%|███████▎  | 2179/3000 [00:01<00:00, 1671.67it/s]warmup should be done:  72%|███████▏  | 2160/3000 [00:01<00:00, 1653.02it/s]warmup should be done:  73%|███████▎  | 2197/3000 [00:01<00:00, 1680.60it/s]warmup should be done:  73%|███████▎  | 2181/3000 [00:01<00:00, 1678.51it/s]warmup should be done:  74%|███████▍  | 2224/3000 [00:01<00:00, 1713.25it/s]warmup should be done:  74%|███████▎  | 2209/3000 [00:01<00:00, 1678.79it/s]warmup should be done:  72%|███████▏  | 2149/3000 [00:01<00:00, 1637.26it/s]warmup should be done:  80%|███████▉  | 2396/3000 [00:01<00:00, 1705.60it/s]warmup should be done:  78%|███████▊  | 2347/3000 [00:01<00:00, 1671.51it/s]warmup should be done:  79%|███████▉  | 2366/3000 [00:01<00:00, 1682.64it/s]warmup should be done:  78%|███████▊  | 2327/3000 [00:01<00:00, 1655.28it/s]warmup should be done:  78%|███████▊  | 2351/3000 [00:01<00:00, 1683.18it/s]warmup should be done:  79%|███████▉  | 2377/3000 [00:01<00:00, 1679.05it/s]warmup should be done:  77%|███████▋  | 2314/3000 [00:01<00:00, 1638.21it/s]warmup should be done:  80%|███████▉  | 2396/3000 [00:01<00:00, 1698.24it/s]warmup should be done:  86%|████████▌ | 2568/3000 [00:01<00:00, 1708.52it/s]warmup should be done:  84%|████████▍ | 2515/3000 [00:01<00:00, 1673.47it/s]warmup should be done:  85%|████████▍ | 2537/3000 [00:01<00:00, 1689.82it/s]warmup should be done:  83%|████████▎ | 2494/3000 [00:01<00:00, 1657.11it/s]warmup should be done:  84%|████████▍ | 2521/3000 [00:01<00:00, 1686.31it/s]warmup should be done:  85%|████████▍ | 2546/3000 [00:01<00:00, 1681.12it/s]warmup should be done:  83%|████████▎ | 2478/3000 [00:01<00:00, 1637.75it/s]warmup should be done:  86%|████████▌ | 2566/3000 [00:01<00:00, 1696.70it/s]warmup should be done:  91%|█████████▏| 2740/3000 [00:01<00:00, 1709.76it/s]warmup should be done:  89%|████████▉ | 2683/3000 [00:01<00:00, 1671.92it/s]warmup should be done:  90%|█████████ | 2708/3000 [00:01<00:00, 1693.78it/s]warmup should be done:  90%|████████▉ | 2690/3000 [00:01<00:00, 1687.09it/s]warmup should be done:  89%|████████▊ | 2661/3000 [00:01<00:00, 1658.56it/s]warmup should be done:  91%|█████████ | 2736/3000 [00:01<00:00, 1696.95it/s]warmup should be done:  90%|█████████ | 2715/3000 [00:01<00:00, 1655.42it/s]warmup should be done:  88%|████████▊ | 2642/3000 [00:01<00:00, 1606.58it/s]warmup should be done:  97%|█████████▋| 2911/3000 [00:01<00:00, 1708.78it/s]warmup should be done:  95%|█████████▌| 2851/3000 [00:01<00:00, 1670.15it/s]warmup should be done:  94%|█████████▍| 2828/3000 [00:01<00:00, 1661.81it/s]warmup should be done:  95%|█████████▌| 2859/3000 [00:01<00:00, 1686.67it/s]warmup should be done:  96%|█████████▌| 2878/3000 [00:01<00:00, 1683.34it/s]warmup should be done:  97%|█████████▋| 2906/3000 [00:01<00:00, 1694.69it/s]warmup should be done:  94%|█████████▎| 2806/3000 [00:01<00:00, 1615.04it/s]warmup should be done:  96%|█████████▌| 2881/3000 [00:01<00:00, 1644.72it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1707.13it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1697.16it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1683.10it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1675.40it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1674.72it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1670.58it/s]warmup should be done: 100%|█████████▉| 2995/3000 [00:01<00:00, 1663.72it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1656.73it/s]warmup should be done:  99%|█████████▉| 2971/3000 [00:01<00:00, 1623.19it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1634.54it/s]2022-12-11 20:40:46.750841: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7ef264028c60 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 20:40:46.750905: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 20:40:46.764612: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f0df7832e50 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 20:40:46.764677: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 20:40:46.780270: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7ef25c02ce30 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 20:40:46.780324: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 20:40:47.006514: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f0df782f8a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 20:40:47.006583: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 20:40:47.383396: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f0dff82c090 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 20:40:47.383466: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 20:40:47.386460: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f0df78304b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 20:40:47.386508: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 20:40:47.396693: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f0df3f91fe0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 20:40:47.396768: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 20:40:47.425546: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7ef27c02fa10 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 20:40:47.425616: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 20:40:49.009931: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 20:40:49.043020: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 20:40:49.058635: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 20:40:49.280848: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 20:40:49.669010: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 20:40:49.680452: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 20:40:49.708957: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 20:40:49.735008: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 20:40:51.897725: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 20:40:51.903024: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 20:40:51.968722: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 20:40:52.138634: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 20:40:52.544923: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 20:40:52.595909: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 20:40:52.602201: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 20:40:52.612828: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][20:41:36.009][ERROR][RK0][tid #139698878002944]: replica 0 reaches 1000, calling init pre replica
[HCTR][20:41:36.009][ERROR][RK0][tid #139698878002944]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][20:41:36.009][ERROR][RK0][main]: replica 6 reaches 1000, calling init pre replica
[HCTR][20:41:36.010][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][20:41:36.015][ERROR][RK0][main]: coll ps creation done
[HCTR][20:41:36.015][ERROR][RK0][main]: replica 6 waits for coll ps creation barrier
[HCTR][20:41:36.018][ERROR][RK0][tid #139698878002944]: coll ps creation done
[HCTR][20:41:36.018][ERROR][RK0][tid #139698878002944]: replica 0 waits for coll ps creation barrier
[HCTR][20:41:36.028][ERROR][RK0][tid #139698861217536]: replica 1 reaches 1000, calling init pre replica
[HCTR][20:41:36.028][ERROR][RK0][tid #139698861217536]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][20:41:36.029][ERROR][RK0][main]: replica 5 reaches 1000, calling init pre replica
[HCTR][20:41:36.029][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][20:41:36.029][ERROR][RK0][main]: replica 7 reaches 1000, calling init pre replica
[HCTR][20:41:36.029][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][20:41:36.036][ERROR][RK0][tid #139698861217536]: coll ps creation done
[HCTR][20:41:36.036][ERROR][RK0][tid #139698861217536]: replica 1 waits for coll ps creation barrier
[HCTR][20:41:36.036][ERROR][RK0][main]: coll ps creation done
[HCTR][20:41:36.036][ERROR][RK0][main]: replica 7 waits for coll ps creation barrier
[HCTR][20:41:36.037][ERROR][RK0][main]: coll ps creation done
[HCTR][20:41:36.037][ERROR][RK0][main]: replica 5 waits for coll ps creation barrier
[HCTR][20:41:36.046][ERROR][RK0][tid #139698408240896]: replica 4 reaches 1000, calling init pre replica
[HCTR][20:41:36.046][ERROR][RK0][tid #139698408240896]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][20:41:36.051][ERROR][RK0][tid #139698408240896]: coll ps creation done
[HCTR][20:41:36.051][ERROR][RK0][tid #139698408240896]: replica 4 waits for coll ps creation barrier
[HCTR][20:41:36.095][ERROR][RK0][main]: replica 3 reaches 1000, calling init pre replica
[HCTR][20:41:36.095][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][20:41:36.100][ERROR][RK0][main]: coll ps creation done
[HCTR][20:41:36.100][ERROR][RK0][main]: replica 3 waits for coll ps creation barrier
[HCTR][20:41:36.112][ERROR][RK0][tid #139698408240896]: replica 2 reaches 1000, calling init pre replica
[HCTR][20:41:36.112][ERROR][RK0][tid #139698408240896]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][20:41:36.117][ERROR][RK0][tid #139698408240896]: coll ps creation done
[HCTR][20:41:36.117][ERROR][RK0][tid #139698408240896]: replica 2 waits for coll ps creation barrier
[HCTR][20:41:36.117][ERROR][RK0][tid #139698878002944]: replica 0 preparing frequency
[HCTR][20:41:36.997][ERROR][RK0][tid #139698878002944]: replica 0 preparing frequency done
[HCTR][20:41:37.028][ERROR][RK0][tid #139698878002944]: replica 0 calling init per replica
[HCTR][20:41:37.028][ERROR][RK0][main]: replica 7 calling init per replica
[HCTR][20:41:37.028][ERROR][RK0][main]: replica 3 calling init per replica
[HCTR][20:41:37.028][ERROR][RK0][tid #139698861217536]: replica 1 calling init per replica
[HCTR][20:41:37.028][ERROR][RK0][main]: replica 5 calling init per replica
[HCTR][20:41:37.028][ERROR][RK0][tid #139698408240896]: replica 2 calling init per replica
[HCTR][20:41:37.028][ERROR][RK0][main]: replica 6 calling init per replica
[HCTR][20:41:37.028][ERROR][RK0][tid #139698408240896]: replica 4 calling init per replica
[HCTR][20:41:37.028][ERROR][RK0][tid #139698878002944]: Calling build_v2
[HCTR][20:41:37.028][ERROR][RK0][main]: Calling build_v2
[HCTR][20:41:37.028][ERROR][RK0][main]: Calling build_v2
[HCTR][20:41:37.028][ERROR][RK0][tid #139698861217536]: Calling build_v2
[HCTR][20:41:37.028][ERROR][RK0][main]: Calling build_v2
[HCTR][20:41:37.028][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][20:41:37.028][ERROR][RK0][tid #139698408240896]: Calling build_v2
[HCTR][20:41:37.028][ERROR][RK0][main]: Calling build_v2
[HCTR][20:41:37.028][ERROR][RK0][tid #139698878002944]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][20:41:37.028][ERROR][RK0][tid #139698408240896]: Calling build_v2
[HCTR][20:41:37.028][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][20:41:37.028][ERROR][RK0][tid #139698861217536]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][20:41:37.028][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][20:41:37.028][ERROR][RK0][tid #139698408240896]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][20:41:37.028][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][20:41:37.028][ERROR][RK0][tid #139698408240896]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[[[[2022-12-11 20:41:37[[[2022-12-11 20:41:37[2022-12-11 20:41:37.2022-12-11 20:41:37.2022-12-11 20:41:372022-12-11 20:41:372022-12-11 20:41:37. 284512022-12-11 20:41:37. 28455... 28446: . 28467:  28458 28467 28467: E 28467: E: : : E : E EEE /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.ccE /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc   /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc: /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:136/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:136:::136] :136] 136136136] using concurrent impl MPSPhase136] using concurrent impl MPSPhase] ] ] using concurrent impl MPSPhase
] using concurrent impl MPSPhase
using concurrent impl MPSPhaseusing concurrent impl MPSPhaseusing concurrent impl MPSPhase
using concurrent impl MPSPhase




[2022-12-11 20:41:37. 32655: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-11 20:41:37. 32695[: 2022-12-11 20:41:37E.  32696/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: :E196 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccassigning 8 to cpu:
178] v100x8, slow pcie
[2022-12-11 20:41:37.[ 327402022-12-11 20:41:37: .E 32751 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE: 178/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] :v100x8, slow pcie196[
] [2022-12-11 20:41:37assigning 8 to cpu2022-12-11 20:41:37[.
.2022-12-11 20:41:37 32784 32784.: :  32798EE:  [ E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 20:41:37/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc :.[:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc212 328272022-12-11 20:41:37178:] : .] 196build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8E[ 32835v100x8, slow pcie] 
 2022-12-11 20:41:37: 
assigning 8 to cpu/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.E
[[: 32880[ 2022-12-11 20:41:372022-12-11 20:41:37212: 2022-12-11 20:41:37/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc..] E.:[ 32924 32926[build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8  329271782022-12-11 20:41:37: : 2022-12-11 20:41:37
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: ] .EE.:Ev100x8, slow pcie 32969 [  32969178 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 20:41:37/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: ] :E:.[:Ev100x8, slow pcie178 213 330412022-12-11 20:41:37196 
] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] : .] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccv100x8, slow pcie:[remote time is 8.68421E 33087assigning 8 to cpu:
2122022-12-11 20:41:37
 : 
178] [./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE[] build asymm link desc with 8X Tesla V100-SXM2-32GB out of 82022-12-11 20:41:37 33151:[ 2022-12-11 20:41:37v100x8, slow pcie
.: 2132022-12-11 20:41:37[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.
 33208E] [.2022-12-11 20:41:37: 33220:  remote time is 8.684212022-12-11 20:41:37 33245.196: E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
.:  33302] E : 33324E[: assigning 8 to cpu /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc196:  2022-12-11 20:41:37E
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:] E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc. :196assigning 8 to cpu : 33393/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc214] 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc212: :] [assigning 8 to cpu:] E213cpu time is 97.05882022-12-11 20:41:37
196build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8 ] 
.[] 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccremote time is 8.68421 334912022-12-11 20:41:37assigning 8 to cpu:[
: .[
2142022-12-11 20:41:37E 33538[2022-12-11 20:41:37] . : 2022-12-11 20:41:37.[cpu time is 97.0588 33582/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE. 335762022-12-11 20:41:37
: :  33600: .E212/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: E 33641 ] :E : /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccbuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8212 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE:
] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 212[build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8:213/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] 2022-12-11 20:41:37
214] :build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8.[] remote time is 8.68421212
 337582022-12-11 20:41:37cpu time is 97.0588
] : .
[build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8E[ 337932022-12-11 20:41:37
 2022-12-11 20:41:37: ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.E[ 33821: 33828 2022-12-11 20:41:37: 213: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.E] E: 33865 remote time is 8.68421 213: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] E::[remote time is 8.68421 2132142022-12-11 20:41:37
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] ] .:remote time is 8.68421[cpu time is 97.0588 33950213
2022-12-11 20:41:37
: ] .[Eremote time is 8.68421 339862022-12-11 20:41:37 
: ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE 34017:[ : 2142022-12-11 20:41:37/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE] .: cpu time is 97.0588 34044214/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
: ] :Ecpu time is 97.0588214 
] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cccpu time is 97.0588:
214] cpu time is 97.0588
[2022-12-11 20:42:54.326935: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-11 20:42:54.367191: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 381.47 MB
[2022-12-11 20:42:54.367277: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 381.47 MB
[2022-12-11 20:42:54.368308: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:74] mapping nid to rank...
[2022-12-11 20:42:54.440728: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:91] counting slots...
[2022-12-11 20:42:54.821235: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:105] Final num slot is 49
[2022-12-11 20:42:54.821331: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:109] counting blocks...
[2022-12-11 20:43:02. 56829: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:118] Final num block is 1024
[2022-12-11 20:43:02. 56924: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:123] counting freq and density...
[2022-12-11 20:43:03.775265: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:154] averaging freq and density...
[2022-12-11 20:43:03.775369: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:155] 1024
[2022-12-11 20:43:03.778333: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
[2022-12-11 20:43:03.778388: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:354] constructing optimal solver, device=8, stream=1
1024 blocks, 8 devices
[2022-12-11 20:43:04.114155: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:527] Add Var...
[2022-12-11 20:43:04.142867: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:545] Capacity...
[2022-12-11 20:43:04.144307: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:548] Connect CPU...
[2022-12-11 20:43:04.166556: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:550] Connect Access To Storage...
[2022-12-11 20:43:04.796813: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:554] Time...
[2022-12-11 20:43:04.799113: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 0, total sm is 80
[2022-12-11 20:43:04.802193: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 1, total sm is 80
[2022-12-11 20:43:04.805110: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 2, total sm is 80
[2022-12-11 20:43:04.807997: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 3, total sm is 80
[2022-12-11 20:43:04.810882: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 4, total sm is 80
[2022-12-11 20:43:04.813740: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 5, total sm is 80
[2022-12-11 20:43:04.816637: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 6, total sm is 80
[2022-12-11 20:43:04.819499: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 7, total sm is 80
[2022-12-11 20:43:15.281020: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:569] Coll Cache init block placement array
[2022-12-11 20:43:15.288197: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:645] Coll Cache init block placement array done
[2022-12-11 20:43:15.288800: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:647] Coll Cache model reset done
[2022-12-11 20:43:15.336072: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-11 20:43:15.336171: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-11 20:43:15.336201: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-11 20:43:15.336229: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-11 20:43:15.336808: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 20:43:15.336858: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:43:15.338006: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:43:15.338763: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:43:15.351576: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 1 solved
[2022-12-11 20:43:15.351646: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 1 initing device 1
[2022-12-11 20:43:15.351810: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc[:2022-12-11 20:43:15202.] 351842: 6 solvedE
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc[:2022-12-11 20:43:15202.] 3518872 solved: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205[] 2022-12-11 20:43:15worker 0 thread 6 initing device 6.
351920: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 2 initing device 2
[2022-12-11 20:43:15.352021: [E2022-12-11 20:43:15 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc352027:: [202E2022-12-11 20:43:15]  .5 solved/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc352085
:: 202[E] 2022-12-11 20:43:15 7 solved./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[
352142:2022-12-11 20:43:15: [1815.E2022-12-11 20:43:15[] 352151 .2022-12-11 20:43:15Building Coll Cache with ... num gpu device is 8: /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc352199.
E:: 352200 205E: /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc[]  E:2022-12-11 20:43:15worker 0 thread 5 initing device 5/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc 202.
:/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc] 352306205:[[4 solved: ] 2022022-12-11 20:43:152022-12-11 20:43:15
Eworker 0 thread 7 initing device 7] .. 
[3 solved352347352348/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-11 20:43:15
: : :.EE[1980352404  2022-12-11 20:43:15] : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.eager alloc mem 381.47 MBE::352448
 18151815: /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc] ] E:Building Coll Cache with ... num gpu device is 8Building Coll Cache with ... num gpu device is 8 205

/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc] :worker 0 thread 4 initing device 4205
] worker 0 thread 3 initing device 3[[
2022-12-11 20:43:152022-12-11 20:43:15..352595352596: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 381.47 MBeager alloc mem 381.47 MB

[2022-12-11 20:43:15.352729: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 20:43:15.352772: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:43:15.352842: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 20:43:15.352891: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:43:15.353006: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-11 20:43:15:.1815353025] : Building Coll Cache with ... num gpu device is 8E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 20:43:15.353069: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-11 20:43:15:.1980353082] : eager alloc mem 381.47 MBE
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:43:15.356976: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:43:15.357203: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:43:15.357254: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:43:15.357309: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:43:15.357379: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:43:15.357438: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:43:15.357944: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:43:15.361448: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:43:15.361566: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:43:15.361615: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:43:15.361668: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:43:15.361733: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:43:15.361793: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:43:15.361840: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 20:43:15.415719: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-11 20:43:15.421008: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 20:43:15.421131: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 20:43:15.427189: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.20 MB
[2022-12-11 20:43:15.427807: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:43:15.428765: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:43:15.429866: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 20:43:15.430600: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 20:43:15.430645: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 493.16 MB
[[[[[[[2022-12-11 20:43:152022-12-11 20:43:152022-12-11 20:43:152022-12-11 20:43:152022-12-11 20:43:152022-12-11 20:43:152022-12-11 20:43:15.......442084442084442084442086442086442086442086: : : : : : : EEEEEEE       /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:::::::1980198019801980198019801980] ] ] ] ] ] ] eager alloc mem 1024.00 Byteseager alloc mem 1024.00 Byteseager alloc mem 1024.00 Byteseager alloc mem 1024.00 Byteseager alloc mem 1024.00 Byteseager alloc mem 1024.00 Byteseager alloc mem 1024.00 Bytes






[2022-12-11 20:43:15.449052: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 20:43:15[.2022-12-11 20:43:15449117.: 449139E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638:] 638eager release cuda mem 1024] 
eager release cuda mem 400000000
[2022-12-11 20:43:15.449188: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[:2022-12-11 20:43:15638.] 449221eager release cuda mem 1024: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 20:43:15.449278: [E2022-12-11 20:43:15 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc449273:: 638E]  eager release cuda mem 400000000/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
:638] eager release cuda mem 1024
[2022-12-11 20:43:15[.2022-12-11 20:43:15449378.: 449369E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638:] 638eager release cuda mem 400000000] [
eager release cuda mem 10242022-12-11 20:43:15
.449440: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 20:43:15.449510: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638[] [2022-12-11 20:43:15eager release cuda mem 4000000002022-12-11 20:43:15.
.449529449519: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 400000000eager release cuda mem 1024

[2022-12-11 20:43:15.449622: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 20:43:15.450165: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.20 MB
[2022-12-11 20:43:15.450875: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.20 MB
[2022-12-11 20:43:15.456201: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.20 MB
[2022-12-11 20:43:15.457117: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.20 MB
[2022-12-11 20:43:15.457676: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.20 MB
[2022-12-11 20:43:15.458246: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.20 MB
[2022-12-11 20:43:15.458764: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.20 MB
[2022-12-11 20:43:15.464804: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:43:15.464955: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:43:15.465177: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:43:15.465453: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:43:15.465501: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:43:15.465559: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:43:15.465593: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:43:15.465715: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:43:15.465865: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:43:15.466140: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:43:15.466380: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:43:15.466437: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:43:15.[4665192022-12-11 20:43:15: .E466525 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE: 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] :eager release cuda mem 625663638
] eager release cuda mem 625663
[2022-12-11 20:43:15.467123: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 20:43:15.467405: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 20:43:15.[4675062022-12-11 20:43:15: .E467518 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: 1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] :eager alloc mem 25.25 KB1980
] eager alloc mem 25.25 KB
[2022-12-11 20:43:15.467853: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 20:43:15.467899: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 492.62 MB
[2022-12-11 20:43:15.468120: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 20:43:15.468164: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 492.00 MB
[[2022-12-11 20:43:152022-12-11 20:43:15..468263468266: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 25855eager release cuda mem 25855

[[2022-12-11 20:43:152022-12-11 20:43:15..468337468337: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 493.16 MBeager alloc mem 491.87 MB

[2022-12-11 20:43:15.469389: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 20:43:15.469979: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 20:43:15.470030: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 491.06 MB
[2022-12-11 20:43:15.470385: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 20:43:15.470896: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 20:43:15.470979: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 20:43:15.471031: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 488.71 MB
[2022-12-11 20:43:15.471509: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 20:43:15.471558: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 486.60 MB
[[[[[[[[2022-12-11 20:43:152022-12-11 20:43:152022-12-11 20:43:152022-12-11 20:43:152022-12-11 20:43:152022-12-11 20:43:152022-12-11 20:43:152022-12-11 20:43:15........611176611177611176611178611177611177611182611180: : : : : : : : EEEEEEEE        /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::::::19261926192619261926192619261926] ] ] Device 6 init p2p of link 0] ] ] ] ] Device 2 init p2p of link 1Device 1 init p2p of link 7
Device 5 init p2p of link 6Device 7 init p2p of link 4Device 4 init p2p of link 5Device 3 init p2p of link 2Device 0 init p2p of link 3






[2022-12-11 20:43:15[.2022-12-11 20:43:15611650.: 611653E: [ E[2022-12-11 20:43:15[/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu [[2022-12-11 20:43:15.2022-12-11 20:43:15:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-11 20:43:15[2022-12-11 20:43:15.611670.1980:.2022-12-11 20:43:15.611674: 611687] 1980611680.611680: E: eager alloc mem 611.00 KB] : 611710: E E
eager alloc mem 611.00 KBE: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu 
 E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980::/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] 19801980:1980] eager alloc mem 611.00 KB] ] 1980] eager alloc mem 611.00 KB

eager alloc mem 611.00 KBeager alloc mem 611.00 KB] eager alloc mem 611.00 KB

eager alloc mem 611.00 KB

[2022-12-11 20:43:15.612636: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:43:15.612695: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:43:15.612796: E[ [[2022-12-11 20:43:15/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-11 20:43:152022-12-11 20:43:15.:..612806[638612811612819: 2022-12-11 20:43:15] : : E.eager release cuda mem 625663EE[ 612840
  2022-12-11 20:43:15/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.:E::612874638 638638: ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] ] E eager release cuda mem 625663:eager release cuda mem 625663eager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
638

:] 638eager release cuda mem 625663] 
eager release cuda mem 625663
[2022-12-11 20:43:15.635914: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 2
[2022-12-11 20:43:15.636067: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:43:15.636300: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 3
[2022-12-11 20:43:15.636450: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:43:15.636470: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 4
[2022-12-11 20:43:15.636632: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:43:15.636695: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 1
[2022-12-11 20:43:15.636849: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:43:15.636880: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:43:15.636970: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 0
[[2022-12-11 20:43:152022-12-11 20:43:15..637117637130: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19261980] ] Device 0 init p2p of link 6eager alloc mem 611.00 KB

[2022-12-11 20:43:15.637265: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:43:15.637289: [E2022-12-11 20:43:15 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu637314:: 1926E] [ Device 4 init p2p of link 72022-12-11 20:43:15/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
.:6373251980: ] Eeager alloc mem 611.00 KB 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 5
[2022-12-11 20:43:15.637446: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:43:15.637480: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:43:15.637506: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:43:15.637626: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:43:15.637971: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:43:15.638154: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:43:15.638276: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-11 20:43:15:.638638290] : eager release cuda mem 625663E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:43:15.649288: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 3
[2022-12-11 20:43:15.649408: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:43:15.649678: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 0
[2022-12-11 20:43:15.649808: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:43:15.649890: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 7
[2022-12-11 20:43:15.649986: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 6
[2022-12-11 20:43:15.650031: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:43:15.650118: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:43:15.650213: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:43:15.650233: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 5
[[2022-12-11 20:43:152022-12-11 20:43:15..650357650350: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801926] ] eager alloc mem 611.00 KBDevice 0 init p2p of link 1

[2022-12-11 20:43:15.650499: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:43:15.650617: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:43:15.650685: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 2
[2022-12-11 20:43:15.650814: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:43:15.650840: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:43:15.650904: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:43:15.650957: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 4
[2022-12-11 20:43:15.651091: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:43:15.651189: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:43:15.651286: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:43:15.651600: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:43:15.651879: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:43:15.673655: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 0
[2022-12-11 20:43:15.673770: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:43:15.674048: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 4
[2022-12-11 20:43:15.674167: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:43:15.674377: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 3
[2022-12-11 20:43:15.674504: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:43:15.674575: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[[2022-12-11 20:43:152022-12-11 20:43:15..674744674746: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19261926] ] Device 7 init p2p of link 5Device 3 init p2p of link 1

[[2022-12-11 20:43:152022-12-11 20:43:15..674896674898: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 611.00 KBeager alloc mem 611.00 KB

[2022-12-11 20:43:15.674975: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:43:15.675274: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:43:15.675369: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 2
[2022-12-11 20:43:15.675489: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:43:15.675674: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] [[Device 4 init p2p of link 62022-12-11 20:43:152022-12-11 20:43:15
..675705675706: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 625663eager release cuda mem 625663

[2022-12-11 20:43:15.675803: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:43:15.675954: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 7
[2022-12-11 20:43:15.676085: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 20:43:15.676271: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:43:15.676590: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:43:15.676862: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 20:43:15.689450: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 4399996
[2022-12-11 20:43:15.689669: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 4399996
[2022-12-11 20:43:15.690087: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 4399996
[2022-12-11 20:43:15.690605: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 4399996
[2022-12-11 20:43:15.690992: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 4399996
[2022-12-11 20:43:15.691352: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 4399996
[2022-12-11 20:43:15.691552: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 4399996
[2022-12-11 20:43:15.691964: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 4399996
[2022-12-11 20:43:15.692353: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 995694 / 100000000 nodes ( 1.00 %~1.00 %) | remote 2987353 / 100000000 nodes ( 2.99 %) | cpu 96016953 / 100000000 nodes ( 96.02 %) | 491.06 MB | 0.340063 secs 
[2022-12-11 20:43:15.692524: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 990869 / 100000000 nodes ( 0.99 %~1.00 %) | remote 2992178 / 100000000 nodes ( 2.99 %) | cpu 96016953 / 100000000 nodes ( 96.02 %) | 488.71 MB | 0.339938 secs 
[2022-12-11 20:43:15.692605: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 998876 / 100000000 nodes ( 1.00 %~1.00 %) | remote 2984171 / 100000000 nodes ( 2.98 %) | cpu 96016953 / 100000000 nodes ( 96.02 %) | 492.62 MB | 0.339841 secs 
[2022-12-11 20:43:15.692828: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 999995 / 100000000 nodes ( 1.00 %~1.00 %) | remote 2983052 / 100000000 nodes ( 2.98 %) | cpu 96016953 / 100000000 nodes ( 96.02 %) | 493.16 MB | 0.339948 secs 
[2022-12-11 20:43:15.692908: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 997343 / 100000000 nodes ( 1.00 %~1.00 %) | remote 2985704 / 100000000 nodes ( 2.99 %) | cpu 96016953 / 100000000 nodes ( 96.02 %) | 491.87 MB | 0.339834 secs 
[2022-12-11 20:43:15.693308: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 1000000 / 100000000 nodes ( 1.00 %~1.00 %) | remote 2983047 / 100000000 nodes ( 2.98 %) | cpu 96016953 / 100000000 nodes ( 96.02 %) | 493.16 MB | 0.356465 secs 
[2022-12-11 20:43:15.693487: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 986552 / 100000000 nodes ( 0.99 %~1.00 %) | remote 2996495 / 100000000 nodes ( 3.00 %) | cpu 96016953 / 100000000 nodes ( 96.02 %) | 486.60 MB | 0.340427 secs 
[2022-12-11 20:43:15.693650: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 997624 / 100000000 nodes ( 1.00 %~1.00 %) | remote 2985423 / 100000000 nodes ( 2.99 %) | cpu 96016953 / 100000000 nodes ( 96.02 %) | 492.00 MB | 0.341061 secs 
[2022-12-11 20:43:15.693694: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2213] before create ctx, mem is 7.12 GB
[2022-12-11 20:43:17. 29918: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2219] after create ctx, mem is 7.38 GB
[2022-12-11 20:43:17. 32349: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2226] after create stream, mem is 7.38 GB
[2022-12-11 20:43:17. 33975: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2213] before create ctx, mem is 7.38 GB
[2022-12-11 20:43:18.262205: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2219] after create ctx, mem is 7.65 GB
[2022-12-11 20:43:18.262828: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2226] after create stream, mem is 7.65 GB
[2022-12-11 20:43:18.263933: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2213] before create ctx, mem is 7.65 GB
[2022-12-11 20:43:19.475146: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2219] after create ctx, mem is 7.86 GB
[2022-12-11 20:43:19.475284: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2226] after create stream, mem is 7.86 GB
[2022-12-11 20:43:19.475613: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2213] before create ctx, mem is 7.86 GB
[2022-12-11 20:43:20.743968: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2219] after create ctx, mem is 8.08 GB
[2022-12-11 20:43:20.744138: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2226] after create stream, mem is 8.08 GB
[2022-12-11 20:43:20.744439: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2243] before create stream, mem is 8.08 GB
[2022-12-11 20:43:20.744620: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2249] after create stream, mem is 8.08 GB
[2022-12-11 20:43:20.744988: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2213] before create ctx, mem is 8.08 GB
[2022-12-11 20:43:21.869136: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2219] after create ctx, mem is 8.27 GB
[2022-12-11 20:43:21.869864: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2226] after create stream, mem is 8.27 GB
[HCTR][20:43:22.790][ERROR][RK0][tid #139698408240896]: replica 2 calling init per replica done, doing barrier
[HCTR][20:43:22.790][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier
[HCTR][20:43:22.790][ERROR][RK0][tid #139698861217536]: replica 1 calling init per replica done, doing barrier
[HCTR][20:43:22.790][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier
[HCTR][20:43:22.790][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier
[HCTR][20:43:22.790][ERROR][RK0][tid #139698408240896]: replica 4 calling init per replica done, doing barrier
[HCTR][20:43:22.790][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier
[HCTR][20:43:22.790][ERROR][RK0][tid #139698878002944]: replica 0 calling init per replica done, doing barrier
[HCTR][20:43:22.790][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier done
[HCTR][20:43:22.790][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier done
[HCTR][20:43:22.790][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier done
[HCTR][20:43:22.790][ERROR][RK0][tid #139698408240896]: replica 2 calling init per replica done, doing barrier done
[HCTR][20:43:22.790][ERROR][RK0][tid #139698878002944]: replica 0 calling init per replica done, doing barrier done
[HCTR][20:43:22.790][ERROR][RK0][tid #139698408240896]: replica 4 calling init per replica done, doing barrier done
[HCTR][20:43:22.790][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier done
[HCTR][20:43:22.790][ERROR][RK0][tid #139698861217536]: replica 1 calling init per replica done, doing barrier done
[HCTR][20:43:22.790][ERROR][RK0][main]: init per replica done
[HCTR][20:43:22.790][ERROR][RK0][main]: init per replica done
[HCTR][20:43:22.790][ERROR][RK0][tid #139698408240896]: init per replica done
[HCTR][20:43:22.790][ERROR][RK0][main]: init per replica done
[HCTR][20:43:22.790][ERROR][RK0][tid #139698408240896]: init per replica done
[HCTR][20:43:22.790][ERROR][RK0][main]: init per replica done
[HCTR][20:43:22.790][ERROR][RK0][tid #139698861217536]: init per replica done
[HCTR][20:43:22.793][ERROR][RK0][tid #139698878002944]: init per replica done
[HCTR][20:43:22.796][ERROR][RK0][tid #139698408240896]: 2 allocated 3276800 at 0x7f0fe6b20000
[HCTR][20:43:22.796][ERROR][RK0][tid #139698408240896]: 2 allocated 6553600 at 0x7f0fe7000000
[HCTR][20:43:22.796][ERROR][RK0][tid #139698408240896]: 2 allocated 3276800 at 0x7f0fe7640000
[HCTR][20:43:22.796][ERROR][RK0][tid #139698408240896]: 2 allocated 6553600 at 0x7f0fe7960000
[HCTR][20:43:22.796][ERROR][RK0][tid #139698861217536]: 1 allocated 3276800 at 0x7f0fe2b20000
[HCTR][20:43:22.796][ERROR][RK0][tid #139698861217536]: 1 allocated 6553600 at 0x7f0fe3000000
[HCTR][20:43:22.796][ERROR][RK0][tid #139698861217536]: 1 allocated 3276800 at 0x7f0fe3640000
[HCTR][20:43:22.796][ERROR][RK0][tid #139698861217536]: 1 allocated 6553600 at 0x7f0fe3960000
[HCTR][20:43:22.797][ERROR][RK0][main]: 5 allocated 3276800 at 0x7f0fe6b20000
[HCTR][20:43:22.797][ERROR][RK0][main]: 6 allocated 3276800 at 0x7f0fe6b20000
[HCTR][20:43:22.797][ERROR][RK0][main]: 5 allocated 6553600 at 0x7f0fe7000000
[HCTR][20:43:22.797][ERROR][RK0][main]: 6 allocated 6553600 at 0x7f0fe7000000
[HCTR][20:43:22.797][ERROR][RK0][main]: 5 allocated 3276800 at 0x7f0fe7640000
[HCTR][20:43:22.797][ERROR][RK0][main]: 6 allocated 3276800 at 0x7f0fe7640000
[HCTR][20:43:22.797][ERROR][RK0][main]: 5 allocated 6553600 at 0x7f0fe7960000
[HCTR][20:43:22.797][ERROR][RK0][main]: 6 allocated 6553600 at 0x7f0fe7960000
[HCTR][20:43:22.797][ERROR][RK0][main]: 4 allocated 3276800 at 0x7f0fe4b20000
[HCTR][20:43:22.797][ERROR][RK0][main]: 4 allocated 6553600 at 0x7f0fe5000000
[HCTR][20:43:22.797][ERROR][RK0][main]: 4 allocated 3276800 at 0x7f0fe5640000
[HCTR][20:43:22.797][ERROR][RK0][main]: 4 allocated 6553600 at 0x7f0fe5960000
[HCTR][20:43:22.797][ERROR][RK0][tid #139698609567488]: 7 allocated 3276800 at 0x7f0fe6b20000
[HCTR][20:43:22.797][ERROR][RK0][tid #139698399848192]: 3 allocated 3276800 at 0x7f0fe6b20000
[HCTR][20:43:22.797][ERROR][RK0][tid #139698609567488]: 7 allocated 6553600 at 0x7f0fe7000000
[HCTR][20:43:22.797][ERROR][RK0][tid #139698399848192]: 3 allocated 6553600 at 0x7f0fe7000000
[HCTR][20:43:22.797][ERROR][RK0][tid #139698609567488]: 7 allocated 3276800 at 0x7f0fe7640000
[HCTR][20:43:22.797][ERROR][RK0][tid #139698399848192]: 3 allocated 3276800 at 0x7f0fe7640000
[HCTR][20:43:22.797][ERROR][RK0][tid #139698609567488]: 7 allocated 6553600 at 0x7f0fe7960000
[HCTR][20:43:22.797][ERROR][RK0][tid #139698399848192]: 3 allocated 6553600 at 0x7f0fe7960000
[HCTR][20:43:22.800][ERROR][RK0][tid #139698878002944]: 0 allocated 3276800 at 0x7f0fe7120000
[HCTR][20:43:22.800][ERROR][RK0][tid #139698878002944]: 0 allocated 6553600 at 0x7f0fe7600000
[HCTR][20:43:22.800][ERROR][RK0][tid #139698878002944]: 0 allocated 3276800 at 0x7f0fe830e800
[HCTR][20:43:22.800][ERROR][RK0][tid #139698878002944]: 0 allocated 6553600 at 0x7f0fe862e800








