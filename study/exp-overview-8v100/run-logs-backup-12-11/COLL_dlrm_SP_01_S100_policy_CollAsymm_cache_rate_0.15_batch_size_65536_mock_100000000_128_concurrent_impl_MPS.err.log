2022-12-12 03:47:36.682904: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.692424: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.697663: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.705645: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.710131: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.716462: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.726383: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.734671: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.790363: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.793040: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.801014: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.804762: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.805637: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.806587: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.807651: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.808579: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.809522: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.810754: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.812294: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.813206: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.813303: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.814709: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.814824: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.816116: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.816478: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.817850: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.818211: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.819349: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.819713: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.821035: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.821413: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.822442: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.822954: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.824241: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.824483: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.825979: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.826898: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.827962: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.829000: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.829925: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.834949: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.835218: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 03:47:36.836076: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.837038: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.838091: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.839169: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.840448: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.841713: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.843148: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.844225: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.845554: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.845609: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.846963: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.847199: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.849210: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.851189: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.851367: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.853201: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.853423: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.855271: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.855515: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.857483: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.857910: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.860128: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.860575: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.861826: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.863043: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.864594: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.865785: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.867206: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.867932: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.868024: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.869637: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.869884: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.880832: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.881048: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.883441: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.883619: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.884527: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.884977: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.885963: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.886057: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.887348: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.887887: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.888724: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.906618: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.922966: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.923176: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.924610: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.924690: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.925303: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.925878: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.926005: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.927146: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.927619: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.929528: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.930233: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.930875: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.931035: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.931178: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.931818: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.934681: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.935492: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.936734: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.936906: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.937087: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.937443: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.939238: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.940482: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.940504: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.940787: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.941388: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.941433: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.943626: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.944721: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.944937: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.945560: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.945829: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.947605: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.948733: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.948779: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.949509: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.950723: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.951804: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.951888: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.952404: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.953593: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.954310: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.954435: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.955026: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.956243: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.956923: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.957006: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.957853: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.958858: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.959908: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.959950: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.960656: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.961544: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.962508: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.962636: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.963123: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.964379: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.965246: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.965379: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.965876: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.966956: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.967926: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.968074: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.969370: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.969382: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.970352: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.970528: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.972014: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.972229: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.973001: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.973088: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.974569: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.974870: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.976610: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.976615: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.977250: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.977372: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.978395: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.979393: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.979441: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.980388: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.980423: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.981834: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.982398: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.982605: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.983434: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.983576: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.984872: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.985490: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.985671: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.986561: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.986838: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.987946: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.988318: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.988694: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.989008: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.990259: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.990397: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.991783: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.991921: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.992337: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.992713: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.993627: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.993892: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.995794: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.995849: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.996464: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.997185: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.997889: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:36.997968: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 03:47:36.998045: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:37.000029: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:37.000031: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:37.000489: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:37.001475: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:37.002076: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:37.002163: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:37.003843: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:37.003972: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:37.005926: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:37.005924: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:37.007365: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:37.007466: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:37.008631: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 03:47:37.008631: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 03:47:37.008726: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:37.008801: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:37.010037: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:37.010129: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:37.010484: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 03:47:37.011811: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:37.011984: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:37.013428: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:37.013613: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:37.014876: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:37.015037: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:37.016272: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:37.017809: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:37.017874: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:37.018146: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:37.018164: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:37.019536: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:37.019588: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:37.022335: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:37.022422: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:37.022644: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:37.022847: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:37.024200: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:37.024275: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:37.026759: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:37.026845: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:37.027159: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:37.027396: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:37.028863: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:37.029002: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:37.031775: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:37.031850: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:37.033220: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:37.036528: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:37.037055: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:37.066279: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:37.069618: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:37.070012: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:37.070937: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:37.074328: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:37.074788: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:37.075819: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:37.079040: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:37.079323: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:37.082926: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:37.084469: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:37.085526: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:37.088754: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:37.090609: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:37.090922: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:37.093056: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:37.096768: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:37.097315: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:37.098771: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:37.100984: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:37.101689: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:37.103025: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:37.133602: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:37.134502: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:37.136900: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 03:47:37.140527: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:37.143676: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:37.146185: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:37.146301: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:37.149313: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:37.152309: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:37.152677: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:37.217047: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 03:47:37.225808: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:37.242661: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:37.242899: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:37.246724: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:37.254212: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 03:47:37.256612: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:37.262912: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:37.269451: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:37.276305: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:38.135873: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:38.136707: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:38.137245: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:38.137713: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 03:47:38.137769: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 03:47:38.156779: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:38.157624: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:38.158351: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:38.159152: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:38.159681: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:38.160732: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 03:47:38.208288: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:47:38.208508: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:47:38.243687: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-12 03:47:38.368852: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:38.369479: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:38.369997: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:38.370699: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 03:47:38.370758: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 03:47:38.379233: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:38.380031: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:38.380648: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:38.381129: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 03:47:38.381196: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 03:47:38.388241: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:38.388863: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:38.389375: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:38.389938: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:38.390541: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:38.391044: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 03:47:38.398970: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:38.400038: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:38.400564: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:38.401363: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:38.401872: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:38.402345: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 03:47:38.407087: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:38.407855: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:38.408402: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:38.409011: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 03:47:38.409070: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 03:47:38.427228: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:38.427856: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:38.428956: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:38.429534: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:38.430039: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:38.430509: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 03:47:38.431431: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:38.432004: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:38.432751: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:38.433222: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 03:47:38.433266: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 03:47:38.450844: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:38.451503: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:38.452013: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:38.452837: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:38.453360: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:38.453821: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 03:47:38.460733: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:38.461622: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:38.462164: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:38.462636: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 03:47:38.462689: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 03:47:38.465640: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:38.466275: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:38.466792: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:38.467276: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 03:47:38.467330: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 03:47:38.473972: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:38.474584: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:38.475109: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:38.475586: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 03:47:38.475640: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 03:47:38.476941: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:47:38.477114: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:47:38.477219: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:47:38.477438: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:47:38.478978: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
2022-12-12 03:47:38.479177: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-12 03:47:38.480167: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:38.480792: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:38.481309: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:38.481887: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:38.482409: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:38.482883: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 03:47:38.485197: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:38.485828: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:38.486346: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:38.486910: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:38.487453: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:38.487922: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 03:47:38.494240: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:38.494870: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:38.495404: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:38.495972: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:38.496238: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:47:38.496419: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:47:38.496508: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 03:47:38.496980: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 03:47:38.497460: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
2022-12-12 03:47:38.500818: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:47:38.501002: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:47:38.502584: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
2022-12-12 03:47:38.529707: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:47:38.529913: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:47:38.530878: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
2022-12-12 03:47:38.534224: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:47:38.534387: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:47:38.536158: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
2022-12-12 03:47:38.543319: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:47:38.543486: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 03:47:38.545237: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
[HCTR][03:47:39.805][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][03:47:39.805][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][03:47:39.805][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][03:47:39.805][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][03:47:39.805][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][03:47:39.806][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][03:47:39.854][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][03:47:39.854][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:01,  1.54s/it]warmup run: 1it [00:01,  1.51s/it]warmup run: 1it [00:01,  1.52s/it]warmup run: 95it [00:01, 80.13it/s]warmup run: 1it [00:01,  1.55s/it]warmup run: 1it [00:01,  1.56s/it]warmup run: 98it [00:01, 84.34it/s]warmup run: 93it [00:01, 79.40it/s]warmup run: 192it [00:01, 175.94it/s]warmup run: 1it [00:01,  1.51s/it]warmup run: 99it [00:01, 83.35it/s]warmup run: 102it [00:01, 85.08it/s]warmup run: 198it [00:01, 184.76it/s]warmup run: 1it [00:01,  1.48s/it]warmup run: 184it [00:01, 169.92it/s]warmup run: 1it [00:01,  1.45s/it]warmup run: 288it [00:01, 280.49it/s]warmup run: 91it [00:01, 78.58it/s]warmup run: 198it [00:01, 180.81it/s]warmup run: 203it [00:01, 183.78it/s]warmup run: 280it [00:01, 270.44it/s]warmup run: 98it [00:01, 85.88it/s]warmup run: 278it [00:01, 273.55it/s]warmup run: 92it [00:01, 81.95it/s]warmup run: 385it [00:01, 390.48it/s]warmup run: 182it [00:01, 170.13it/s]warmup run: 297it [00:01, 288.32it/s]warmup run: 303it [00:01, 291.72it/s]warmup run: 363it [00:01, 360.82it/s]warmup run: 196it [00:01, 185.51it/s]warmup run: 378it [00:01, 390.48it/s]warmup run: 184it [00:01, 176.77it/s]warmup run: 486it [00:02, 505.40it/s]warmup run: 273it [00:01, 270.56it/s]warmup run: 395it [00:01, 398.51it/s]warmup run: 401it [00:01, 399.79it/s]warmup run: 463it [00:02, 480.30it/s]warmup run: 294it [00:01, 294.51it/s]warmup run: 478it [00:02, 504.44it/s]warmup run: 275it [00:01, 278.76it/s]warmup run: 589it [00:02, 614.96it/s]warmup run: 365it [00:01, 375.71it/s]warmup run: 493it [00:02, 506.01it/s]warmup run: 502it [00:02, 512.31it/s]warmup run: 564it [00:02, 592.12it/s]warmup run: 392it [00:01, 406.75it/s]warmup run: 580it [00:02, 612.17it/s]warmup run: 365it [00:01, 381.57it/s]warmup run: 690it [00:02, 704.78it/s]warmup run: 457it [00:02, 477.88it/s]warmup run: 593it [00:02, 608.64it/s]warmup run: 596it [00:02, 598.44it/s]warmup run: 665it [00:02, 688.84it/s]warmup run: 491it [00:01, 516.71it/s]warmup run: 683it [00:02, 708.01it/s]warmup run: 456it [00:01, 482.30it/s]warmup run: 791it [00:02, 779.56it/s]warmup run: 550it [00:02, 572.44it/s]warmup run: 692it [00:02, 695.79it/s]warmup run: 690it [00:02, 665.11it/s]warmup run: 767it [00:02, 770.39it/s]warmup run: 591it [00:02, 619.36it/s]warmup run: 785it [00:02, 785.08it/s]warmup run: 549it [00:02, 577.82it/s]warmup run: 892it [00:02, 838.24it/s]warmup run: 643it [00:02, 654.07it/s]warmup run: 791it [00:02, 767.07it/s]warmup run: 793it [00:02, 753.63it/s]warmup run: 868it [00:02, 831.78it/s]warmup run: 691it [00:02, 708.07it/s]warmup run: 887it [00:02, 844.96it/s]warmup run: 643it [00:02, 661.00it/s]warmup run: 991it [00:02, 876.00it/s]warmup run: 737it [00:02, 724.33it/s]warmup run: 892it [00:02, 828.36it/s]warmup run: 893it [00:02, 816.14it/s]warmup run: 969it [00:02, 879.46it/s]warmup run: 791it [00:02, 779.48it/s]warmup run: 987it [00:02, 881.22it/s]warmup run: 737it [00:02, 729.52it/s]warmup run: 1090it [00:02, 905.43it/s]warmup run: 835it [00:02, 789.34it/s]warmup run: 992it [00:02, 872.78it/s]warmup run: 990it [00:02, 857.11it/s]warmup run: 1070it [00:02, 914.41it/s]warmup run: 891it [00:02, 835.88it/s]warmup run: 1086it [00:02, 900.43it/s]warmup run: 831it [00:02, 783.70it/s]warmup run: 1189it [00:02, 925.01it/s]warmup run: 930it [00:02, 832.18it/s]warmup run: 1093it [00:02, 908.61it/s]warmup run: 1092it [00:02, 901.14it/s]warmup run: 1171it [00:02, 941.27it/s]warmup run: 990it [00:02, 877.42it/s]warmup run: 1184it [00:02, 907.99it/s]warmup run: 926it [00:02, 827.07it/s]warmup run: 1289it [00:02, 944.27it/s]warmup run: 1024it [00:02, 855.24it/s]warmup run: 1194it [00:02, 937.21it/s]warmup run: 1191it [00:02, 920.40it/s]warmup run: 1272it [00:02, 959.95it/s]warmup run: 1091it [00:02, 912.12it/s]warmup run: 1027it [00:02, 877.78it/s]warmup run: 1281it [00:02, 914.46it/s]warmup run: 1388it [00:02, 957.33it/s]warmup run: 1296it [00:02, 960.89it/s]warmup run: 1117it [00:02, 871.02it/s]warmup run: 1295it [00:02, 954.58it/s]warmup run: 1373it [00:02, 974.11it/s]warmup run: 1190it [00:02, 931.55it/s]warmup run: 1129it [00:02, 917.56it/s]warmup run: 1377it [00:02, 919.84it/s]warmup run: 1487it [00:03, 966.14it/s]warmup run: 1398it [00:02, 977.09it/s]warmup run: 1212it [00:02, 891.73it/s]warmup run: 1399it [00:02, 978.06it/s]warmup run: 1474it [00:03, 970.42it/s]warmup run: 1290it [00:02, 948.92it/s]warmup run: 1231it [00:02, 947.12it/s]warmup run: 1586it [00:03, 971.49it/s]warmup run: 1472it [00:03, 924.28it/s]warmup run: 1499it [00:03, 982.00it/s]warmup run: 1309it [00:02, 912.46it/s]warmup run: 1504it [00:03, 998.10it/s]warmup run: 1575it [00:03, 979.37it/s]warmup run: 1389it [00:02, 959.93it/s]warmup run: 1334it [00:02, 971.33it/s]warmup run: 1685it [00:03, 975.83it/s]warmup run: 1567it [00:03, 926.46it/s]warmup run: 1600it [00:03, 988.64it/s]warmup run: 1404it [00:03, 921.99it/s]warmup run: 1609it [00:03, 1012.79it/s]warmup run: 1675it [00:03, 980.50it/s]warmup run: 1489it [00:02, 969.77it/s]warmup run: 1436it [00:02, 983.24it/s]warmup run: 1784it [00:03, 976.92it/s]warmup run: 1662it [00:03, 931.27it/s]warmup run: 1701it [00:03, 992.82it/s]warmup run: 1500it [00:03, 931.17it/s]warmup run: 1715it [00:03, 1024.63it/s]warmup run: 1775it [00:03, 983.83it/s]warmup run: 1589it [00:03, 976.70it/s]warmup run: 1537it [00:03, 987.17it/s]warmup run: 1883it [00:03, 980.24it/s]warmup run: 1757it [00:03, 934.21it/s]warmup run: 1802it [00:03, 996.89it/s]warmup run: 1597it [00:03, 940.73it/s]warmup run: 1820it [00:03, 1031.32it/s]warmup run: 1875it [00:03, 985.37it/s]warmup run: 1690it [00:03, 984.90it/s]warmup run: 1638it [00:03, 986.62it/s]warmup run: 1982it [00:03, 981.97it/s]warmup run: 1852it [00:03, 937.36it/s]warmup run: 1903it [00:03, 999.93it/s]warmup run: 1696it [00:03, 955.06it/s]warmup run: 1925it [00:03, 1034.47it/s]warmup run: 1790it [00:03, 988.54it/s]warmup run: 1975it [00:03, 985.31it/s]warmup run: 1738it [00:03, 987.13it/s]warmup run: 2096it [00:03, 1026.56it/s]warmup run: 1947it [00:03, 940.71it/s]warmup run: 1795it [00:03, 962.64it/s]warmup run: 2004it [00:03, 992.38it/s]warmup run: 2032it [00:03, 1044.53it/s]warmup run: 2089it [00:03, 1030.92it/s]warmup run: 1891it [00:03, 993.66it/s]warmup run: 1841it [00:03, 997.69it/s]warmup run: 2214it [00:03, 1070.04it/s]warmup run: 2052it [00:03, 972.45it/s]warmup run: 1895it [00:03, 973.64it/s]warmup run: 2121it [00:03, 1044.55it/s]warmup run: 2152it [00:03, 1090.13it/s]warmup run: 2210it [00:03, 1083.97it/s]warmup run: 1993it [00:03, 998.94it/s]warmup run: 1945it [00:03, 1008.66it/s]warmup run: 2332it [00:03, 1101.05it/s]warmup run: 2170it [00:03, 1034.03it/s]warmup run: 1996it [00:03, 984.13it/s]warmup run: 2239it [00:03, 1083.53it/s]warmup run: 2272it [00:03, 1122.32it/s]warmup run: 2331it [00:03, 1119.82it/s]warmup run: 2113it [00:03, 1056.28it/s]warmup run: 2056it [00:03, 1036.46it/s]warmup run: 2450it [00:03, 1124.27it/s]warmup run: 2291it [00:03, 1086.36it/s]warmup run: 2117it [00:03, 1049.23it/s]warmup run: 2357it [00:03, 1110.15it/s]warmup run: 2392it [00:03, 1145.14it/s]warmup run: 2453it [00:03, 1149.21it/s]warmup run: 2234it [00:03, 1099.55it/s]warmup run: 2176it [00:03, 1083.04it/s]warmup run: 2568it [00:04, 1140.18it/s]warmup run: 2413it [00:03, 1125.88it/s]warmup run: 2239it [00:03, 1099.33it/s]warmup run: 2475it [00:03, 1129.03it/s]warmup run: 2513it [00:03, 1161.62it/s]warmup run: 2569it [00:04, 1148.65it/s]warmup run: 2354it [00:03, 1126.86it/s]warmup run: 2296it [00:03, 1116.35it/s]warmup run: 2685it [00:04, 1146.68it/s]warmup run: 2535it [00:04, 1152.88it/s]warmup run: 2361it [00:03, 1135.05it/s]warmup run: 2593it [00:04, 1142.40it/s]warmup run: 2632it [00:04, 1168.33it/s]warmup run: 2684it [00:04, 1145.17it/s]warmup run: 2474it [00:03, 1147.03it/s]warmup run: 2416it [00:03, 1139.82it/s]warmup run: 2803it [00:04, 1154.70it/s]warmup run: 2654it [00:04, 1162.63it/s]warmup run: 2483it [00:04, 1159.42it/s]warmup run: 2711it [00:04, 1151.66it/s]warmup run: 2752it [00:04, 1176.50it/s]warmup run: 2805it [00:04, 1163.18it/s]warmup run: 2594it [00:03, 1162.00it/s]warmup run: 2536it [00:03, 1155.00it/s]warmup run: 2921it [00:04, 1161.46it/s]warmup run: 2774it [00:04, 1171.49it/s]warmup run: 2605it [00:04, 1176.41it/s]warmup run: 2827it [00:04, 1153.09it/s]warmup run: 2872it [00:04, 1182.50it/s]warmup run: 2927it [00:04, 1177.33it/s]warmup run: 2713it [00:04, 1167.81it/s]warmup run: 3000it [00:04, 676.43it/s] warmup run: 2654it [00:04, 1160.12it/s]warmup run: 2892it [00:04, 1172.92it/s]warmup run: 3000it [00:04, 681.71it/s] warmup run: 2725it [00:04, 1182.03it/s]warmup run: 2944it [00:04, 1156.15it/s]warmup run: 2992it [00:04, 1185.57it/s]warmup run: 3000it [00:04, 681.84it/s] warmup run: 2833it [00:04, 1174.81it/s]warmup run: 3000it [00:04, 678.43it/s] warmup run: 3000it [00:04, 673.53it/s] warmup run: 2772it [00:04, 1163.59it/s]warmup run: 2847it [00:04, 1191.63it/s]warmup run: 2951it [00:04, 1174.93it/s]warmup run: 3000it [00:04, 690.18it/s] warmup run: 2890it [00:04, 1168.29it/s]warmup run: 2968it [00:04, 1196.93it/s]warmup run: 3000it [00:04, 672.79it/s] warmup run: 3000it [00:04, 686.19it/s] 

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]


warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   5%|▌         | 161/3000 [00:00<00:01, 1607.84it/s]warmup should be done:   5%|▌         | 163/3000 [00:00<00:01, 1626.77it/s]warmup should be done:   5%|▌         | 164/3000 [00:00<00:01, 1632.79it/s]warmup should be done:   5%|▌         | 163/3000 [00:00<00:01, 1627.90it/s]warmup should be done:   5%|▌         | 160/3000 [00:00<00:01, 1596.92it/s]warmup should be done:   5%|▌         | 163/3000 [00:00<00:01, 1624.44it/s]warmup should be done:   5%|▌         | 162/3000 [00:00<00:01, 1611.26it/s]warmup should be done:   5%|▌         | 161/3000 [00:00<00:01, 1602.07it/s]warmup should be done:  11%|█         | 328/3000 [00:00<00:01, 1639.44it/s]warmup should be done:  11%|█         | 323/3000 [00:00<00:01, 1614.64it/s]warmup should be done:  11%|█         | 325/3000 [00:00<00:01, 1622.13it/s]warmup should be done:  11%|█         | 330/3000 [00:00<00:01, 1648.32it/s]warmup should be done:  11%|█         | 327/3000 [00:00<00:01, 1632.67it/s]warmup should be done:  11%|█         | 325/3000 [00:00<00:01, 1619.84it/s]warmup should be done:  11%|█         | 329/3000 [00:00<00:01, 1637.34it/s]warmup should be done:  11%|█         | 325/3000 [00:00<00:01, 1619.74it/s]warmup should be done:  16%|█▌        | 486/3000 [00:00<00:01, 1619.28it/s]warmup should be done:  16%|█▋        | 492/3000 [00:00<00:01, 1636.57it/s]warmup should be done:  16%|█▋        | 488/3000 [00:00<00:01, 1621.63it/s]warmup should be done:  17%|█▋        | 496/3000 [00:00<00:01, 1649.87it/s]warmup should be done:  16%|█▌        | 487/3000 [00:00<00:01, 1619.35it/s]warmup should be done:  16%|█▌        | 487/3000 [00:00<00:01, 1616.55it/s]warmup should be done:  16%|█▋        | 493/3000 [00:00<00:01, 1633.84it/s]warmup should be done:  16%|█▋        | 491/3000 [00:00<00:01, 1629.00it/s]warmup should be done:  22%|██▏       | 648/3000 [00:00<00:01, 1617.04it/s]warmup should be done:  22%|██▏       | 656/3000 [00:00<00:01, 1633.45it/s]warmup should be done:  22%|██▏       | 662/3000 [00:00<00:01, 1650.18it/s]warmup should be done:  22%|██▏       | 651/3000 [00:00<00:01, 1619.97it/s]warmup should be done:  22%|██▏       | 654/3000 [00:00<00:01, 1625.49it/s]warmup should be done:  22%|██▏       | 657/3000 [00:00<00:01, 1631.91it/s]warmup should be done:  22%|██▏       | 649/3000 [00:00<00:01, 1610.73it/s]warmup should be done:  22%|██▏       | 649/3000 [00:00<00:01, 1554.61it/s]warmup should be done:  27%|██▋       | 810/3000 [00:00<00:01, 1614.02it/s]warmup should be done:  28%|██▊       | 828/3000 [00:00<00:01, 1649.53it/s]warmup should be done:  27%|██▋       | 820/3000 [00:00<00:01, 1629.61it/s]warmup should be done:  27%|██▋       | 817/3000 [00:00<00:01, 1623.88it/s]warmup should be done:  27%|██▋       | 813/3000 [00:00<00:01, 1614.30it/s]warmup should be done:  27%|██▋       | 821/3000 [00:00<00:01, 1629.66it/s]warmup should be done:  27%|██▋       | 811/3000 [00:00<00:01, 1607.01it/s]warmup should be done:  27%|██▋       | 811/3000 [00:00<00:01, 1577.44it/s]warmup should be done:  32%|███▏      | 972/3000 [00:00<00:01, 1610.48it/s]warmup should be done:  33%|███▎      | 993/3000 [00:00<00:01, 1644.46it/s]warmup should be done:  33%|███▎      | 980/3000 [00:00<00:01, 1618.04it/s]warmup should be done:  33%|███▎      | 984/3000 [00:00<00:01, 1623.51it/s]warmup should be done:  32%|███▏      | 972/3000 [00:00<00:01, 1599.18it/s]warmup should be done:  32%|███▎      | 975/3000 [00:00<00:01, 1602.85it/s]warmup should be done:  33%|███▎      | 983/3000 [00:00<00:01, 1613.98it/s]warmup should be done:  32%|███▏      | 971/3000 [00:00<00:01, 1583.36it/s]warmup should be done:  39%|███▊      | 1158/3000 [00:00<00:01, 1644.44it/s]warmup should be done:  38%|███▊      | 1142/3000 [00:00<00:01, 1618.22it/s]warmup should be done:  38%|███▊      | 1134/3000 [00:00<00:01, 1606.31it/s]warmup should be done:  38%|███▊      | 1147/3000 [00:00<00:01, 1623.30it/s]warmup should be done:  38%|███▊      | 1132/3000 [00:00<00:01, 1597.87it/s]warmup should be done:  38%|███▊      | 1145/3000 [00:00<00:01, 1614.64it/s]warmup should be done:  38%|███▊      | 1136/3000 [00:00<00:01, 1602.29it/s]warmup should be done:  38%|███▊      | 1134/3000 [00:00<00:01, 1595.45it/s]warmup should be done:  43%|████▎     | 1304/3000 [00:00<00:01, 1618.65it/s]warmup should be done:  44%|████▍     | 1323/3000 [00:00<00:01, 1642.05it/s]warmup should be done:  43%|████▎     | 1296/3000 [00:00<00:01, 1607.67it/s]warmup should be done:  44%|████▎     | 1310/3000 [00:00<00:01, 1623.00it/s]warmup should be done:  43%|████▎     | 1292/3000 [00:00<00:01, 1597.28it/s]warmup should be done:  44%|████▎     | 1307/3000 [00:00<00:01, 1614.70it/s]warmup should be done:  43%|████▎     | 1297/3000 [00:00<00:01, 1602.93it/s]warmup should be done:  43%|████▎     | 1297/3000 [00:00<00:01, 1604.69it/s]warmup should be done:  49%|████▉     | 1467/3000 [00:00<00:00, 1620.30it/s]warmup should be done:  50%|████▉     | 1489/3000 [00:00<00:00, 1645.52it/s]warmup should be done:  49%|████▊     | 1458/3000 [00:00<00:00, 1610.57it/s]warmup should be done:  49%|████▉     | 1473/3000 [00:00<00:00, 1623.02it/s]warmup should be done:  48%|████▊     | 1452/3000 [00:00<00:00, 1596.98it/s]warmup should be done:  49%|████▉     | 1471/3000 [00:00<00:00, 1619.51it/s]warmup should be done:  49%|████▊     | 1458/3000 [00:00<00:00, 1602.64it/s]warmup should be done:  49%|████▊     | 1460/3000 [00:00<00:00, 1611.82it/s]warmup should be done:  54%|█████▍    | 1632/3000 [00:01<00:00, 1626.67it/s]warmup should be done:  55%|█████▌    | 1655/3000 [00:01<00:00, 1647.72it/s]warmup should be done:  54%|█████▍    | 1620/3000 [00:01<00:00, 1610.68it/s]warmup should be done:  55%|█████▍    | 1636/3000 [00:01<00:00, 1623.04it/s]warmup should be done:  54%|█████▎    | 1612/3000 [00:01<00:00, 1595.99it/s]warmup should be done:  54%|█████▍    | 1619/3000 [00:01<00:00, 1603.89it/s]warmup should be done:  54%|█████▍    | 1633/3000 [00:01<00:00, 1617.07it/s]warmup should be done:  54%|█████▍    | 1622/3000 [00:01<00:00, 1579.61it/s]warmup should be done:  60%|█████▉    | 1795/3000 [00:01<00:00, 1625.67it/s]warmup should be done:  59%|█████▉    | 1782/3000 [00:01<00:00, 1610.46it/s]warmup should be done:  60%|█████▉    | 1799/3000 [00:01<00:00, 1622.16it/s]warmup should be done:  59%|█████▉    | 1774/3000 [00:01<00:00, 1600.82it/s]warmup should be done:  60%|█████▉    | 1795/3000 [00:01<00:00, 1617.84it/s]warmup should be done:  59%|█████▉    | 1780/3000 [00:01<00:00, 1604.10it/s]warmup should be done:  61%|██████    | 1820/3000 [00:01<00:00, 1628.40it/s]warmup should be done:  59%|█████▉    | 1781/3000 [00:01<00:00, 1571.01it/s]warmup should be done:  65%|██████▌   | 1958/3000 [00:01<00:00, 1622.47it/s]warmup should be done:  65%|██████▌   | 1962/3000 [00:01<00:00, 1622.68it/s]warmup should be done:  65%|██████▍   | 1936/3000 [00:01<00:00, 1603.83it/s]warmup should be done:  65%|██████▌   | 1957/3000 [00:01<00:00, 1616.91it/s]warmup should be done:  65%|██████▍   | 1941/3000 [00:01<00:00, 1603.89it/s]warmup should be done:  65%|██████▍   | 1944/3000 [00:01<00:00, 1604.31it/s]warmup should be done:  66%|██████▌   | 1983/3000 [00:01<00:00, 1621.75it/s]warmup should be done:  65%|██████▍   | 1939/3000 [00:01<00:00, 1567.72it/s]warmup should be done:  71%|███████   | 2121/3000 [00:01<00:00, 1622.06it/s]warmup should be done:  71%|███████   | 2125/3000 [00:01<00:00, 1623.32it/s]warmup should be done:  70%|██████▉   | 2098/3000 [00:01<00:00, 1608.55it/s]warmup should be done:  70%|███████   | 2102/3000 [00:01<00:00, 1605.36it/s]warmup should be done:  71%|███████   | 2119/3000 [00:01<00:00, 1611.75it/s]warmup should be done:  70%|███████   | 2105/3000 [00:01<00:00, 1600.16it/s]warmup should be done:  72%|███████▏  | 2146/3000 [00:01<00:00, 1616.58it/s]warmup should be done:  70%|██████▉   | 2096/3000 [00:01<00:00, 1564.57it/s]warmup should be done:  75%|███████▌  | 2259/3000 [00:01<00:00, 1608.05it/s]warmup should be done:  76%|███████▋  | 2288/3000 [00:01<00:00, 1621.88it/s]warmup should be done:  75%|███████▌  | 2263/3000 [00:01<00:00, 1603.47it/s]warmup should be done:  76%|███████▌  | 2284/3000 [00:01<00:00, 1615.97it/s]warmup should be done:  76%|███████▌  | 2281/3000 [00:01<00:00, 1608.32it/s]warmup should be done:  76%|███████▌  | 2266/3000 [00:01<00:00, 1595.55it/s]warmup should be done:  77%|███████▋  | 2308/3000 [00:01<00:00, 1610.02it/s]warmup should be done:  75%|███████▌  | 2257/3000 [00:01<00:00, 1576.73it/s]warmup should be done:  82%|████████▏ | 2451/3000 [00:01<00:00, 1624.28it/s]warmup should be done:  81%|████████  | 2422/3000 [00:01<00:00, 1611.65it/s]warmup should be done:  81%|████████  | 2425/3000 [00:01<00:00, 1605.60it/s]warmup should be done:  82%|████████▏ | 2447/3000 [00:01<00:00, 1618.97it/s]warmup should be done:  81%|████████▏ | 2442/3000 [00:01<00:00, 1598.61it/s]warmup should be done:  81%|████████  | 2426/3000 [00:01<00:00, 1589.49it/s]warmup should be done:  82%|████████▏ | 2470/3000 [00:01<00:00, 1609.30it/s]warmup should be done:  81%|████████  | 2420/3000 [00:01<00:00, 1591.03it/s]warmup should be done:  87%|████████▋ | 2614/3000 [00:01<00:00, 1622.62it/s]warmup should be done:  86%|████████▌ | 2584/3000 [00:01<00:00, 1609.98it/s]warmup should be done:  86%|████████▌ | 2586/3000 [00:01<00:00, 1605.18it/s]warmup should be done:  87%|████████▋ | 2609/3000 [00:01<00:00, 1614.48it/s]warmup should be done:  87%|████████▋ | 2605/3000 [00:01<00:00, 1606.67it/s]warmup should be done:  86%|████████▌ | 2585/3000 [00:01<00:00, 1587.20it/s]warmup should be done:  88%|████████▊ | 2631/3000 [00:01<00:00, 1609.06it/s]warmup should be done:  86%|████████▌ | 2584/3000 [00:01<00:00, 1603.06it/s]warmup should be done:  93%|█████████▎| 2777/3000 [00:01<00:00, 1621.00it/s]warmup should be done:  92%|█████████▏| 2745/3000 [00:01<00:00, 1608.43it/s]warmup should be done:  92%|█████████▏| 2747/3000 [00:01<00:00, 1603.86it/s]warmup should be done:  92%|█████████▏| 2771/3000 [00:01<00:00, 1610.94it/s]warmup should be done:  92%|█████████▏| 2768/3000 [00:01<00:00, 1612.61it/s]warmup should be done:  91%|█████████▏| 2744/3000 [00:01<00:00, 1587.72it/s]warmup should be done:  93%|█████████▎| 2792/3000 [00:01<00:00, 1607.95it/s]warmup should be done:  92%|█████████▏| 2748/3000 [00:01<00:00, 1611.94it/s]warmup should be done:  98%|█████████▊| 2940/3000 [00:01<00:00, 1621.62it/s]warmup should be done:  97%|█████████▋| 2908/3000 [00:01<00:00, 1613.10it/s]warmup should be done:  97%|█████████▋| 2909/3000 [00:01<00:00, 1607.11it/s]warmup should be done:  98%|█████████▊| 2935/3000 [00:01<00:00, 1617.70it/s]warmup should be done:  98%|█████████▊| 2932/3000 [00:01<00:00, 1619.58it/s]warmup should be done:  97%|█████████▋| 2905/3000 [00:01<00:00, 1592.83it/s]warmup should be done:  98%|█████████▊| 2955/3000 [00:01<00:00, 1612.19it/s]warmup should be done:  97%|█████████▋| 2913/3000 [00:01<00:00, 1621.23it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1627.03it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1623.31it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1620.53it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1617.43it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1607.23it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1605.42it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1601.53it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1595.75it/s]






warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1659.14it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1646.63it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1674.37it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1664.13it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1663.94it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1643.81it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1662.69it/s]warmup should be done:   5%|▌         | 164/3000 [00:00<00:01, 1630.20it/s]warmup should be done:  11%|█         | 336/3000 [00:00<00:01, 1677.60it/s]warmup should be done:  11%|█         | 335/3000 [00:00<00:01, 1672.88it/s]warmup should be done:  11%|█         | 333/3000 [00:00<00:01, 1661.30it/s]warmup should be done:  11%|█         | 336/3000 [00:00<00:01, 1678.08it/s]warmup should be done:  11%|█         | 331/3000 [00:00<00:01, 1651.53it/s]warmup should be done:  11%|█         | 334/3000 [00:00<00:01, 1665.00it/s]warmup should be done:  11%|█         | 329/3000 [00:00<00:01, 1639.53it/s]warmup should be done:  11%|█         | 332/3000 [00:00<00:01, 1654.88it/s]warmup should be done:  17%|█▋        | 505/3000 [00:00<00:01, 1682.13it/s]warmup should be done:  17%|█▋        | 503/3000 [00:00<00:01, 1675.73it/s]warmup should be done:  17%|█▋        | 506/3000 [00:00<00:01, 1686.93it/s]warmup should be done:  17%|█▋        | 501/3000 [00:00<00:01, 1666.97it/s]warmup should be done:  17%|█▋        | 500/3000 [00:00<00:01, 1662.03it/s]warmup should be done:  17%|█▋        | 498/3000 [00:00<00:01, 1657.35it/s]warmup should be done:  17%|█▋        | 499/3000 [00:00<00:01, 1661.44it/s]warmup should be done:  16%|█▋        | 494/3000 [00:00<00:01, 1640.44it/s]warmup should be done:  22%|██▏       | 674/3000 [00:00<00:01, 1684.57it/s]warmup should be done:  22%|██▏       | 669/3000 [00:00<00:01, 1671.13it/s]warmup should be done:  22%|██▏       | 671/3000 [00:00<00:01, 1674.85it/s]warmup should be done:  22%|██▏       | 667/3000 [00:00<00:01, 1665.03it/s]warmup should be done:  22%|██▏       | 664/3000 [00:00<00:01, 1658.24it/s]warmup should be done:  22%|██▏       | 666/3000 [00:00<00:01, 1660.47it/s]warmup should be done:  22%|██▏       | 659/3000 [00:00<00:01, 1638.19it/s]warmup should be done:  22%|██▎       | 675/3000 [00:00<00:01, 1670.92it/s]warmup should be done:  28%|██▊       | 834/3000 [00:00<00:01, 1666.48it/s]warmup should be done:  28%|██▊       | 843/3000 [00:00<00:01, 1684.08it/s]warmup should be done:  28%|██▊       | 839/3000 [00:00<00:01, 1674.52it/s]warmup should be done:  28%|██▊       | 830/3000 [00:00<00:01, 1656.48it/s]warmup should be done:  28%|██▊       | 838/3000 [00:00<00:01, 1674.31it/s]warmup should be done:  28%|██▊       | 833/3000 [00:00<00:01, 1662.42it/s]warmup should be done:  27%|██▋       | 824/3000 [00:00<00:01, 1638.98it/s]warmup should be done:  28%|██▊       | 844/3000 [00:00<00:01, 1677.38it/s]warmup should be done:  33%|███▎      | 1001/3000 [00:00<00:01, 1666.16it/s]warmup should be done:  34%|███▎      | 1007/3000 [00:00<00:01, 1675.89it/s]warmup should be done:  34%|███▎      | 1012/3000 [00:00<00:01, 1682.39it/s]warmup should be done:  33%|███▎      | 996/3000 [00:00<00:01, 1655.94it/s]warmup should be done:  34%|███▎      | 1006/3000 [00:00<00:01, 1672.91it/s]warmup should be done:  33%|███▎      | 1001/3000 [00:00<00:01, 1666.06it/s]warmup should be done:  33%|███▎      | 989/3000 [00:00<00:01, 1641.04it/s]warmup should be done:  34%|███▍      | 1013/3000 [00:00<00:01, 1680.08it/s]warmup should be done:  39%|███▉      | 1175/3000 [00:00<00:01, 1676.26it/s]warmup should be done:  39%|███▉      | 1168/3000 [00:00<00:01, 1665.02it/s]warmup should be done:  39%|███▉      | 1181/3000 [00:00<00:01, 1682.66it/s]warmup should be done:  39%|███▉      | 1163/3000 [00:00<00:01, 1658.09it/s]warmup should be done:  39%|███▉      | 1168/3000 [00:00<00:01, 1666.70it/s]warmup should be done:  39%|███▉      | 1174/3000 [00:00<00:01, 1669.38it/s]warmup should be done:  38%|███▊      | 1154/3000 [00:00<00:01, 1641.48it/s]warmup should be done:  39%|███▉      | 1182/3000 [00:00<00:01, 1682.23it/s]warmup should be done:  45%|████▍     | 1343/3000 [00:00<00:00, 1675.06it/s]warmup should be done:  44%|████▍     | 1335/3000 [00:00<00:01, 1664.89it/s]warmup should be done:  45%|████▌     | 1350/3000 [00:00<00:00, 1682.75it/s]warmup should be done:  44%|████▍     | 1335/3000 [00:00<00:00, 1667.03it/s]warmup should be done:  44%|████▍     | 1329/3000 [00:00<00:01, 1655.20it/s]warmup should be done:  45%|████▍     | 1342/3000 [00:00<00:00, 1672.27it/s]warmup should be done:  44%|████▍     | 1319/3000 [00:00<00:01, 1639.50it/s]warmup should be done:  45%|████▌     | 1351/3000 [00:00<00:00, 1665.62it/s]warmup should be done:  50%|█████     | 1502/3000 [00:00<00:00, 1664.35it/s]warmup should be done:  50%|█████     | 1511/3000 [00:00<00:00, 1673.22it/s]warmup should be done:  50%|████▉     | 1495/3000 [00:00<00:00, 1656.37it/s]warmup should be done:  50%|█████     | 1503/3000 [00:00<00:00, 1668.84it/s]warmup should be done:  51%|█████     | 1519/3000 [00:00<00:00, 1680.36it/s]warmup should be done:  50%|█████     | 1510/3000 [00:00<00:00, 1672.89it/s]warmup should be done:  49%|████▉     | 1484/3000 [00:00<00:00, 1640.15it/s]warmup should be done:  51%|█████     | 1518/3000 [00:00<00:00, 1662.89it/s]warmup should be done:  56%|█████▌    | 1669/3000 [00:01<00:00, 1664.44it/s]warmup should be done:  56%|█████▌    | 1671/3000 [00:01<00:00, 1671.94it/s]warmup should be done:  56%|█████▌    | 1679/3000 [00:01<00:00, 1673.04it/s]warmup should be done:  55%|█████▌    | 1662/3000 [00:01<00:00, 1658.73it/s]warmup should be done:  56%|█████▋    | 1688/3000 [00:01<00:00, 1678.59it/s]warmup should be done:  56%|█████▌    | 1678/3000 [00:01<00:00, 1667.41it/s]warmup should be done:  55%|█████▍    | 1649/3000 [00:01<00:00, 1642.17it/s]warmup should be done:  56%|█████▌    | 1685/3000 [00:01<00:00, 1661.47it/s]warmup should be done:  61%|██████▏   | 1839/3000 [00:01<00:00, 1673.57it/s]warmup should be done:  61%|██████    | 1829/3000 [00:01<00:00, 1660.82it/s]warmup should be done:  62%|██████▏   | 1847/3000 [00:01<00:00, 1672.75it/s]warmup should be done:  62%|██████▏   | 1857/3000 [00:01<00:00, 1680.98it/s]warmup should be done:  60%|██████    | 1814/3000 [00:01<00:00, 1643.56it/s]warmup should be done:  61%|██████    | 1836/3000 [00:01<00:00, 1648.13it/s]warmup should be done:  62%|██████▏   | 1845/3000 [00:01<00:00, 1661.16it/s]warmup should be done:  62%|██████▏   | 1852/3000 [00:01<00:00, 1662.31it/s]warmup should be done:  67%|██████▋   | 2015/3000 [00:01<00:00, 1672.39it/s]warmup should be done:  67%|██████▋   | 2007/3000 [00:01<00:00, 1671.31it/s]warmup should be done:  67%|██████▋   | 1996/3000 [00:01<00:00, 1658.36it/s]warmup should be done:  68%|██████▊   | 2026/3000 [00:01<00:00, 1681.40it/s]warmup should be done:  66%|██████▌   | 1979/3000 [00:01<00:00, 1642.21it/s]warmup should be done:  67%|██████▋   | 2001/3000 [00:01<00:00, 1645.41it/s]warmup should be done:  67%|██████▋   | 2012/3000 [00:01<00:00, 1654.14it/s]warmup should be done:  67%|██████▋   | 2019/3000 [00:01<00:00, 1661.82it/s]warmup should be done:  73%|███████▎  | 2183/3000 [00:01<00:00, 1671.64it/s]warmup should be done:  72%|███████▎  | 2175/3000 [00:01<00:00, 1669.94it/s]warmup should be done:  72%|███████▏  | 2162/3000 [00:01<00:00, 1656.86it/s]warmup should be done:  73%|███████▎  | 2195/3000 [00:01<00:00, 1680.37it/s]warmup should be done:  71%|███████▏  | 2144/3000 [00:01<00:00, 1643.41it/s]warmup should be done:  72%|███████▏  | 2166/3000 [00:01<00:00, 1643.58it/s]warmup should be done:  73%|███████▎  | 2178/3000 [00:01<00:00, 1648.04it/s]warmup should be done:  73%|███████▎  | 2186/3000 [00:01<00:00, 1660.29it/s]warmup should be done:  78%|███████▊  | 2351/3000 [00:01<00:00, 1673.33it/s]warmup should be done:  78%|███████▊  | 2328/3000 [00:01<00:00, 1656.71it/s]warmup should be done:  78%|███████▊  | 2343/3000 [00:01<00:00, 1670.47it/s]warmup should be done:  79%|███████▉  | 2364/3000 [00:01<00:00, 1679.79it/s]warmup should be done:  77%|███████▋  | 2311/3000 [00:01<00:00, 1648.70it/s]warmup should be done:  78%|███████▊  | 2331/3000 [00:01<00:00, 1644.04it/s]warmup should be done:  78%|███████▊  | 2343/3000 [00:01<00:00, 1646.75it/s]warmup should be done:  78%|███████▊  | 2353/3000 [00:01<00:00, 1659.80it/s]warmup should be done:  84%|████████▍ | 2519/3000 [00:01<00:00, 1674.87it/s]warmup should be done:  83%|████████▎ | 2495/3000 [00:01<00:00, 1658.71it/s]warmup should be done:  84%|████████▎ | 2511/3000 [00:01<00:00, 1670.83it/s]warmup should be done:  84%|████████▍ | 2534/3000 [00:01<00:00, 1685.63it/s]warmup should be done:  83%|████████▎ | 2477/3000 [00:01<00:00, 1651.21it/s]warmup should be done:  83%|████████▎ | 2498/3000 [00:01<00:00, 1650.22it/s]warmup should be done:  84%|████████▎ | 2511/3000 [00:01<00:00, 1655.48it/s]warmup should be done:  84%|████████▍ | 2519/3000 [00:01<00:00, 1657.43it/s]warmup should be done:  90%|████████▉ | 2687/3000 [00:01<00:00, 1675.28it/s]warmup should be done:  89%|████████▊ | 2661/3000 [00:01<00:00, 1657.14it/s]warmup should be done:  89%|████████▉ | 2679/3000 [00:01<00:00, 1671.14it/s]warmup should be done:  90%|█████████ | 2705/3000 [00:01<00:00, 1691.16it/s]warmup should be done:  88%|████████▊ | 2644/3000 [00:01<00:00, 1654.90it/s]warmup should be done:  89%|████████▉ | 2664/3000 [00:01<00:00, 1651.99it/s]warmup should be done:  89%|████████▉ | 2679/3000 [00:01<00:00, 1661.88it/s]warmup should be done:  90%|████████▉ | 2685/3000 [00:01<00:00, 1653.90it/s]warmup should be done:  95%|█████████▌| 2855/3000 [00:01<00:00, 1670.54it/s]warmup should be done:  94%|█████████▍| 2827/3000 [00:01<00:00, 1655.38it/s]warmup should be done:  96%|█████████▌| 2875/3000 [00:01<00:00, 1693.23it/s]warmup should be done:  95%|█████████▍| 2847/3000 [00:01<00:00, 1669.79it/s]warmup should be done:  94%|█████████▎| 2811/3000 [00:01<00:00, 1658.15it/s]warmup should be done:  94%|█████████▍| 2830/3000 [00:01<00:00, 1654.26it/s]warmup should be done:  95%|█████████▍| 2847/3000 [00:01<00:00, 1665.31it/s]warmup should be done:  95%|█████████▌| 2851/3000 [00:01<00:00, 1652.30it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1685.03it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1673.39it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1668.14it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1664.66it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1663.86it/s]warmup should be done: 100%|█████████▉| 2993/3000 [00:01<00:00, 1656.63it/s]warmup should be done:  99%|█████████▉| 2978/3000 [00:01<00:00, 1660.89it/s]warmup should be done: 100%|█████████▉| 2997/3000 [00:01<00:00, 1658.18it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1656.74it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1656.61it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1647.05it/s]2022-12-12 03:49:15.660208: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f9b2b82c150 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 03:49:15.660272: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 03:49:15.667964: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f9b3f830eb0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 03:49:15.668009: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 03:49:15.674537: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f7e1c030eb0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 03:49:15.674597: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 03:49:15.690753: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f9b2bf92500 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 03:49:15.690812: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 03:49:15.857921: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f7d5c02e7d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 03:49:15.858020: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 03:49:15.934359: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f9b2782ced0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 03:49:15.934434: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 03:49:16.112935: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f9b27830230 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 03:49:16.113000: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 03:49:16.154810: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f9b2b832f80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 03:49:16.154880: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 03:49:17.922510: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 03:49:17.966700: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 03:49:17.978523: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 03:49:18.118055: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 03:49:18.184374: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 03:49:18.229909: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 03:49:18.464301: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 03:49:18.516383: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 03:49:20.772635: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 03:49:20.844161: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 03:49:20.952587: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 03:49:21.014993: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 03:49:21.030776: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 03:49:21.153175: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 03:49:21.422741: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 03:49:21.426896: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][03:49:47.245][ERROR][RK0][main]: replica 0 reaches 1000, calling init pre replica
[HCTR][03:49:47.245][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][03:49:47.252][ERROR][RK0][main]: coll ps creation done
[HCTR][03:49:47.252][ERROR][RK0][main]: replica 0 waits for coll ps creation barrier
[HCTR][03:49:47.266][ERROR][RK0][tid #140304996869888]: replica 1 reaches 1000, calling init pre replica
[HCTR][03:49:47.266][ERROR][RK0][tid #140304996869888]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][03:49:47.272][ERROR][RK0][tid #140304996869888]: coll ps creation done
[HCTR][03:49:47.272][ERROR][RK0][tid #140304996869888]: replica 1 waits for coll ps creation barrier
[HCTR][03:49:47.310][ERROR][RK0][main]: replica 7 reaches 1000, calling init pre replica
[HCTR][03:49:47.310][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][03:49:47.318][ERROR][RK0][main]: coll ps creation done
[HCTR][03:49:47.318][ERROR][RK0][main]: replica 7 waits for coll ps creation barrier
[HCTR][03:49:47.348][ERROR][RK0][tid #140306339047168]: replica 2 reaches 1000, calling init pre replica
[HCTR][03:49:47.348][ERROR][RK0][tid #140306339047168]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][03:49:47.356][ERROR][RK0][tid #140306339047168]: coll ps creation done
[HCTR][03:49:47.356][ERROR][RK0][tid #140306339047168]: replica 2 waits for coll ps creation barrier
[HCTR][03:49:47.359][ERROR][RK0][tid #140305005262592]: replica 6 reaches 1000, calling init pre replica
[HCTR][03:49:47.359][ERROR][RK0][tid #140305005262592]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][03:49:47.364][ERROR][RK0][tid #140305005262592]: coll ps creation done
[HCTR][03:49:47.364][ERROR][RK0][tid #140305005262592]: replica 6 waits for coll ps creation barrier
[HCTR][03:49:47.391][ERROR][RK0][main]: replica 3 reaches 1000, calling init pre replica
[HCTR][03:49:47.391][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][03:49:47.399][ERROR][RK0][main]: coll ps creation done
[HCTR][03:49:47.399][ERROR][RK0][main]: replica 3 waits for coll ps creation barrier
[HCTR][03:49:47.465][ERROR][RK0][tid #140304996869888]: replica 4 reaches 1000, calling init pre replica
[HCTR][03:49:47.465][ERROR][RK0][tid #140304996869888]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][03:49:47.473][ERROR][RK0][tid #140304996869888]: coll ps creation done
[HCTR][03:49:47.473][ERROR][RK0][tid #140304996869888]: replica 4 waits for coll ps creation barrier
[HCTR][03:49:47.478][ERROR][RK0][main]: replica 5 reaches 1000, calling init pre replica
[HCTR][03:49:47.478][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][03:49:47.485][ERROR][RK0][main]: coll ps creation done
[HCTR][03:49:47.485][ERROR][RK0][main]: replica 5 waits for coll ps creation barrier
[HCTR][03:49:47.485][ERROR][RK0][main]: replica 0 preparing frequency
[HCTR][03:49:48.534][ERROR][RK0][main]: replica 0 preparing frequency done
[HCTR][03:49:48.574][ERROR][RK0][main]: replica 0 calling init per replica
[HCTR][03:49:48.574][ERROR][RK0][main]: replica 3 calling init per replica
[HCTR][03:49:48.574][ERROR][RK0][main]: replica 7 calling init per replica
[HCTR][03:49:48.574][ERROR][RK0][tid #140304996869888]: replica 4 calling init per replica
[HCTR][03:49:48.574][ERROR][RK0][tid #140305005262592]: replica 6 calling init per replica
[HCTR][03:49:48.574][ERROR][RK0][tid #140304996869888]: replica 1 calling init per replica
[HCTR][03:49:48.574][ERROR][RK0][tid #140306339047168]: replica 2 calling init per replica
[HCTR][03:49:48.574][ERROR][RK0][main]: replica 5 calling init per replica
[HCTR][03:49:48.574][ERROR][RK0][main]: Calling build_v2
[HCTR][03:49:48.574][ERROR][RK0][main]: Calling build_v2
[HCTR][03:49:48.574][ERROR][RK0][main]: Calling build_v2
[HCTR][03:49:48.574][ERROR][RK0][tid #140304996869888]: Calling build_v2
[HCTR][03:49:48.574][ERROR][RK0][tid #140305005262592]: Calling build_v2
[HCTR][03:49:48.574][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][03:49:48.574][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][03:49:48.574][ERROR][RK0][tid #140304996869888]: Calling build_v2
[HCTR][03:49:48.574][ERROR][RK0][tid #140306339047168]: Calling build_v2
[HCTR][03:49:48.574][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][03:49:48.574][ERROR][RK0][main]: Calling build_v2
[HCTR][03:49:48.574][ERROR][RK0][tid #140304996869888]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][03:49:48.574][ERROR][RK0][tid #140305005262592]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][03:49:48.574][ERROR][RK0][tid #140304996869888]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][03:49:48.574][ERROR][RK0][tid #140306339047168]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][03:49:48.574][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[[[[[[2022-12-12 03:49:48[2022-12-12 03:49:482022-12-12 03:49:482022-12-12 03:49:48.2022-12-12 03:49:48[...2022-12-12 03:49:485748822022-12-12 03:49:48.5748795748785748892022-12-12 03:49:48.: .574878: : : .574898E574905: EEE574909:  : E   : E/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.ccE /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.ccE : /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc::: /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc136/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:136136136/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:] :136] ] ] :136using concurrent impl MPS136] using concurrent impl MPSusing concurrent impl MPSusing concurrent impl MPS136] 
] using concurrent impl MPS


] using concurrent impl MPSusing concurrent impl MPS
using concurrent impl MPS


[2022-12-12 03:49:48.579243: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-12 03:49:48.579282: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[:2022-12-12 03:49:48196.] 579289assigning 8 to cpu: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-12 03:49:48.579336: [E2022-12-12 03:49:48 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc579337:[: 1962022-12-12 03:49:48E] . assigning 8 to cpu579359/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
: :E178[ ] 2022-12-12 03:49:48/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccv100x8, slow pcie.:
579387212: [] E[2022-12-12 03:49:48build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8 2022-12-12 03:49:48.
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.[579415:5794192022-12-12 03:49:48: 178: .E] [E579431 v100x8, slow pcie2022-12-12 03:49:48 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE:[579456: [2122022-12-12 03:49:48: 196/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 03:49:48] .E] :.build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8579475 assigning 8 to cpu178579487
: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
] : [E:v100x8, slow pcieE[2022-12-12 03:49:48 213
 2022-12-12 03:49:48./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[.[579528:remote time is 8.68421:2022-12-12 03:49:48[5795472022-12-12 03:49:48: 178
196.2022-12-12 03:49:48: .E] ] 579575[.E579578 v100x8, slow pcieassigning 8 to cpu: 2022-12-12 03:49:48579579 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc

E.[: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE: 5796382022-12-12 03:49:48E: 178/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: . [213/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] :E579718/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 03:49:48] :v100x8, slow pcie212 : :.remote time is 8.68421196
] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE178579753
] build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8:[ ] : assigning 8 to cpu
[2142022-12-12 03:49:48/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccv100x8, slow pcieE
2022-12-12 03:49:48] .[:
 .cpu time is 97.05885798522022-12-12 03:49:48196/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc579870
[: .[] :: 2022-12-12 03:49:48E5799032022-12-12 03:49:48assigning 8 to cpu212E. : .
]  579948/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE579954build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: : : 
:E196/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE[214 [] : 2022-12-12 03:49:48] cpu time is 97.0588/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 03:49:48assigning 8 to cpu213/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.
:.
] :580065196580077remote time is 8.68421212: ] : 
] E[assigning 8 to cpuEbuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8 [2022-12-12 03:49:48
 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 03:49:48./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:.580183[:212580200: 2022-12-12 03:49:48213] : E.[] build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8E 5802392022-12-12 03:49:48remote time is 8.68421
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: .
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:[E[2022-12-12 03:49:48580268:2122022-12-12 03:49:48 .: 214] ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc580330E] build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8580345::  cpu time is 97.0588
: 213E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
E]  [: remote time is 8.68421/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 03:49:48212/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
:.] :213580459[build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8214] : 2022-12-12 03:49:48
] remote time is 8.68421E.cpu time is 97.0588
 580507
[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: [2022-12-12 03:49:48:E2022-12-12 03:49:48.213 .580567] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc580577: remote time is 8.68421:: E
214E ]  [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cccpu time is 97.0588/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 03:49:48:
:.213214580670] ] : remote time is 8.68421cpu time is 97.0588E

 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:214] [cpu time is 97.05882022-12-12 03:49:48
.580772: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:214] cpu time is 97.0588
[2022-12-12 03:51:07.557611: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-12 03:51:07.597700: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 381.47 MB
[2022-12-12 03:51:07.597793: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 381.47 MB
[2022-12-12 03:51:07.598908: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:74] mapping nid to rank...
[2022-12-12 03:51:07.677024: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:91] counting slots...
[2022-12-12 03:51:08. 78996: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:105] Final num slot is 49
[2022-12-12 03:51:08. 79088: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:109] counting blocks...
[2022-12-12 03:51:13.683979: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:118] Final num block is 1024
[2022-12-12 03:51:13.684077: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:123] counting freq and density...
[2022-12-12 03:51:15.398054: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:154] averaging freq and density...
[2022-12-12 03:51:15.398166: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:155] 1024
[2022-12-12 03:51:15.401145: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
[2022-12-12 03:51:15.401205: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:354] constructing optimal solver, device=8, stream=1
1024 blocks, 8 devices
[2022-12-12 03:51:15.706490: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:527] Add Var...
[2022-12-12 03:51:15.735222: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:545] Capacity...
[2022-12-12 03:51:15.736704: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:548] Connect CPU...
[2022-12-12 03:51:15.756912: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:550] Connect Access To Storage...
[2022-12-12 03:51:16.275694: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:554] Time...
[2022-12-12 03:54:37.372385: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:569] Coll Cache init block placement array
[2022-12-12 03:54:37.381827: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:645] Coll Cache init block placement array done
[2022-12-12 03:54:37.396958: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:647] Coll Cache model reset done
[2022-12-12 03:54:37.443963: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-12 03:54:37.444065: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-12 03:54:37.444097: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-12 03:54:37.444127: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-12 03:54:37.444688: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 03:54:37.444740: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:54:37.445654: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:54:37.446299: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:54:37.459449: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 5 solved
[2022-12-12 03:54:37.459533: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 5 initing device 5
[2022-12-12 03:54:37.459673: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 4 solved
[2022-12-12 03:54:37.459755: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 4 initing device 4
[2022-12-12 03:54:37.459923: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 2 solved
[[2022-12-12 03:54:372022-12-12 03:54:37..459981459988: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::1815205] ] Building Coll Cache with ... num gpu device is 8worker 0 thread 2 initing device 2

[2022-12-12 03:54:37.460102: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:54:37.460224: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 03:54:37.460277: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-12 03:54:37:.1980460274] : eager alloc mem 381.47 MBE
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] [1 solved2022-12-12 03:54:37
.460340: E[ 2022-12-12 03:54:37/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc.:460394202: ] E7 solved 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] [worker 0 thread 1 initing device 12022-12-12 03:54:37
.460452: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 7 initing device 7
[2022-12-12 03:54:37.460508: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 03:54:37.460563: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:54:37.460891: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815[] 2022-12-12 03:54:37Building Coll Cache with ... num gpu device is 8.
460910: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8[
2022-12-12 03:54:37.460951: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] [eager alloc mem 381.47 MB2022-12-12 03:54:37
.460975: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:54:37.463365: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:54:37.463587: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:54:37.463646: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:54:37.463712: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:54:37.464199: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:54:37.464421: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 6 solved
[[2022-12-12 03:54:372022-12-12 03:54:37..464455464476: : E E/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc205:] 202worker 0 thread 6 initing device 6] 
3 solved
[2022-12-12 03:54:37.464566: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 3 initing device 3
[2022-12-12 03:54:37.464976: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] [Building Coll Cache with ... num gpu device is 82022-12-12 03:54:37
.464998: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] [Building Coll Cache with ... num gpu device is 82022-12-12 03:54:37
.465029: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] [eager alloc mem 381.47 MB2022-12-12 03:54:37
.465058: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:54:37.467653: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:54:37.467769: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:54:37.467858: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:54:37.468051: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:54:37.468541: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:54:37.469062: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:54:37.469564: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:54:37.472616: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:54:37.472668: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 03:54:37.523003: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-12 03:54:37.528730: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 03:54:37.528877: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 03:54:37.529689: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 57.60 MB
[2022-12-12 03:54:37.530333: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:54:37.531418: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:54:37.531469: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 57.22 MB
[2022-12-12 03:54:37.547575: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[[2022-12-12 03:54:37[[2022-12-12 03:54:37[[.2022-12-12 03:54:372022-12-12 03:54:37.2022-12-12 03:54:372022-12-12 03:54:37547644..547644..: 547648547648: 547648547648E: : E: :  EE EE/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu  :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980::1980::] 19801980] 19801980eager alloc mem 1024.00 Bytes] ] eager alloc mem 1024.00 Bytes] ] 
eager alloc mem 1024.00 Byteseager alloc mem 1024.00 Bytes
eager alloc mem 1024.00 Byteseager alloc mem 1024.00 Bytes



[2022-12-12 03:54:37.553859: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 03:54:37.553939: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 03:54:37.554080: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 03:54:37.554161: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 03:54:37.554445: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 03:54:37.554531: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 03:54:37.554585: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 03:54:37.554654: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] [eager release cuda mem 10242022-12-12 03:54:37
.554696: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:[6382022-12-12 03:54:37] .eager release cuda mem 400000000554712
: [[E2022-12-12 03:54:372022-12-12 03:54:37 ../hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu554744554733:: : 1980EE]   eager alloc mem 57.60 MB/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[
::2022-12-12 03:54:37638638.] ] 554809eager release cuda mem 400000000eager release cuda mem 1024: 

E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 03:54:37.554909: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-12 03:54:37:.638554921] : eager release cuda mem 400000000E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 03:54:37.555979: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 57.60 MB
[2022-12-12 03:54:37.556483: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 57.60 MB
[2022-12-12 03:54:37.557151: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 57.60 MB
[2022-12-12 03:54:37.558130: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 57.60 MB
[2022-12-12 03:54:37.558749: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 57.60 MB
[2022-12-12 03:54:37.559284: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 57.60 MB
[2022-12-12 03:54:37.559617: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:54:37.560366: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:54:37.560472: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:54:37.560610: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:54:37.560661: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:54:37.560707: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 57.17 MB
[2022-12-12 03:54:37.560757: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:54:37.560804: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:54:37.560840: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:54:37.561395: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:54:37.561441: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 57.15 MB
[2022-12-12 03:54:37.561522: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:54:37.561569: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 57.09 MB
[2022-12-12 03:54:37.561654: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:54:37.561705: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 57.20 MB
[2022-12-12 03:54:37.561779: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:54:37.561827: W [/hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc2022-12-12 03:54:37:.43561834] : WORKER[0] alloc host memory 57.22 MBE
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:[6382022-12-12 03:54:37] .eager release cuda mem 625663561863
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:54:37.561901: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:[432022-12-12 03:54:37] .WORKER[0] alloc host memory 57.21 MB561918
: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 57.03 MB
[2022-12-12 03:54:37.570669: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 03:54:37.571321: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 03:54:37.571366: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 7.16 GB
[2022-12-12 03:54:37.598034: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 03:54:37.598650: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 03:54:37.598693: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 7.15 GB
[2022-12-12 03:54:37.599018: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 03:54:37.599630: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 03:54:37.599674: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 7.15 GB
[2022-12-12 03:54:37.599858: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 03:54:37.600459: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 03:54:37.600500: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 7.14 GB
[2022-12-12 03:54:37.600510: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 03:54:37.601137: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 03:54:37.601178: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[:2022-12-12 03:54:371980.] 601173eager alloc mem 7.15 GB: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 03:54:37.601241: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 03:54:37.601581: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 03:54:37.601801: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 03:54:37.601844: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[:2022-12-12 03:54:371980.] 601852eager alloc mem 7.16 GB: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 03:54:37.601906: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 7.16 GB
[2022-12-12 03:54:37.602190: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 03:54:37.602232: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 7.13 GB
[[[[[[[[2022-12-12 03:54:402022-12-12 03:54:402022-12-12 03:54:402022-12-12 03:54:402022-12-12 03:54:402022-12-12 03:54:402022-12-12 03:54:402022-12-12 03:54:40........ 54782 54782 54782 54782 54782 54782 54782 54784: : : : : : : : EEEEEEEE        /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::::::19261926192619261926192619261926] ] ] ] ] ] ] ] Device 2 init p2p of link 1Device 7 init p2p of link 4Device 6 init p2p of link 0Device 5 init p2p of link 6Device 1 init p2p of link 7Device 4 init p2p of link 5Device 3 init p2p of link 2Device 0 init p2p of link 3







[2022-12-12 03:54:40. 55399[[[[: 2022-12-12 03:54:40[[2022-12-12 03:54:402022-12-12 03:54:402022-12-12 03:54:40E.2022-12-12 03:54:402022-12-12 03:54:40...  55403.[. 55403 55409 55403/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:  554112022-12-12 03:54:40 55410: : : :E: .: EEE1980 E 55456E   ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 611.00 KB:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:::
1980: :198019801980] 1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980] ] ] eager alloc mem 611.00 KB] :] eager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KB
eager alloc mem 611.00 KB1980eager alloc mem 611.00 KB



] 
eager alloc mem 611.00 KB
[2022-12-12 03:54:40. 56516: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:54:40. 56544: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:54:40.[ 565752022-12-12 03:54:40: .[E 56581[[2022-12-12 03:54:40 : 2022-12-12 03:54:40[2022-12-12 03:54:40./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE.2022-12-12 03:54:40. 56599:  56600. 56601: 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:  56619: E] :E: E eager release cuda mem 625663638 E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:eager release cuda mem 625663:/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638
638:638] ] 638] eager release cuda mem 625663eager release cuda mem 625663] eager release cuda mem 625663

eager release cuda mem 625663

[2022-12-12 03:54:40. 72224: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 5
[2022-12-12 03:54:40. 72383: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:54:40. 73289: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:54:40. 73801: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 2
[2022-12-12 03:54:40. 73948: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:54:40. 74565: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 4
[2022-12-12 03:54:40. 74720: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:54:40. 74741: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 1
[2022-12-12 03:54:40. 74852: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:54:40. 74908: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:54:40. 75621: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:54:40. 75722: [E2022-12-12 03:54:40 .[/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu 757382022-12-12 03:54:40:: .1926E 75760]  : Device 2 init p2p of link 3/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE
: 1926/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] :Device 4 init p2p of link 7638
] eager release cuda mem 625663
[2022-12-12 03:54:40. 75897: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[[:2022-12-12 03:54:40[2022-12-12 03:54:401926.2022-12-12 03:54:40.]  75920. 75930Device 0 init p2p of link 6:  75940: 
E: E E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926:1980] 1980] Device 3 init p2p of link 0] eager alloc mem 611.00 KB
eager alloc mem 611.00 KB

[2022-12-12 03:54:40. 76121: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:54:40. 76201: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:54:40. 76914: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] [eager release cuda mem 6256632022-12-12 03:54:40
. 76937: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:54:40. 76970: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:54:40. 77086: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:54:40. 82865: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 4
[2022-12-12 03:54:40. 82990: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:54:40. 83853: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:54:40. 91709: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 7
[2022-12-12 03:54:40. 91767: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 3
[2022-12-12 03:54:40. 91838: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:54:40. 91891: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:54:40. 92259: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 6
[2022-12-12 03:54:40. 92349: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] [Device 0 init p2p of link 12022-12-12 03:54:40
. 92390: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:54:40. 92506: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:54:40. 92692: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:54:40. 92749: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:54:40. 93128: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 0
[2022-12-12 03:54:40. 93253: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:54:40. 93288: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:54:40. 93421: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:54:40. 93578: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 2
[2022-12-12 03:54:40. 93702: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:54:40. 93948: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 5
[2022-12-12 03:54:40. 94084: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:54:40. 94113: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:54:40. 94555: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:54:40. 94906: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:54:40. 98315: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 7
[2022-12-12 03:54:40. 98440: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:54:40. 99307: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:54:40.114885: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 0
[2022-12-12 03:54:40.114966: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926[] 2022-12-12 03:54:40Device 3 init p2p of link 1.
115005: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:54:40.115108: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:54:40.115411: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 3
[2022-12-12 03:54:40.115529: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:54:40.115839: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 6
[2022-12-12 03:54:40.115884: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:54:40.115960: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] [eager alloc mem 611.00 KB2022-12-12 03:54:40
.115979: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:54:40.116363: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:54:40.116820: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:54:40.117236: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 5
[2022-12-12 03:54:40.117326: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926[] 2022-12-12 03:54:40Device 0 init p2p of link 2.
117360: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:54:40.117488: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:54:40.117533: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 4
[2022-12-12 03:54:40.117654: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 03:54:40.118253: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:54:40.118358: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:54:40.118482: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 03:54:40.120142: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 60400000
[2022-12-12 03:54:40.124172: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 14997518 / 100000000 nodes ( 15.00 %~15.00 %) | remote 44817420 / 100000000 nodes ( 44.82 %) | cpu 40185062 / 100000000 nodes ( 40.19 %) | 7.16 GB | 2.65915 secs 
[2022-12-12 03:54:40.137369: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 60400000
[2022-12-12 03:54:40.137962: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 60400000
[2022-12-12 03:54:40.138566: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 60400000
[2022-12-12 03:54:40.138891: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 60400000
[2022-12-12 03:54:40.139309: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 60400000
[2022-12-12 03:54:40.139789: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 60400000
[2022-12-12 03:54:40.140874: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 60400000
[2022-12-12 03:54:40.160494: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 14980702 / 100000000 nodes ( 14.98 %~15.00 %) | remote 44834236 / 100000000 nodes ( 44.83 %) | cpu 40185062 / 100000000 nodes ( 40.19 %) | 7.15 GB | 2.7004 secs 
[2022-12-12 03:54:40.160652: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 14994725 / 100000000 nodes ( 14.99 %~15.00 %) | remote 44820213 / 100000000 nodes ( 44.82 %) | cpu 40185062 / 100000000 nodes ( 40.19 %) | 7.15 GB | 2.7001 secs 
[2022-12-12 03:54:40.160787: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 14985754 / 100000000 nodes ( 14.99 %~15.00 %) | remote 44829184 / 100000000 nodes ( 44.83 %) | cpu 40185062 / 100000000 nodes ( 40.19 %) | 7.15 GB | 2.69984 secs 
[2022-12-12 03:54:40.160928: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 14965713 / 100000000 nodes ( 14.97 %~15.00 %) | remote 44849225 / 100000000 nodes ( 44.85 %) | cpu 40185062 / 100000000 nodes ( 40.19 %) | 7.14 GB | 2.69588 secs 
[2022-12-12 03:54:40.161082: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 14950090 / 100000000 nodes ( 14.95 %~15.00 %) | remote 44864848 / 100000000 nodes ( 44.86 %) | cpu 40185062 / 100000000 nodes ( 40.19 %) | 7.13 GB | 2.70082 secs 
[2022-12-12 03:54:40.161219: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 14999134 / 100000000 nodes ( 15.00 %~15.00 %) | remote 44815804 / 100000000 nodes ( 44.82 %) | cpu 40185062 / 100000000 nodes ( 40.19 %) | 7.16 GB | 2.70025 secs 
[2022-12-12 03:54:40.161413: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 14999005 / 100000000 nodes ( 15.00 %~15.00 %) | remote 44815933 / 100000000 nodes ( 44.82 %) | cpu 40185062 / 100000000 nodes ( 40.19 %) | 7.16 GB | 2.71669 secs 
[2022-12-12 03:54:40.161851: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 13.80 GB
[2022-12-12 03:54:41.459646: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 14.06 GB
[2022-12-12 03:54:41.459818: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 14.06 GB
[2022-12-12 03:54:41.460712: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 14.06 GB
[2022-12-12 03:54:42.786770: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 14.32 GB
[2022-12-12 03:54:42.787004: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 14.32 GB
[2022-12-12 03:54:42.787474: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 14.32 GB
[2022-12-12 03:54:44.160140: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 14.54 GB
[2022-12-12 03:54:44.160276: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 14.54 GB
[2022-12-12 03:54:44.160922: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 14.54 GB
[2022-12-12 03:54:45.463317: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 14.75 GB
[2022-12-12 03:54:45.463585: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 14.75 GB
[2022-12-12 03:54:45.464289: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 14.75 GB
[2022-12-12 03:54:46.642212: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 15.21 GB
[2022-12-12 03:54:46.642977: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 15.21 GB
[2022-12-12 03:54:46.643666: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 15.21 GB
[2022-12-12 03:54:47.972293: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 15.41 GB
[2022-12-12 03:54:47.972929: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 15.41 GB
[HCTR][03:54:49.083][ERROR][RK0][tid #140305005262592]: replica 6 calling init per replica done, doing barrier
[HCTR][03:54:49.083][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier
[HCTR][03:54:49.083][ERROR][RK0][tid #140304996869888]: replica 4 calling init per replica done, doing barrier
[HCTR][03:54:49.083][ERROR][RK0][tid #140304996869888]: replica 1 calling init per replica done, doing barrier
[HCTR][03:54:49.083][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier
[HCTR][03:54:49.083][ERROR][RK0][tid #140306339047168]: replica 2 calling init per replica done, doing barrier
[HCTR][03:54:49.083][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier
[HCTR][03:54:49.083][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier
[HCTR][03:54:49.083][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier done
[HCTR][03:54:49.083][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier done
[HCTR][03:54:49.083][ERROR][RK0][tid #140304996869888]: replica 1 calling init per replica done, doing barrier done
[HCTR][03:54:49.083][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier done
[HCTR][03:54:49.083][ERROR][RK0][tid #140304996869888]: replica 4 calling init per replica done, doing barrier done
[HCTR][03:54:49.083][ERROR][RK0][tid #140306339047168]: replica 2 calling init per replica done, doing barrier done
[HCTR][03:54:49.083][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier done
[HCTR][03:54:49.083][ERROR][RK0][tid #140305005262592]: replica 6 calling init per replica done, doing barrier done
[HCTR][03:54:49.083][ERROR][RK0][main]: init per replica done
[HCTR][03:54:49.083][ERROR][RK0][main]: init per replica done
[HCTR][03:54:49.083][ERROR][RK0][tid #140304996869888]: init per replica done
[HCTR][03:54:49.083][ERROR][RK0][tid #140304996869888]: init per replica done
[HCTR][03:54:49.083][ERROR][RK0][tid #140306339047168]: init per replica done
[HCTR][03:54:49.083][ERROR][RK0][main]: init per replica done
[HCTR][03:54:49.083][ERROR][RK0][tid #140305005262592]: init per replica done
[HCTR][03:54:49.086][ERROR][RK0][main]: init per replica done
[HCTR][03:54:49.089][ERROR][RK0][tid #140304996869888]: 1 allocated 3276800 at 0x7f9d23320000
[HCTR][03:54:49.089][ERROR][RK0][tid #140304996869888]: 1 allocated 6553600 at 0x7f81e2800000
[HCTR][03:54:49.089][ERROR][RK0][tid #140304996869888]: 1 allocated 3276800 at 0x7f81e2e40000
[HCTR][03:54:49.089][ERROR][RK0][tid #140304996869888]: 1 allocated 6553600 at 0x7f81e3160000
[HCTR][03:54:49.089][ERROR][RK0][tid #140304996869888]: 4 allocated 3276800 at 0x7f9d23320000
[HCTR][03:54:49.089][ERROR][RK0][tid #140304996869888]: 4 allocated 6553600 at 0x7f81be800000
[HCTR][03:54:49.089][ERROR][RK0][main]: 7 allocated 3276800 at 0x7f9d23320000
[HCTR][03:54:49.089][ERROR][RK0][tid #140304996869888]: 4 allocated 3276800 at 0x7f81bee40000
[HCTR][03:54:49.089][ERROR][RK0][main]: 7 allocated 6553600 at 0x7f80b6800000
[HCTR][03:54:49.089][ERROR][RK0][tid #140304996869888]: 4 allocated 6553600 at 0x7f81bf160000
[HCTR][03:54:49.089][ERROR][RK0][main]: 7 allocated 3276800 at 0x7f80b6e40000
[HCTR][03:54:49.089][ERROR][RK0][main]: 7 allocated 6553600 at 0x7f80b7160000
[HCTR][03:54:49.089][ERROR][RK0][main]: 5 allocated 3276800 at 0x7f9d23320000
[HCTR][03:54:49.089][ERROR][RK0][main]: 5 allocated 6553600 at 0x7f814a800000
[HCTR][03:54:49.089][ERROR][RK0][main]: 5 allocated 3276800 at 0x7f814ae40000
[HCTR][03:54:49.089][ERROR][RK0][main]: 5 allocated 6553600 at 0x7f814b160000
[HCTR][03:54:49.089][ERROR][RK0][main]: 3 allocated 3276800 at 0x7f9d21320000
[HCTR][03:54:49.089][ERROR][RK0][main]: 3 allocated 6553600 at 0x7f81ce800000
[HCTR][03:54:49.089][ERROR][RK0][main]: 3 allocated 3276800 at 0x7f81cee40000
[HCTR][03:54:49.089][ERROR][RK0][main]: 3 allocated 6553600 at 0x7f81cf160000
[HCTR][03:54:49.090][ERROR][RK0][tid #140305005262592]: 6 allocated 3276800 at 0x7f9d23320000
[HCTR][03:54:49.090][ERROR][RK0][tid #140305005262592]: 6 allocated 6553600 at 0x7f81c6800000
[HCTR][03:54:49.090][ERROR][RK0][tid #140305005262592]: 6 allocated 3276800 at 0x7f81c6e40000
[HCTR][03:54:49.090][ERROR][RK0][tid #140305005262592]: 6 allocated 6553600 at 0x7f81c7160000
[HCTR][03:54:49.090][ERROR][RK0][main]: 2 allocated 3276800 at 0x7f9d23320000
[HCTR][03:54:49.090][ERROR][RK0][main]: 2 allocated 6553600 at 0x7f8136800000
[HCTR][03:54:49.090][ERROR][RK0][main]: 2 allocated 3276800 at 0x7f8136e40000
[HCTR][03:54:49.090][ERROR][RK0][main]: 2 allocated 6553600 at 0x7f8137160000
[HCTR][03:54:49.092][ERROR][RK0][tid #140304929761024]: 0 allocated 3276800 at 0x7f844ed20000
[HCTR][03:54:49.093][ERROR][RK0][tid #140304929761024]: 0 allocated 6553600 at 0x7f844f200000
[HCTR][03:54:49.093][ERROR][RK0][tid #140304929761024]: 0 allocated 3276800 at 0x7f80c650e800
[HCTR][03:54:49.093][ERROR][RK0][tid #140304929761024]: 0 allocated 6553600 at 0x7f80c682e800








