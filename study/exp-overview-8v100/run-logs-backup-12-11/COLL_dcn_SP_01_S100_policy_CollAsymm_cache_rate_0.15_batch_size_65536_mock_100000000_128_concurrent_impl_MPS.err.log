2022-12-12 04:29:05.064888: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.071238: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.077484: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.081702: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.094224: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.102295: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.105958: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.117150: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.169033: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.171115: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.172104: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.173073: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.173939: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.175014: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.176125: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.177333: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.178941: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.179969: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.180130: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.181619: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.181645: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.183023: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.183223: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.184529: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.184804: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.186197: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.186407: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.187706: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.188088: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.189080: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.189723: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.191015: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.191284: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.192842: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.193838: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.194862: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.195824: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.196771: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.197784: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.198822: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.204277: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 04:29:05.209162: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.210133: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.210691: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.212125: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.212650: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.213346: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.214362: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.214643: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.215060: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.216107: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.217321: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.217576: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.217889: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.218923: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.220317: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.220563: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.220839: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.223739: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.224087: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.224354: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.225880: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.227543: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.228134: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.228234: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.229937: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.230133: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.231351: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.231990: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.233907: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.234188: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.234839: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.235398: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.236770: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.236906: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.237208: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.237917: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.238432: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.240017: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.240320: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.240780: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.241499: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.242834: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.243353: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.243789: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.244439: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.245359: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.246033: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.246635: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.247300: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.247899: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.248809: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.249324: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.249933: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.250185: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.252403: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.253822: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.253859: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.255772: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.255817: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.257210: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.276622: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.278588: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.285500: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.292395: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.293250: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.293287: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.293326: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.294697: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.295901: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.296029: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.296420: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.296463: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.297954: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.299544: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.300694: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.300715: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.300853: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.300897: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.301478: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.303518: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.304448: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.305612: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.305693: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.305819: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.305878: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.306435: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.307777: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.308822: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.311015: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.311075: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.311269: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.311312: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.311934: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.313176: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.314166: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.316368: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.316522: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.316826: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.316867: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.317512: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.318643: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.319892: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.321210: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.321255: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.321485: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.321528: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.322621: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.324592: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.324771: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.326655: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.326815: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.326930: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.327115: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.327655: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.329676: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.329724: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.331346: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.331435: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.331523: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.331741: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.332275: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.334663: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.334707: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.335952: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.336092: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.336176: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.336585: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.336959: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.339548: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.339557: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.340908: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.340958: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.341052: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.341195: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.341757: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.343868: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.343942: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.345647: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.345819: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.345950: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.346156: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.346709: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.349186: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.349192: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.350628: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.350773: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.350857: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.351097: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.351496: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.354035: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.354039: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.355305: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.355489: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.355577: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.356064: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.356439: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.358658: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.358703: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.360122: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.360351: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.360397: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.360653: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.361262: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.363761: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.364995: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.365864: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.365998: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.366482: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.366600: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.367390: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.368222: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 04:29:05.368660: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.369744: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.370002: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.370768: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.370856: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.371429: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.373668: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.373998: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.374307: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.374986: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.375035: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.376406: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.377362: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.377573: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.377985: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.378361: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.379049: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.379373: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.381022: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.382142: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.382193: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.382855: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.383465: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.383806: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.384196: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.386588: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.387839: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.387898: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.388487: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.389312: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.389578: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.389928: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.391592: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.392435: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.393039: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.394110: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.394461: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.394811: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.396307: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.397313: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.398150: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.399164: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.399432: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.399852: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.401563: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.402330: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.403649: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.405325: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.405730: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.406890: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 04:29:05.407199: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 04:29:05.407314: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.408313: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 04:29:05.408346: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.410052: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.411939: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 04:29:05.414555: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 04:29:05.415953: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 04:29:05.416136: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.416405: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.417897: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.418261: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.418463: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.421098: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.421196: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.421278: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.424031: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.424877: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.428492: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.461487: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.461617: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.461871: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.461890: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.466298: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.466585: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:05.466643: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:06.581087: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:06.581918: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:06.582459: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:06.582932: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 04:29:06.582988: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 04:29:06.600617: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:06.601255: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:06.601974: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:06.602558: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:06.603252: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:06.603724: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 04:29:06.647945: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:29:06.648166: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:29:06.691770: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-12 04:29:06.801689: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:06.802514: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:06.803056: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:06.803750: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 04:29:06.803806: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 04:29:06.821233: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:06.821861: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:06.823016: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:06.823890: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:06.824444: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:06.825097: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 04:29:06.870619: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:06.871257: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:06.871776: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:06.872245: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 04:29:06.872296: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 04:29:06.874034: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:06.875155: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:06.876199: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:06.876669: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 04:29:06.876722: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 04:29:06.889829: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:06.890498: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:06.891067: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:06.891092: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:06.891092: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:06.891260: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:06.893575: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:06.893587: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:06.893662: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:06.893921: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:06.894696: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:06.895685: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:06.895861: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:06.895941: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:06.896534: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:06.897796: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:06.898737: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 04:29:06.898772: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 04:29:06.898799: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 04:29:06.898823: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 04:29:06.898861: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 04:29:06.898907: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 04:29:06.899250: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 04:29:06.899970: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:06.900559: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:06.901080: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:06.901731: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 04:29:06.904817: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:29:06.904995: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:29:06.906853: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-12 04:29:06.916327: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:06.916678: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:06.917362: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:06.917433: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:06.918076: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:06.918974: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:06.919225: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:06.919474: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:06.920459: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:06.920581: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:06.920951: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:06.921806: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:06.922055: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:06.922293: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:06.923142: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 04:29:06.923401: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:06.923469: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 04:29:06.923900: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 04:29:06.944098: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:06.944724: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:06.945259: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:06.945503: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:29:06.945701: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:29:06.945722: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 04:29:06.945777: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 04:29:06.947561: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
2022-12-12 04:29:06.963231: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:06.963896: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:06.964399: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:06.964985: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:06.965512: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:29:06.965985: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 04:29:06.967251: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:29:06.967420: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:29:06.967964: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:29:06.967995: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:29:06.968101: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:29:06.968131: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:29:06.969216: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
2022-12-12 04:29:06.969760: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
2022-12-12 04:29:06.970183: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
2022-12-12 04:29:06.987150: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:29:06.987341: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:29:06.989153: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
2022-12-12 04:29:07.011296: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:29:07.011506: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:29:07.013210: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
[HCTR][04:29:08.273][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][04:29:08.274][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][04:29:08.274][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][04:29:08.274][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][04:29:08.274][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][04:29:08.274][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][04:29:08.297][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][04:29:08.297][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:01,  1.53s/it]warmup run: 1it [00:01,  1.51s/it]warmup run: 1it [00:01,  1.54s/it]warmup run: 1it [00:01,  1.53s/it]warmup run: 1it [00:01,  1.50s/it]warmup run: 99it [00:01, 84.47it/s]warmup run: 1it [00:01,  1.49s/it]warmup run: 1it [00:01,  1.49s/it]warmup run: 99it [00:01, 85.01it/s]warmup run: 100it [00:01, 84.50it/s]warmup run: 101it [00:01, 85.75it/s]warmup run: 1it [00:01,  1.48s/it]warmup run: 96it [00:01, 83.11it/s]warmup run: 198it [00:01, 183.10it/s]warmup run: 99it [00:01, 86.44it/s]warmup run: 100it [00:01, 87.19it/s]warmup run: 201it [00:01, 187.30it/s]warmup run: 201it [00:01, 184.28it/s]warmup run: 202it [00:01, 185.86it/s]warmup run: 99it [00:01, 86.96it/s]warmup run: 193it [00:01, 180.86it/s]warmup run: 300it [00:01, 295.61it/s]warmup run: 199it [00:01, 187.91it/s]warmup run: 200it [00:01, 188.39it/s]warmup run: 301it [00:01, 296.84it/s]warmup run: 301it [00:01, 293.01it/s]warmup run: 302it [00:01, 294.93it/s]warmup run: 198it [00:01, 187.91it/s]warmup run: 291it [00:01, 289.31it/s]warmup run: 401it [00:01, 410.36it/s]warmup run: 301it [00:01, 301.87it/s]warmup run: 299it [00:01, 297.75it/s]warmup run: 399it [00:01, 407.00it/s]warmup run: 401it [00:01, 406.17it/s]warmup run: 403it [00:01, 409.66it/s]warmup run: 298it [00:01, 299.38it/s]warmup run: 392it [00:01, 406.14it/s]warmup run: 403it [00:01, 419.21it/s]warmup run: 493it [00:02, 490.75it/s]warmup run: 397it [00:01, 409.00it/s]warmup run: 497it [00:02, 513.94it/s]warmup run: 497it [00:02, 508.43it/s]warmup run: 505it [00:02, 523.09it/s]warmup run: 396it [00:01, 410.91it/s]warmup run: 494it [00:02, 521.09it/s]warmup run: 504it [00:01, 530.49it/s]warmup run: 592it [00:02, 592.83it/s]warmup run: 497it [00:01, 520.55it/s]warmup run: 597it [00:02, 615.49it/s]warmup run: 592it [00:02, 600.14it/s]warmup run: 609it [00:02, 630.42it/s]warmup run: 491it [00:01, 512.60it/s]warmup run: 597it [00:02, 627.90it/s]warmup run: 607it [00:02, 636.65it/s]warmup run: 693it [00:02, 686.14it/s]warmup run: 598it [00:02, 624.00it/s]warmup run: 697it [00:02, 702.75it/s]warmup run: 686it [00:02, 676.00it/s]warmup run: 712it [00:02, 722.19it/s]warmup run: 591it [00:02, 615.42it/s]warmup run: 699it [00:02, 717.94it/s]warmup run: 710it [00:02, 728.28it/s]warmup run: 795it [00:02, 766.61it/s]warmup run: 699it [00:02, 713.30it/s]warmup run: 797it [00:02, 774.90it/s]warmup run: 780it [00:02, 739.47it/s]warmup run: 814it [00:02, 795.77it/s]warmup run: 687it [00:02, 695.47it/s]warmup run: 798it [00:02, 783.73it/s]warmup run: 813it [00:02, 801.76it/s]warmup run: 898it [00:02, 833.40it/s]warmup run: 802it [00:02, 790.35it/s]warmup run: 897it [00:02, 832.03it/s]warmup run: 874it [00:02, 791.28it/s]warmup run: 915it [00:02, 850.20it/s]warmup run: 788it [00:02, 773.08it/s]warmup run: 898it [00:02, 839.34it/s]warmup run: 916it [00:02, 859.56it/s]warmup run: 1002it [00:02, 886.82it/s]warmup run: 904it [00:02, 848.40it/s]warmup run: 997it [00:02, 875.80it/s]warmup run: 969it [00:02, 833.05it/s]warmup run: 1016it [00:02, 891.16it/s]warmup run: 889it [00:02, 834.34it/s]warmup run: 997it [00:02, 867.59it/s]warmup run: 1018it [00:02, 902.03it/s]warmup run: 1006it [00:02, 894.30it/s]warmup run: 1096it [00:02, 907.14it/s]warmup run: 1064it [00:02, 864.91it/s]warmup run: 1102it [00:02, 856.41it/s]warmup run: 1118it [00:02, 925.10it/s]warmup run: 990it [00:02, 881.55it/s]warmup run: 1095it [00:02, 896.50it/s]warmup run: 1119it [00:02, 931.03it/s]warmup run: 1107it [00:02, 924.43it/s]warmup run: 1196it [00:02, 933.13it/s]warmup run: 1163it [00:02, 898.13it/s]warmup run: 1201it [00:02, 891.25it/s]warmup run: 1221it [00:02, 954.23it/s]warmup run: 1092it [00:02, 920.26it/s]warmup run: 1198it [00:02, 933.04it/s]warmup run: 1220it [00:02, 950.43it/s]warmup run: 1208it [00:02, 932.39it/s]warmup run: 1295it [00:02, 947.25it/s]warmup run: 1263it [00:02, 927.08it/s]warmup run: 1300it [00:02, 917.71it/s]warmup run: 1195it [00:02, 949.94it/s]warmup run: 1323it [00:02, 969.32it/s]warmup run: 1300it [00:02, 956.31it/s]warmup run: 1321it [00:02, 965.61it/s]warmup run: 1364it [00:02, 950.60it/s]warmup run: 1394it [00:02, 957.35it/s]warmup run: 1399it [00:02, 938.02it/s]warmup run: 1307it [00:02, 933.40it/s]warmup run: 1297it [00:02, 969.80it/s]warmup run: 1425it [00:02, 981.56it/s]warmup run: 1404it [00:02, 978.03it/s]warmup run: 1423it [00:02, 979.61it/s]warmup run: 1465it [00:03, 967.95it/s]warmup run: 1493it [00:03, 963.04it/s]warmup run: 1499it [00:03, 954.55it/s]warmup run: 1399it [00:02, 984.30it/s]warmup run: 1405it [00:02, 939.53it/s]warmup run: 1527it [00:03, 920.69it/s]warmup run: 1507it [00:03, 992.87it/s]warmup run: 1526it [00:02, 993.67it/s]warmup run: 1568it [00:03, 985.72it/s]warmup run: 1593it [00:03, 971.72it/s]warmup run: 1598it [00:03, 962.70it/s]warmup run: 1502it [00:03, 942.70it/s]warmup run: 1501it [00:02, 985.19it/s]warmup run: 1610it [00:03, 1001.26it/s]warmup run: 1622it [00:03, 886.20it/s]warmup run: 1629it [00:03, 1002.55it/s]warmup run: 1671it [00:03, 996.98it/s]warmup run: 1693it [00:03, 977.44it/s]warmup run: 1698it [00:03, 971.70it/s]warmup run: 1599it [00:03, 946.55it/s]warmup run: 1602it [00:03, 989.46it/s]warmup run: 1712it [00:03, 1002.46it/s]warmup run: 1726it [00:03, 926.79it/s]warmup run: 1732it [00:03, 1009.31it/s]warmup run: 1772it [00:03, 996.76it/s]warmup run: 1793it [00:03, 983.73it/s]warmup run: 1799it [00:03, 982.39it/s]warmup run: 1695it [00:03, 948.96it/s]warmup run: 1705it [00:03, 1000.33it/s]warmup run: 1814it [00:03, 988.99it/s] warmup run: 1830it [00:03, 956.84it/s]warmup run: 1835it [00:03, 1015.36it/s]warmup run: 1873it [00:03, 998.51it/s]warmup run: 1894it [00:03, 991.29it/s]warmup run: 1899it [00:03, 986.86it/s]warmup run: 1791it [00:03, 950.65it/s]warmup run: 1808it [00:03, 1007.59it/s]warmup run: 1914it [00:03, 978.71it/s]warmup run: 1933it [00:03, 977.17it/s]warmup run: 1938it [00:03, 1015.40it/s]warmup run: 1975it [00:03, 1002.34it/s]warmup run: 1997it [00:03, 1000.69it/s]warmup run: 2000it [00:03, 992.41it/s]warmup run: 1911it [00:03, 1012.74it/s]warmup run: 1887it [00:03, 949.84it/s]warmup run: 2013it [00:03, 976.16it/s]warmup run: 2042it [00:03, 1008.93it/s]warmup run: 2045it [00:03, 1029.65it/s]warmup run: 2091it [00:03, 1047.70it/s]warmup run: 2115it [00:03, 1053.39it/s]warmup run: 2118it [00:03, 1048.07it/s]warmup run: 2016it [00:03, 1022.08it/s]warmup run: 1983it [00:03, 952.36it/s]warmup run: 2135it [00:03, 1047.30it/s]warmup run: 2163it [00:03, 1067.26it/s]warmup run: 2164it [00:03, 1074.72it/s]warmup run: 2212it [00:03, 1094.72it/s]warmup run: 2235it [00:03, 1094.78it/s]warmup run: 2238it [00:03, 1090.94it/s]warmup run: 2137it [00:03, 1076.91it/s]warmup run: 2097it [00:03, 1006.07it/s]warmup run: 2257it [00:03, 1097.99it/s]warmup run: 2284it [00:03, 1109.20it/s]warmup run: 2283it [00:03, 1106.89it/s]warmup run: 2333it [00:03, 1127.58it/s]warmup run: 2355it [00:03, 1124.14it/s]warmup run: 2358it [00:03, 1121.87it/s]warmup run: 2258it [00:03, 1115.65it/s]warmup run: 2216it [00:03, 1058.79it/s]warmup run: 2380it [00:03, 1134.86it/s]warmup run: 2405it [00:03, 1138.81it/s]warmup run: 2402it [00:03, 1129.04it/s]warmup run: 2454it [00:03, 1150.49it/s]warmup run: 2475it [00:03, 1145.10it/s]warmup run: 2478it [00:03, 1144.00it/s]warmup run: 2379it [00:03, 1142.78it/s]warmup run: 2335it [00:03, 1095.73it/s]warmup run: 2503it [00:03, 1160.27it/s]warmup run: 2526it [00:03, 1159.27it/s]warmup run: 2520it [00:03, 1144.13it/s]warmup run: 2575it [00:04, 1166.40it/s]warmup run: 2595it [00:04, 1158.95it/s]warmup run: 2595it [00:04, 1150.68it/s]warmup run: 2500it [00:03, 1161.11it/s]warmup run: 2453it [00:03, 1120.72it/s]warmup run: 2625it [00:04, 1178.00it/s]warmup run: 2647it [00:04, 1174.20it/s]warmup run: 2639it [00:04, 1156.02it/s]warmup run: 2696it [00:04, 1178.09it/s]warmup run: 2715it [00:04, 1168.44it/s]warmup run: 2711it [00:04, 1150.70it/s]warmup run: 2621it [00:03, 1173.33it/s]warmup run: 2571it [00:04, 1138.38it/s]warmup run: 2747it [00:04, 1189.85it/s]warmup run: 2766it [00:04, 1178.44it/s]warmup run: 2758it [00:04, 1159.52it/s]warmup run: 2816it [00:04, 1181.93it/s]warmup run: 2833it [00:04, 1170.79it/s]warmup run: 2827it [00:04, 1150.90it/s]warmup run: 2742it [00:04, 1182.97it/s]warmup run: 2690it [00:04, 1151.69it/s]warmup run: 2868it [00:04, 1193.83it/s]warmup run: 2888it [00:04, 1187.93it/s]warmup run: 2877it [00:04, 1165.81it/s]warmup run: 2937it [00:04, 1187.60it/s]warmup run: 2952it [00:04, 1175.69it/s]warmup run: 2943it [00:04, 1152.54it/s]warmup run: 2862it [00:04, 1186.15it/s]warmup run: 2807it [00:04, 1156.07it/s]warmup run: 3000it [00:04, 683.43it/s] warmup run: 3000it [00:04, 685.02it/s] warmup run: 2990it [00:04, 1198.66it/s]warmup run: 3000it [00:04, 679.10it/s] warmup run: 3000it [00:04, 676.02it/s] warmup run: 3000it [00:04, 689.61it/s] warmup run: 2995it [00:04, 1167.18it/s]warmup run: 3000it [00:04, 695.15it/s] warmup run: 2923it [00:04, 1151.71it/s]warmup run: 2981it [00:04, 1173.94it/s]warmup run: 3000it [00:04, 694.49it/s] warmup run: 3000it [00:04, 682.41it/s] 

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]


warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   5%|▌         | 160/3000 [00:00<00:01, 1596.96it/s]warmup should be done:   5%|▌         | 158/3000 [00:00<00:01, 1577.77it/s]warmup should be done:   5%|▌         | 159/3000 [00:00<00:01, 1587.04it/s]warmup should be done:   5%|▌         | 161/3000 [00:00<00:01, 1609.33it/s]warmup should be done:   5%|▌         | 156/3000 [00:00<00:01, 1553.29it/s]warmup should be done:   5%|▌         | 162/3000 [00:00<00:01, 1614.52it/s]warmup should be done:   5%|▌         | 162/3000 [00:00<00:01, 1611.50it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1641.54it/s]warmup should be done:  11%|█         | 323/3000 [00:00<00:01, 1615.24it/s]warmup should be done:  11%|█         | 321/3000 [00:00<00:01, 1603.77it/s]warmup should be done:  11%|█         | 323/3000 [00:00<00:01, 1612.31it/s]warmup should be done:  10%|█         | 312/3000 [00:00<00:01, 1555.25it/s]warmup should be done:  11%|█         | 328/3000 [00:00<00:01, 1639.45it/s]warmup should be done:  11%|█         | 330/3000 [00:00<00:01, 1645.65it/s]warmup should be done:  11%|█         | 325/3000 [00:00<00:01, 1618.26it/s]warmup should be done:  11%|█         | 316/3000 [00:00<00:01, 1566.32it/s]warmup should be done:  16%|█▋        | 494/3000 [00:00<00:01, 1645.04it/s]warmup should be done:  16%|█▌        | 485/3000 [00:00<00:01, 1610.63it/s]warmup should be done:  16%|█▌        | 487/3000 [00:00<00:01, 1617.48it/s]warmup should be done:  16%|█▌        | 470/3000 [00:00<00:01, 1562.11it/s]warmup should be done:  16%|█▌        | 485/3000 [00:00<00:01, 1609.38it/s]warmup should be done:  16%|█▋        | 495/3000 [00:00<00:01, 1639.35it/s]warmup should be done:  16%|█▌        | 474/3000 [00:00<00:01, 1569.21it/s]warmup should be done:  16%|█▌        | 482/3000 [00:00<00:01, 1539.20it/s]warmup should be done:  21%|██        | 632/3000 [00:00<00:01, 1583.83it/s]warmup should be done:  22%|██▏       | 651/3000 [00:00<00:01, 1624.27it/s]warmup should be done:  22%|██▏       | 659/3000 [00:00<00:01, 1642.43it/s]warmup should be done:  21%|██        | 636/3000 [00:00<00:01, 1588.82it/s]warmup should be done:  22%|██▏       | 660/3000 [00:00<00:01, 1641.90it/s]warmup should be done:  22%|██▏       | 647/3000 [00:00<00:01, 1607.90it/s]warmup should be done:  22%|██▏       | 646/3000 [00:00<00:01, 1598.25it/s]warmup should be done:  21%|██▏       | 640/3000 [00:00<00:01, 1554.71it/s]warmup should be done:  27%|██▋       | 815/3000 [00:00<00:01, 1629.26it/s]warmup should be done:  26%|██▋       | 794/3000 [00:00<00:01, 1594.90it/s]warmup should be done:  27%|██▋       | 797/3000 [00:00<00:01, 1594.61it/s]warmup should be done:  27%|██▋       | 824/3000 [00:00<00:01, 1641.80it/s]warmup should be done:  27%|██▋       | 808/3000 [00:00<00:01, 1606.45it/s]warmup should be done:  28%|██▊       | 826/3000 [00:00<00:01, 1645.43it/s]warmup should be done:  27%|██▋       | 806/3000 [00:00<00:01, 1593.41it/s]warmup should be done:  27%|██▋       | 802/3000 [00:00<00:01, 1575.69it/s]warmup should be done:  33%|███▎      | 979/3000 [00:00<00:01, 1629.89it/s]warmup should be done:  32%|███▏      | 956/3000 [00:00<00:01, 1600.84it/s]warmup should be done:  32%|███▏      | 959/3000 [00:00<00:01, 1600.66it/s]warmup should be done:  33%|███▎      | 992/3000 [00:00<00:01, 1647.66it/s]warmup should be done:  32%|███▏      | 969/3000 [00:00<00:01, 1602.50it/s]warmup should be done:  33%|███▎      | 989/3000 [00:00<00:01, 1629.58it/s]warmup should be done:  32%|███▏      | 966/3000 [00:00<00:01, 1588.14it/s]warmup should be done:  32%|███▏      | 964/3000 [00:00<00:01, 1590.17it/s]warmup should be done:  38%|███▊      | 1142/3000 [00:00<00:01, 1627.18it/s]warmup should be done:  39%|███▊      | 1157/3000 [00:00<00:01, 1646.23it/s]warmup should be done:  38%|███▊      | 1130/3000 [00:00<00:01, 1596.65it/s]warmup should be done:  38%|███▊      | 1152/3000 [00:00<00:01, 1628.76it/s]warmup should be done:  37%|███▋      | 1120/3000 [00:00<00:01, 1588.27it/s]warmup should be done:  37%|███▋      | 1117/3000 [00:00<00:01, 1583.31it/s]warmup should be done:  38%|███▊      | 1125/3000 [00:00<00:01, 1580.62it/s]warmup should be done:  38%|███▊      | 1125/3000 [00:00<00:01, 1595.46it/s]warmup should be done:  44%|████▍     | 1323/3000 [00:00<00:01, 1648.54it/s]warmup should be done:  44%|████▎     | 1305/3000 [00:00<00:01, 1618.46it/s]warmup should be done:  43%|████▎     | 1290/3000 [00:00<00:01, 1596.14it/s]warmup should be done:  44%|████▍     | 1317/3000 [00:00<00:01, 1632.32it/s]warmup should be done:  43%|████▎     | 1284/3000 [00:00<00:01, 1580.12it/s]warmup should be done:  43%|████▎     | 1279/3000 [00:00<00:01, 1575.92it/s]warmup should be done:  43%|████▎     | 1276/3000 [00:00<00:01, 1572.81it/s]warmup should be done:  43%|████▎     | 1287/3000 [00:00<00:01, 1602.69it/s]warmup should be done:  50%|████▉     | 1489/3000 [00:00<00:00, 1650.09it/s]warmup should be done:  49%|████▉     | 1469/3000 [00:00<00:00, 1623.46it/s]warmup should be done:  48%|████▊     | 1450/3000 [00:00<00:00, 1595.66it/s]warmup should be done:  49%|████▉     | 1482/3000 [00:00<00:00, 1635.00it/s]warmup should be done:  48%|████▊     | 1443/3000 [00:00<00:00, 1579.35it/s]warmup should be done:  48%|████▊     | 1436/3000 [00:00<00:00, 1579.44it/s]warmup should be done:  48%|████▊     | 1437/3000 [00:00<00:00, 1573.80it/s]warmup should be done:  48%|████▊     | 1449/3000 [00:00<00:00, 1606.14it/s]warmup should be done:  55%|█████▌    | 1655/3000 [00:01<00:00, 1651.19it/s]warmup should be done:  54%|█████▍    | 1633/3000 [00:01<00:00, 1626.92it/s]warmup should be done:  54%|█████▎    | 1610/3000 [00:01<00:00, 1594.78it/s]warmup should be done:  55%|█████▍    | 1646/3000 [00:01<00:00, 1635.22it/s]warmup should be done:  53%|█████▎    | 1598/3000 [00:01<00:00, 1590.54it/s]warmup should be done:  53%|█████▎    | 1601/3000 [00:01<00:00, 1573.49it/s]warmup should be done:  53%|█████▎    | 1595/3000 [00:01<00:00, 1564.10it/s]warmup should be done:  54%|█████▎    | 1610/3000 [00:01<00:00, 1605.19it/s]warmup should be done:  61%|██████    | 1821/3000 [00:01<00:00, 1650.86it/s]warmup should be done:  60%|█████▉    | 1797/3000 [00:01<00:00, 1629.31it/s]warmup should be done:  60%|██████    | 1810/3000 [00:01<00:00, 1625.44it/s]warmup should be done:  59%|█████▊    | 1762/3000 [00:01<00:00, 1602.52it/s]warmup should be done:  59%|█████▉    | 1770/3000 [00:01<00:00, 1573.18it/s]warmup should be done:  59%|█████▊    | 1759/3000 [00:01<00:00, 1568.98it/s]warmup should be done:  59%|█████▉    | 1771/3000 [00:01<00:00, 1605.68it/s]warmup should be done:  58%|█████▊    | 1752/3000 [00:01<00:00, 1557.92it/s]warmup should be done:  66%|██████▌   | 1987/3000 [00:01<00:00, 1650.38it/s]warmup should be done:  65%|██████▌   | 1960/3000 [00:01<00:00, 1625.04it/s]warmup should be done:  66%|██████▌   | 1974/3000 [00:01<00:00, 1629.66it/s]warmup should be done:  64%|██████▍   | 1926/3000 [00:01<00:00, 1611.95it/s]warmup should be done:  64%|██████▍   | 1916/3000 [00:01<00:00, 1566.85it/s]warmup should be done:  64%|██████▍   | 1933/3000 [00:01<00:00, 1609.95it/s]warmup should be done:  64%|██████▎   | 1908/3000 [00:01<00:00, 1555.50it/s]warmup should be done:  64%|██████▍   | 1928/3000 [00:01<00:00, 1560.94it/s]warmup should be done:  72%|███████▏  | 2153/3000 [00:01<00:00, 1649.91it/s]warmup should be done:  71%|███████   | 2124/3000 [00:01<00:00, 1627.70it/s]warmup should be done:  71%|███████▏  | 2139/3000 [00:01<00:00, 1633.26it/s]warmup should be done:  70%|██████▉   | 2091/3000 [00:01<00:00, 1621.79it/s]warmup should be done:  69%|██████▉   | 2073/3000 [00:01<00:00, 1566.75it/s]warmup should be done:  70%|██████▉   | 2095/3000 [00:01<00:00, 1612.80it/s]warmup should be done:  69%|██████▉   | 2064/3000 [00:01<00:00, 1554.46it/s]warmup should be done:  70%|██████▉   | 2085/3000 [00:01<00:00, 1559.89it/s]warmup should be done:  76%|███████▌  | 2287/3000 [00:01<00:00, 1628.13it/s]warmup should be done:  77%|███████▋  | 2319/3000 [00:01<00:00, 1650.76it/s]warmup should be done:  77%|███████▋  | 2304/3000 [00:01<00:00, 1636.38it/s]warmup should be done:  75%|███████▌  | 2257/3000 [00:01<00:00, 1613.88it/s]warmup should be done:  74%|███████▍  | 2233/3000 [00:01<00:00, 1575.34it/s]warmup should be done:  74%|███████▍  | 2224/3000 [00:01<00:00, 1566.97it/s]warmup should be done:  75%|███████▍  | 2245/3000 [00:01<00:00, 1569.73it/s]warmup should be done:  75%|███████▌  | 2254/3000 [00:01<00:00, 1479.89it/s]warmup should be done:  82%|████████▏ | 2450/3000 [00:01<00:00, 1625.80it/s]warmup should be done:  83%|████████▎ | 2485/3000 [00:01<00:00, 1648.46it/s]warmup should be done:  82%|████████▏ | 2468/3000 [00:01<00:00, 1632.58it/s]warmup should be done:  80%|███████▉  | 2393/3000 [00:01<00:00, 1580.70it/s]warmup should be done:  81%|████████  | 2419/3000 [00:01<00:00, 1611.92it/s]warmup should be done:  80%|███████▉  | 2385/3000 [00:01<00:00, 1577.47it/s]warmup should be done:  80%|████████  | 2404/3000 [00:01<00:00, 1574.24it/s]warmup should be done:  80%|████████  | 2405/3000 [00:01<00:00, 1413.49it/s]warmup should be done:  87%|████████▋ | 2614/3000 [00:01<00:00, 1627.01it/s]warmup should be done:  88%|████████▊ | 2651/3000 [00:01<00:00, 1650.13it/s]warmup should be done:  88%|████████▊ | 2632/3000 [00:01<00:00, 1633.07it/s]warmup should be done:  85%|████████▌ | 2554/3000 [00:01<00:00, 1586.59it/s]warmup should be done:  86%|████████▌ | 2581/3000 [00:01<00:00, 1612.40it/s]warmup should be done:  85%|████████▍ | 2549/3000 [00:01<00:00, 1595.82it/s]warmup should be done:  85%|████████▌ | 2564/3000 [00:01<00:00, 1579.91it/s]warmup should be done:  86%|████████▌ | 2565/3000 [00:01<00:00, 1462.72it/s]warmup should be done:  93%|█████████▎| 2778/3000 [00:01<00:00, 1628.28it/s]warmup should be done:  94%|█████████▍| 2817/3000 [00:01<00:00, 1651.00it/s]warmup should be done:  91%|█████████▏| 2743/3000 [00:01<00:00, 1612.25it/s]warmup should be done:  90%|█████████ | 2715/3000 [00:01<00:00, 1590.61it/s]warmup should be done:  93%|█████████▎| 2796/3000 [00:01<00:00, 1621.96it/s]warmup should be done:  90%|█████████ | 2713/3000 [00:01<00:00, 1607.72it/s]warmup should be done:  91%|█████████ | 2724/3000 [00:01<00:00, 1584.98it/s]warmup should be done:  91%|█████████ | 2720/3000 [00:01<00:00, 1485.34it/s]warmup should be done:  98%|█████████▊| 2944/3000 [00:01<00:00, 1636.76it/s]warmup should be done:  99%|█████████▉| 2984/3000 [00:01<00:00, 1653.91it/s]warmup should be done:  96%|█████████▌| 2876/3000 [00:01<00:00, 1595.79it/s]warmup should be done:  97%|█████████▋| 2907/3000 [00:01<00:00, 1617.80it/s]warmup should be done:  99%|█████████▉| 2963/3000 [00:01<00:00, 1633.35it/s]warmup should be done:  96%|█████████▌| 2878/3000 [00:01<00:00, 1618.89it/s]warmup should be done:  96%|█████████▌| 2886/3000 [00:01<00:00, 1593.11it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1649.07it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1632.83it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1627.42it/s]warmup should be done:  96%|█████████▌| 2880/3000 [00:01<00:00, 1515.87it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1602.69it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1589.17it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1587.20it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1583.73it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1547.20it/s]




warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1679.85it/s]warmup should be done:   6%|▌         | 169/3000 [00:00<00:01, 1687.78it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1676.99it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1645.08it/s]warmup should be done:   5%|▌         | 163/3000 [00:00<00:01, 1622.17it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1643.75it/s]warmup should be done:   6%|▌         | 169/3000 [00:00<00:01, 1682.40it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1650.49it/s]warmup should be done:  11%|█         | 337/3000 [00:00<00:01, 1684.76it/s]warmup should be done:  11%|█▏        | 339/3000 [00:00<00:01, 1689.49it/s]warmup should be done:  11%|█▏        | 338/3000 [00:00<00:01, 1684.93it/s]warmup should be done:  11%|█         | 327/3000 [00:00<00:01, 1629.82it/s]warmup should be done:  11%|█         | 332/3000 [00:00<00:01, 1652.64it/s]warmup should be done:  11%|█         | 330/3000 [00:00<00:01, 1642.28it/s]warmup should be done:  11%|█         | 330/3000 [00:00<00:01, 1640.60it/s]warmup should be done:  11%|█         | 336/3000 [00:00<00:01, 1643.56it/s]warmup should be done:  17%|█▋        | 506/3000 [00:00<00:01, 1686.92it/s]warmup should be done:  17%|█▋        | 508/3000 [00:00<00:01, 1689.19it/s]warmup should be done:  16%|█▋        | 491/3000 [00:00<00:01, 1632.22it/s]warmup should be done:  16%|█▋        | 495/3000 [00:00<00:01, 1644.82it/s]warmup should be done:  17%|█▋        | 508/3000 [00:00<00:01, 1688.22it/s]warmup should be done:  16%|█▋        | 495/3000 [00:00<00:01, 1643.70it/s]warmup should be done:  17%|█▋        | 498/3000 [00:00<00:01, 1651.45it/s]warmup should be done:  17%|█▋        | 501/3000 [00:00<00:01, 1646.07it/s]warmup should be done:  22%|██▎       | 675/3000 [00:00<00:01, 1685.35it/s]warmup should be done:  23%|██▎       | 677/3000 [00:00<00:01, 1688.51it/s]warmup should be done:  23%|██▎       | 677/3000 [00:00<00:01, 1683.54it/s]warmup should be done:  22%|██▏       | 655/3000 [00:00<00:01, 1630.96it/s]warmup should be done:  22%|██▏       | 661/3000 [00:00<00:01, 1646.90it/s]warmup should be done:  22%|██▏       | 661/3000 [00:00<00:01, 1646.50it/s]warmup should be done:  22%|██▏       | 667/3000 [00:00<00:01, 1663.40it/s]warmup should be done:  22%|██▏       | 666/3000 [00:00<00:01, 1619.18it/s]warmup should be done:  28%|██▊       | 844/3000 [00:00<00:01, 1685.47it/s]warmup should be done:  28%|██▊       | 847/3000 [00:00<00:01, 1690.50it/s]warmup should be done:  27%|██▋       | 819/3000 [00:00<00:01, 1632.46it/s]warmup should be done:  28%|██▊       | 827/3000 [00:00<00:01, 1648.38it/s]warmup should be done:  28%|██▊       | 834/3000 [00:00<00:01, 1664.13it/s]warmup should be done:  28%|██▊       | 827/3000 [00:00<00:01, 1648.28it/s]warmup should be done:  28%|██▊       | 846/3000 [00:00<00:01, 1680.53it/s]warmup should be done:  28%|██▊       | 829/3000 [00:00<00:01, 1620.44it/s]warmup should be done:  34%|███▍      | 1014/3000 [00:00<00:01, 1687.93it/s]warmup should be done:  34%|███▍      | 1017/3000 [00:00<00:01, 1690.61it/s]warmup should be done:  33%|███▎      | 984/3000 [00:00<00:01, 1635.91it/s]warmup should be done:  33%|███▎      | 992/3000 [00:00<00:01, 1646.62it/s]warmup should be done:  33%|███▎      | 992/3000 [00:00<00:01, 1646.40it/s]warmup should be done:  33%|███▎      | 1001/3000 [00:00<00:01, 1660.46it/s]warmup should be done:  34%|███▍      | 1015/3000 [00:00<00:01, 1676.69it/s]warmup should be done:  33%|███▎      | 997/3000 [00:00<00:01, 1638.91it/s]warmup should be done:  39%|███▉      | 1183/3000 [00:00<00:01, 1687.09it/s]warmup should be done:  38%|███▊      | 1148/3000 [00:00<00:01, 1636.11it/s]warmup should be done:  39%|███▊      | 1157/3000 [00:00<00:01, 1647.27it/s]warmup should be done:  40%|███▉      | 1187/3000 [00:00<00:01, 1688.26it/s]warmup should be done:  39%|███▊      | 1157/3000 [00:00<00:01, 1645.87it/s]warmup should be done:  39%|███▉      | 1168/3000 [00:00<00:01, 1655.87it/s]warmup should be done:  39%|███▉      | 1183/3000 [00:00<00:01, 1671.58it/s]warmup should be done:  39%|███▉      | 1165/3000 [00:00<00:01, 1649.11it/s]warmup should be done:  45%|████▌     | 1353/3000 [00:00<00:00, 1689.59it/s]warmup should be done:  44%|████▎     | 1312/3000 [00:00<00:01, 1633.06it/s]warmup should be done:  44%|████▍     | 1322/3000 [00:00<00:01, 1644.38it/s]warmup should be done:  44%|████▍     | 1322/3000 [00:00<00:01, 1643.62it/s]warmup should be done:  45%|████▌     | 1356/3000 [00:00<00:00, 1685.44it/s]warmup should be done:  45%|████▌     | 1351/3000 [00:00<00:00, 1673.01it/s]warmup should be done:  44%|████▍     | 1334/3000 [00:00<00:01, 1655.40it/s]warmup should be done:  44%|████▍     | 1330/3000 [00:00<00:01, 1622.89it/s]warmup should be done:  51%|█████     | 1522/3000 [00:00<00:00, 1689.46it/s]warmup should be done:  50%|████▉     | 1487/3000 [00:00<00:00, 1646.02it/s]warmup should be done:  49%|████▉     | 1476/3000 [00:00<00:00, 1633.25it/s]warmup should be done:  50%|████▉     | 1487/3000 [00:00<00:00, 1636.03it/s]warmup should be done:  50%|█████     | 1500/3000 [00:00<00:00, 1654.63it/s]warmup should be done:  51%|█████     | 1519/3000 [00:00<00:00, 1672.48it/s]warmup should be done:  51%|█████     | 1525/3000 [00:00<00:00, 1665.98it/s]warmup should be done:  50%|████▉     | 1498/3000 [00:00<00:00, 1640.37it/s]warmup should be done:  56%|█████▋    | 1692/3000 [00:01<00:00, 1689.94it/s]warmup should be done:  55%|█████▌    | 1652/3000 [00:01<00:00, 1647.20it/s]warmup should be done:  55%|█████▍    | 1640/3000 [00:01<00:00, 1635.17it/s]warmup should be done:  55%|█████▌    | 1652/3000 [00:01<00:00, 1638.53it/s]warmup should be done:  56%|█████▌    | 1666/3000 [00:01<00:00, 1653.06it/s]warmup should be done:  56%|█████▌    | 1687/3000 [00:01<00:00, 1669.76it/s]warmup should be done:  56%|█████▋    | 1692/3000 [00:01<00:00, 1660.04it/s]warmup should be done:  56%|█████▌    | 1667/3000 [00:01<00:00, 1654.93it/s]warmup should be done:  62%|██████▏   | 1862/3000 [00:01<00:00, 1691.81it/s]warmup should be done:  60%|██████    | 1804/3000 [00:01<00:00, 1635.40it/s]warmup should be done:  61%|██████    | 1817/3000 [00:01<00:00, 1640.51it/s]warmup should be done:  61%|██████    | 1817/3000 [00:01<00:00, 1634.52it/s]warmup should be done:  61%|██████    | 1835/3000 [00:01<00:00, 1662.31it/s]warmup should be done:  62%|██████▏   | 1854/3000 [00:01<00:00, 1667.66it/s]warmup should be done:  62%|██████▏   | 1859/3000 [00:01<00:00, 1658.75it/s]warmup should be done:  61%|██████    | 1837/3000 [00:01<00:00, 1667.71it/s]warmup should be done:  68%|██████▊   | 2032/3000 [00:01<00:00, 1691.76it/s]warmup should be done:  66%|██████▌   | 1968/3000 [00:01<00:00, 1634.75it/s]warmup should be done:  66%|██████▌   | 1982/3000 [00:01<00:00, 1636.85it/s]warmup should be done:  67%|██████▋   | 2004/3000 [00:01<00:00, 1667.76it/s]warmup should be done:  66%|██████▌   | 1982/3000 [00:01<00:00, 1638.03it/s]warmup should be done:  67%|██████▋   | 2021/3000 [00:01<00:00, 1662.74it/s]warmup should be done:  68%|██████▊   | 2025/3000 [00:01<00:00, 1656.88it/s]warmup should be done:  67%|██████▋   | 2005/3000 [00:01<00:00, 1669.40it/s]warmup should be done:  71%|███████   | 2133/3000 [00:01<00:00, 1638.57it/s]warmup should be done:  73%|███████▎  | 2202/3000 [00:01<00:00, 1686.18it/s]warmup should be done:  72%|███████▏  | 2147/3000 [00:01<00:00, 1639.35it/s]warmup should be done:  72%|███████▏  | 2146/3000 [00:01<00:00, 1637.99it/s]warmup should be done:  72%|███████▏  | 2172/3000 [00:01<00:00, 1669.66it/s]warmup should be done:  73%|███████▎  | 2188/3000 [00:01<00:00, 1654.99it/s]warmup should be done:  73%|███████▎  | 2191/3000 [00:01<00:00, 1639.16it/s]warmup should be done:  72%|███████▏  | 2172/3000 [00:01<00:00, 1664.30it/s]warmup should be done:  77%|███████▋  | 2299/3000 [00:01<00:00, 1643.18it/s]warmup should be done:  79%|███████▉  | 2371/3000 [00:01<00:00, 1679.09it/s]warmup should be done:  77%|███████▋  | 2312/3000 [00:01<00:00, 1643.98it/s]warmup should be done:  77%|███████▋  | 2313/3000 [00:01<00:00, 1643.10it/s]warmup should be done:  78%|███████▊  | 2339/3000 [00:01<00:00, 1665.22it/s]warmup should be done:  78%|███████▊  | 2354/3000 [00:01<00:00, 1652.18it/s]warmup should be done:  79%|███████▊  | 2357/3000 [00:01<00:00, 1643.88it/s]warmup should be done:  78%|███████▊  | 2339/3000 [00:01<00:00, 1663.52it/s]warmup should be done:  82%|████████▏ | 2464/3000 [00:01<00:00, 1644.16it/s]warmup should be done:  85%|████████▍ | 2539/3000 [00:01<00:00, 1675.91it/s]warmup should be done:  83%|████████▎ | 2479/3000 [00:01<00:00, 1645.26it/s]warmup should be done:  83%|████████▎ | 2477/3000 [00:01<00:00, 1636.31it/s]warmup should be done:  84%|████████▎ | 2506/3000 [00:01<00:00, 1659.84it/s]warmup should be done:  84%|████████▍ | 2521/3000 [00:01<00:00, 1656.00it/s]warmup should be done:  84%|████████▍ | 2523/3000 [00:01<00:00, 1648.54it/s]warmup should be done:  84%|████████▎ | 2506/3000 [00:01<00:00, 1664.65it/s]warmup should be done:  88%|████████▊ | 2631/3000 [00:01<00:00, 1649.38it/s]warmup should be done:  90%|█████████ | 2708/3000 [00:01<00:00, 1677.40it/s]warmup should be done:  88%|████████▊ | 2644/3000 [00:01<00:00, 1645.63it/s]warmup should be done:  88%|████████▊ | 2642/3000 [00:01<00:00, 1639.59it/s]warmup should be done:  90%|████████▉ | 2688/3000 [00:01<00:00, 1657.97it/s]warmup should be done:  89%|████████▉ | 2672/3000 [00:01<00:00, 1643.21it/s]warmup should be done:  90%|████████▉ | 2689/3000 [00:01<00:00, 1649.75it/s]warmup should be done:  89%|████████▉ | 2673/3000 [00:01<00:00, 1662.43it/s]warmup should be done:  93%|█████████▎| 2797/3000 [00:01<00:00, 1650.78it/s]warmup should be done:  96%|█████████▌| 2877/3000 [00:01<00:00, 1680.57it/s]warmup should be done:  94%|█████████▎| 2810/3000 [00:01<00:00, 1648.63it/s]warmup should be done:  94%|█████████▎| 2807/3000 [00:01<00:00, 1640.21it/s]warmup should be done:  95%|█████████▌| 2854/3000 [00:01<00:00, 1657.56it/s]warmup should be done:  95%|█████████▍| 2837/3000 [00:01<00:00, 1635.83it/s]warmup should be done:  95%|█████████▌| 2855/3000 [00:01<00:00, 1650.53it/s]warmup should be done:  95%|█████████▍| 2840/3000 [00:01<00:00, 1659.30it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1684.91it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1666.82it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1664.55it/s]warmup should be done:  99%|█████████▉| 2965/3000 [00:01<00:00, 1658.27it/s]warmup should be done:  99%|█████████▉| 2979/3000 [00:01<00:00, 1660.41it/s]warmup should be done:  99%|█████████▉| 2972/3000 [00:01<00:00, 1643.10it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1652.66it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1652.56it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1646.98it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1641.63it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1641.32it/s]
WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f9c5eb2ad30>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f9c5e7e40d0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f9c5e7f31c0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f9c5e7e41f0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f9c5eb27e80>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f9c5e7e4130>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f9c5e7f32b0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f9c5eb28730>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
2022-12-12 04:30:36.971311: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f979302d220 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 04:30:36.971371: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 04:30:36.980942: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 04:30:37.285866: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f9792f92e00 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 04:30:37.285929: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 04:30:37.295670: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 04:30:37.556262: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f978f029d10 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 04:30:37.556321: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 04:30:37.566628: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 04:30:37.799100: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f979e830a20 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 04:30:37.799176: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 04:30:37.807265: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 04:30:37.811267: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f979e82f8f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 04:30:37.811330: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 04:30:37.818964: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f9792830410 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 04:30:37.819014: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 04:30:37.819153: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 04:30:37.826471: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 04:30:37.831329: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f979b0312a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 04:30:37.831377: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 04:30:37.832600: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f979e833ef0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 04:30:37.832640: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 04:30:37.838675: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 04:30:37.839816: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 04:30:44.292170: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 04:30:44.364848: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 04:30:44.441991: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 04:30:44.582540: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 04:30:44.582809: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 04:30:44.614750: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 04:30:44.650189: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 04:30:44.673434: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][04:31:36.073][ERROR][RK0][tid #140290367137536]: replica 5 reaches 1000, calling init pre replica
[HCTR][04:31:36.073][ERROR][RK0][tid #140290367137536]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][04:31:36.082][ERROR][RK0][tid #140290367137536]: coll ps creation done
[HCTR][04:31:36.082][ERROR][RK0][tid #140290367137536]: replica 5 waits for coll ps creation barrier
[HCTR][04:31:36.186][ERROR][RK0][tid #140289561831168]: replica 7 reaches 1000, calling init pre replica
[HCTR][04:31:36.186][ERROR][RK0][tid #140289561831168]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][04:31:36.190][ERROR][RK0][tid #140289561831168]: coll ps creation done
[HCTR][04:31:36.190][ERROR][RK0][tid #140289561831168]: replica 7 waits for coll ps creation barrier
[HCTR][04:31:36.319][ERROR][RK0][tid #140289687656192]: replica 4 reaches 1000, calling init pre replica
[HCTR][04:31:36.319][ERROR][RK0][tid #140289687656192]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][04:31:36.324][ERROR][RK0][tid #140289687656192]: coll ps creation done
[HCTR][04:31:36.324][ERROR][RK0][tid #140289687656192]: replica 4 waits for coll ps creation barrier
[HCTR][04:31:36.409][ERROR][RK0][tid #140290090309376]: replica 6 reaches 1000, calling init pre replica
[HCTR][04:31:36.409][ERROR][RK0][tid #140290090309376]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][04:31:36.417][ERROR][RK0][tid #140290090309376]: coll ps creation done
[HCTR][04:31:36.417][ERROR][RK0][tid #140290090309376]: replica 6 waits for coll ps creation barrier
[HCTR][04:31:36.468][ERROR][RK0][tid #140289964484352]: replica 1 reaches 1000, calling init pre replica
[HCTR][04:31:36.468][ERROR][RK0][tid #140289964484352]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][04:31:36.477][ERROR][RK0][tid #140289964484352]: coll ps creation done
[HCTR][04:31:36.477][ERROR][RK0][tid #140289964484352]: replica 1 waits for coll ps creation barrier
[HCTR][04:31:36.518][ERROR][RK0][main]: replica 3 reaches 1000, calling init pre replica
[HCTR][04:31:36.518][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][04:31:36.519][ERROR][RK0][tid #140289561831168]: replica 0 reaches 1000, calling init pre replica
[HCTR][04:31:36.519][ERROR][RK0][tid #140289561831168]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][04:31:36.526][ERROR][RK0][main]: coll ps creation done
[HCTR][04:31:36.526][ERROR][RK0][main]: replica 3 waits for coll ps creation barrier
[HCTR][04:31:36.526][ERROR][RK0][tid #140289561831168]: coll ps creation done
[HCTR][04:31:36.526][ERROR][RK0][tid #140289561831168]: replica 0 waits for coll ps creation barrier
[HCTR][04:31:36.581][ERROR][RK0][tid #140289561831168]: replica 2 reaches 1000, calling init pre replica
[HCTR][04:31:36.581][ERROR][RK0][tid #140289561831168]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][04:31:36.586][ERROR][RK0][tid #140289561831168]: coll ps creation done
[HCTR][04:31:36.586][ERROR][RK0][tid #140289561831168]: replica 2 waits for coll ps creation barrier
[HCTR][04:31:36.586][ERROR][RK0][tid #140289561831168]: replica 0 preparing frequency
[HCTR][04:31:37.444][ERROR][RK0][tid #140289561831168]: replica 0 preparing frequency done
[HCTR][04:31:37.499][ERROR][RK0][tid #140289561831168]: replica 0 calling init per replica
[HCTR][04:31:37.499][ERROR][RK0][tid #140290367137536]: replica 5 calling init per replica
[HCTR][04:31:37.499][ERROR][RK0][tid #140289964484352]: replica 1 calling init per replica
[HCTR][04:31:37.499][ERROR][RK0][tid #140289687656192]: replica 4 calling init per replica
[HCTR][04:31:37.499][ERROR][RK0][tid #140290090309376]: replica 6 calling init per replica
[HCTR][04:31:37.499][ERROR][RK0][tid #140289561831168]: replica 2 calling init per replica
[HCTR][04:31:37.499][ERROR][RK0][main]: replica 3 calling init per replica
[HCTR][04:31:37.499][ERROR][RK0][tid #140289561831168]: replica 7 calling init per replica
[HCTR][04:31:37.499][ERROR][RK0][tid #140289561831168]: Calling build_v2
[HCTR][04:31:37.499][ERROR][RK0][tid #140290367137536]: Calling build_v2
[HCTR][04:31:37.499][ERROR][RK0][tid #140289964484352]: Calling build_v2
[HCTR][04:31:37.499][ERROR][RK0][tid #140289687656192]: Calling build_v2
[HCTR][04:31:37.499][ERROR][RK0][tid #140290090309376]: Calling build_v2
[HCTR][04:31:37.499][ERROR][RK0][main]: Calling build_v2
[HCTR][04:31:37.499][ERROR][RK0][tid #140289561831168]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][04:31:37.499][ERROR][RK0][tid #140289561831168]: Calling build_v2
[HCTR][04:31:37.499][ERROR][RK0][tid #140290367137536]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][04:31:37.499][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][04:31:37.499][ERROR][RK0][tid #140289964484352]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][04:31:37.499][ERROR][RK0][tid #140289687656192]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][04:31:37.499][ERROR][RK0][tid #140290090309376]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][04:31:37.499][ERROR][RK0][tid #140289561831168]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][04:31:37.499][ERROR][RK0][tid #140289561831168]: Calling build_v2
[HCTR][04:31:37.499][ERROR][RK0][tid #140289561831168]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[[[[2022-12-12 04:31:372022-12-12 04:31:37.2022-12-12 04:31:37.[[499214.499224[[: 2022-12-12 04:31:372022-12-12 04:31:374992202022-12-12 04:31:37: E.2022-12-12 04:31:37.2022-12-12 04:31:37: .E 499240.499252.E499252 /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc: 499258: 499269 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:E: E: /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.ccE:136 E E: 136] /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc 136/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc] using concurrent impl MPS:/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc] :using concurrent impl MPS
136:136:using concurrent impl MPS136
] 136] 136
] using concurrent impl MPS] using concurrent impl MPS] using concurrent impl MPS
using concurrent impl MPS
using concurrent impl MPS


[2022-12-12 04:31:37.503830: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-12 04:31:37.503870: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:196] assigning 8 to cpu
[2022-12-12 04:31:37.503916: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:212] build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8
[2022-12-12 04:31:37.503967: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:213] remote time is 8.68421
[2022-12-12 04:31:37.503996: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:214] cpu time is 97.0588
[2022-12-12 04:31:37.504067: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-12 04:31:37.504104: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:196] assigning 8 to cpu
[2022-12-12 04:31:37.504119: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie[
2022-12-12 04:31:37.504154: [E2022-12-12 04:31:37[ .2022-12-12 04:31:37/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc504170.:: 504170212E: ] [ Ebuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 82022-12-12 04:31:37/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 
.:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc504217196:[: [] 1782022-12-12 04:31:37E2022-12-12 04:31:37assigning 8 to cpu] . .
v100x8, slow pcie504269/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc504266[
: :: 2022-12-12 04:31:37E178[E. [] 2022-12-12 04:31:37 504316[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 04:31:37v100x8, slow pcie./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 2022-12-12 04:31:37:.
504343:E.213504353: 178[ 504359] : E] 2022-12-12 04:31:37/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: remote time is 8.68421E v100x8, slow pcie.:E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
504428178 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:[: ] [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:1962022-12-12 04:31:37Ev100x8, slow pcie2022-12-12 04:31:37:212] . 
.178] assigning 8 to cpu504513/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc504521] [build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8
: :: v100x8, slow pcie2022-12-12 04:31:37
E196E
. ]  [504594/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[[assigning 8 to cpu/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 04:31:37: :2022-12-12 04:31:372022-12-12 04:31:37
:.E214..196504646 ] 504652504652] : /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cccpu time is 97.0588: : [assigning 8 to cpuE:
EE2022-12-12 04:31:37
 196  ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc504744:assigning 8 to cpu::: 212[
213196E] 2022-12-12 04:31:37] ]  build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8.remote time is 8.68421assigning 8 to cpu[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
504827

2022-12-12 04:31:37:[: .[2122022-12-12 04:31:37E5048672022-12-12 04:31:37] . : [.build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8504904/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE2022-12-12 04:31:37504901
: : .: E212/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[504939E ] :2022-12-12 04:31:37:  /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccbuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8212.E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:
] 504997 :214build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[213] 
E:2022-12-12 04:31:37] cpu time is 97.0588 212.[remote time is 8.68421
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] 5050942022-12-12 04:31:37
:build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8: .213
[E505155] 2022-12-12 04:31:37 : [remote time is 8.68421./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE2022-12-12 04:31:37
505212: .: 213[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc505239E] 2022-12-12 04:31:37::  remote time is 8.68421.213E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
505279]  :: remote time is 8.68421[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc214E
2022-12-12 04:31:37:[]  .2132022-12-12 04:31:37cpu time is 97.0588/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc505342] .
:: remote time is 8.68421505381214E
: ]  E[cpu time is 97.0588/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 2022-12-12 04:31:37
:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.214:505447] 214: cpu time is 97.0588] E
cpu time is 97.0588 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:214] cpu time is 97.0588
[2022-12-12 04:32:56.665038: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-12 04:32:56.705178: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 381.47 MB
[2022-12-12 04:32:56.705259: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 381.47 MB
[2022-12-12 04:32:56.706224: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:74] mapping nid to rank...
[2022-12-12 04:32:56.784413: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:91] counting slots...
[2022-12-12 04:32:57.168184: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:105] Final num slot is 49
[2022-12-12 04:32:57.168274: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:109] counting blocks...
[2022-12-12 04:33:03.776905: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:118] Final num block is 1024
[2022-12-12 04:33:03.776997: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:123] counting freq and density...
[2022-12-12 04:33:05.496893: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:154] averaging freq and density...
[2022-12-12 04:33:05.496989: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:155] 1024
[2022-12-12 04:33:05.499721: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
[2022-12-12 04:33:05.499779: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:354] constructing optimal solver, device=8, stream=1
1024 blocks, 8 devices
[2022-12-12 04:33:05.813275: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:527] Add Var...
[2022-12-12 04:33:05.842144: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:545] Capacity...
[2022-12-12 04:33:05.843578: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:548] Connect CPU...
[2022-12-12 04:33:05.864593: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:550] Connect Access To Storage...
[2022-12-12 04:33:06.383169: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:554] Time...
[2022-12-12 04:36:27.356161: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:569] Coll Cache init block placement array
[2022-12-12 04:36:27.364749: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:645] Coll Cache init block placement array done
[2022-12-12 04:36:27.370578: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:647] Coll Cache model reset done
[2022-12-12 04:36:27.420175: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-12 04:36:27.420282: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-12 04:36:27.420313: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-12 04:36:27.420341: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-12 04:36:27.420897: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 04:36:27.420947: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:36:27.422131: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:36:27.422813: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:36:27.435634: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 7 solved
[2022-12-12 04:36:27.435710: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 7 initing device 7
[2022-12-12 04:36:27.436158: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 04:36:27.436210: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:36:27.437090: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:36:27.437614: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 1 solved
[2022-12-12 04:36:27.437673: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 1 initing device 1
[2022-12-12 04:36:27.437675: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 6 solved
[2022-12-12 04:36:27.437744: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 6 initing device 6
[2022-12-12 04:36:27.437785: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:36:27.438080: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-12 04:36:27:.1815438083] : Building Coll Cache with ... num gpu device is 8E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 2 solved
[[2022-12-12 04:36:272022-12-12 04:36:27..438146438148: : EE  [/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-12 04:36:27/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc[:.:2022-12-12 04:36:271980438161205.] : ] 438176eager alloc mem 381.47 MBEworker 0 thread 2 initing device 2[: 
 
2022-12-12 04:36:27E/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc. :438219/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu202: :] E1815 4 solved[] /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc
2022-12-12 04:36:27Building Coll Cache with ... num gpu device is 8:.
202[438305] 2022-12-12 04:36:27: 5 solved.E
[438354 2022-12-12 04:36:27[: /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc.2022-12-12 04:36:27E:438386. 202: 438400/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc] E3 solved: : 
E205/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu [] :/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc2022-12-12 04:36:27worker 0 thread 4 initing device 41980:.
] 205438478eager alloc mem 381.47 MB] : 
worker 0 thread 5 initing device 5E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 3 initing device 3
[2022-12-12 04:36:27.438660: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 04:36:27.438709: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:36:27.438911: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[[[2022-12-12 04:36:272022-12-12 04:36:272022-12-12 04:36:27...438952438954438960: : : EEE   /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:::181518151980] ] ] Building Coll Cache with ... num gpu device is 8Building Coll Cache with ... num gpu device is 8eager alloc mem 381.47 MB


[[2022-12-12 04:36:272022-12-12 04:36:27..439108439109: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 381.47 MBeager alloc mem 381.47 MB

[2022-12-12 04:36:27.442040: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:36:27.442343: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:36:27.442401: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:36:27.442460: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:36:27.442961: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:36:27.443016: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:36:27.445803: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:36:27.445976: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:36:27.446028: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:36:27.446080: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:36:27.446133: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:36:27.446638: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:36:27.501942: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-12 04:36:27.507203: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 04:36:27.507339: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 04:36:27.508154: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 57.60 MB
[2022-12-12 04:36:27.508785: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:36:27.509848: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:36:27.509895: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 57.18 MB
[2022-12-12 04:36:27.516309: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-12 04:36:27.521591: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 04:36:27.521723: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 04:36:27.522570: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 57.60 MB
[2022-12-12 04:36:27.523178: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:36:27.524247: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:36:27.524294: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 57.17 MB
[2022-12-12 04:36:27.524289: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[[[[2022-12-12 04:36:272022-12-12 04:36:272022-12-12 04:36:272022-12-12 04:36:27....524352524351524352524352: : : : EEEE    /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::1980198019801980] ] ] ] eager alloc mem 1024.00 Byteseager alloc mem 1024.00 Byteseager alloc mem 1024.00 Byteseager alloc mem 1024.00 Bytes



[2022-12-12 04:36:27.526531: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-12 04:36:27.530420: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[[2022-12-12 04:36:272022-12-12 04:36:27..530493530508: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 1024eager release cuda mem 400000000

[2022-12-12 04:36:27.530592: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000[
2022-12-12 04:36:27.530601: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 04:36:27.530685: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[:2022-12-12 04:36:27638.] 530685eager release cuda mem 400000000: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 04:36:27.530784: E[ 2022-12-12 04:36:27/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.:530780638: ] Eeager release cuda mem 400000000 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 04:36:27.530886: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 04:36:27.531272: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 04:36:27.531347: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 04:36:27.531507: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 57.60 MB
[2022-12-12 04:36:27.532068: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 57.60 MB
[2022-12-12 04:36:27.533212: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 57.60 MB
[2022-12-12 04:36:27.533860: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 57.60 MB
[2022-12-12 04:36:27.534426: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 57.60 MB
[2022-12-12 04:36:27.535055: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 57.60 MB
[2022-12-12 04:36:27.535804: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:36:27.535849: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:36:27.536242: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:36:27.536336: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:36:27.536395: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB[
2022-12-12 04:36:27.536422: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:36:27.536820: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:36:27.536867: [W2022-12-12 04:36:27 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc536872:: 43E]  WORKER[0] alloc host memory 56.99 MB/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
:638] eager release cuda mem 625663
[2022-12-12 04:36:27.536934: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 57.21 MB
[2022-12-12 04:36:27.537287: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:36:27.537336: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 57.06 MB
[2022-12-12 04:36:27.537361: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:36:27.537407: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 57.21 MB
[2022-12-12 04:36:27.537428: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] [eager release cuda mem 6256632022-12-12 04:36:27
.537450: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:36:27.537481: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43[] 2022-12-12 04:36:27WORKER[0] alloc host memory 57.17 MB.
537499: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 57.15 MB
[2022-12-12 04:36:27.549438: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 04:36:27.550074: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 04:36:27.550117: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 7.15 GB
[2022-12-12 04:36:27.563652: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 04:36:27.564286: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 04:36:27.564330: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 7.15 GB
[2022-12-12 04:36:27.573243: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 04:36:27.573848: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 04:36:27.573888: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 7.13 GB
[2022-12-12 04:36:27.574046: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 04:36:27.574648: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 04:36:27.574691: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 7.16 GB
[2022-12-12 04:36:27.574872: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 04:36:27.575473: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 04:36:27.575517: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[:2022-12-12 04:36:271980.] 575515eager alloc mem 7.16 GB: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[[2022-12-12 04:36:27[2022-12-12 04:36:27.2022-12-12 04:36:27.576128.576130: 576154: E: E E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:1980:1980] 638] eager alloc mem 25.25 KB] eager alloc mem 25.25 KB
eager release cuda mem 25855

[2022-12-12 04:36:27.576254: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 7.15 GB
[2022-12-12 04:36:27.[5767902022-12-12 04:36:27: .E576794 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE: 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] :eager release cuda mem 25855638
] eager release cuda mem 25855
[2022-12-12 04:36:27.[5768542022-12-12 04:36:27: .E576860 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: 1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] :eager alloc mem 7.14 GB1980
] eager alloc mem 7.15 GB
[[[[[[[[2022-12-12 04:36:302022-12-12 04:36:302022-12-12 04:36:302022-12-12 04:36:302022-12-12 04:36:302022-12-12 04:36:302022-12-12 04:36:302022-12-12 04:36:30........ 67976 67976 67976 67979 67978 67976 67976 67978: : : : : : : : EEEEEEEE        /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::::::19261926192619261926192619261926] ] ] ] ] ] ] ] Device 1 init p2p of link 7Device 2 init p2p of link 1Device 4 init p2p of link 5Device 7 init p2p of link 4Device 0 init p2p of link 3Device 5 init p2p of link 6Device 3 init p2p of link 2Device 6 init p2p of link 0







[[[2022-12-12 04:36:30[[2022-12-12 04:36:30[[2022-12-12 04:36:30[.2022-12-12 04:36:302022-12-12 04:36:30.2022-12-12 04:36:302022-12-12 04:36:30.2022-12-12 04:36:30 68520.. 68520.. 68520.:  68523 68523:  68523 68526:  68534E: : E: : E:  EE EE E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980::1980::1980:] 19801980] 19801980] 1980eager alloc mem 611.00 KB] ] eager alloc mem 611.00 KB] ] eager alloc mem 611.00 KB] 
eager alloc mem 611.00 KBeager alloc mem 611.00 KB
eager alloc mem 611.00 KBeager alloc mem 611.00 KB
eager alloc mem 611.00 KB




[2022-12-12 04:36:30. 69681: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[[2022-12-12 04:36:302022-12-12 04:36:30[..2022-12-12 04:36:30[ 69707[[ 69710.2022-12-12 04:36:30: 2022-12-12 04:36:30[2022-12-12 04:36:30:  69717.E.2022-12-12 04:36:30.E:  69736  69730. 69731 E: /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:  69748: /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc E:E: E:/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc 638 E 638:/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] 638:eager release cuda mem 625663:/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:eager release cuda mem 625663] 638
638:638
eager release cuda mem 625663] ] 638] 
eager release cuda mem 625663eager release cuda mem 625663] eager release cuda mem 625663

eager release cuda mem 625663

[2022-12-12 04:36:30. 85179: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 2
[2022-12-12 04:36:30. 85337: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:36:30. 86216: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:36:30. 86406: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 5
[2022-12-12 04:36:30. 86552: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:36:30. 87423: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:36:30. 87449: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 7
[2022-12-12 04:36:30. 87530: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 3
[2022-12-12 04:36:30. 87593: [E2022-12-12 04:36:30 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu 87616:: 1926E]  Device 7 init p2p of link 1/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
:1980] eager alloc mem 611.00 KB
[2022-12-12 04:36:30. 87687: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:36:30. 87766: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:36:30. 88159: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 0
[2022-12-12 04:36:30. 88302: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:36:30. 88385: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 4
[2022-12-12 04:36:30. 88471: E[ 2022-12-12 04:36:30/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.: 884941926: ] E[Device 0 init p2p of link 6 2022-12-12 04:36:30
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.[: 885322022-12-12 04:36:30638: .] E 88554eager release cuda mem 625663 : [
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE2022-12-12 04:36:30: .1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc 88597] :: eager alloc mem 611.00 KB638E
] [ eager release cuda mem 6256632022-12-12 04:36:30/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
.: 88665638: ] Eeager release cuda mem 625663 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:36:30. 89154: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:36:30. 89502: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:36:30. 89553: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:36:30. 97720: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 3
[2022-12-12 04:36:30. 97839: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:36:30. 98685: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:36:30.101472: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 4
[2022-12-12 04:36:30.101587: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:36:30.102460: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:36:30.104146: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 6
[2022-12-12 04:36:30.104263: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:36:30.104701: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 1
[2022-12-12 04:36:30.104820: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:36:30.104877: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 2
[2022-12-12 04:36:30.104995: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:36:30.105026: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 0
[2022-12-12 04:36:30.105123: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:36:30.105149: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[[2022-12-12 04:36:302022-12-12 04:36:30..105666105681: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::1926638] ] Device 3 init p2p of link 5eager release cuda mem 625663

[2022-12-12 04:36:30.105738: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 7
[2022-12-12 04:36:30.105805: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:36:30.105831: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] [eager release cuda mem 6256632022-12-12 04:36:30
.105852: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:36:30.105982: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:36:30.106628: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:36:30.106707: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:36:30.112019: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 0
[2022-12-12 04:36:30.112139: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:36:30.112988: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:36:30.113653: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 7
[2022-12-12 04:36:30.113763: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:36:30.114606: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:36:30.122936: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 6
[2022-12-12 04:36:30.123051: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:36:30.123785: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 1
[[2022-12-12 04:36:302022-12-12 04:36:30..123898123898: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::1980638] ] eager alloc mem 611.00 KBeager release cuda mem 625663

[2022-12-12 04:36:30.124815: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:36:30.125548: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 2
[2022-12-12 04:36:30.125672: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:36:30.125786: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 5
[2022-12-12 04:36:30.125907: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:36:30.125968: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 4
[2022-12-12 04:36:30.126083: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:36:30.126546: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:36:30.126791: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:36:30.126919: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:36:30.127311: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 3
[2022-12-12 04:36:30.127425: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:36:30.128236: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:36:30.129654: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 60400000
[2022-12-12 04:36:30.130744: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 60400000
[2022-12-12 04:36:30.132686: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 14939150 / 100000000 nodes ( 14.94 %~15.00 %) | remote 35885421 / 100000000 nodes ( 35.89 %) | cpu 49175429 / 100000000 nodes ( 49.18 %) | 7.13 GB | 2.69432 secs 
[2022-12-12 04:36:30.133964: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 14982498 / 100000000 nodes ( 14.98 %~15.00 %) | remote 35842073 / 100000000 nodes ( 35.84 %) | cpu 49175429 / 100000000 nodes ( 49.18 %) | 7.15 GB | 2.69583 secs 
[2022-12-12 04:36:30.138594: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 60400000
[2022-12-12 04:36:30.140798: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 14957216 / 100000000 nodes ( 14.96 %~15.00 %) | remote 35867355 / 100000000 nodes ( 35.87 %) | cpu 49175429 / 100000000 nodes ( 49.18 %) | 7.14 GB | 2.70185 secs 
[2022-12-12 04:36:30.141780: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 60400000
[2022-12-12 04:36:30.143191: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 14996819 / 100000000 nodes ( 15.00 %~15.00 %) | remote 35827752 / 100000000 nodes ( 35.83 %) | cpu 49175429 / 100000000 nodes ( 49.18 %) | 7.16 GB | 2.7041 secs 
[2022-12-12 04:36:30.143704: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 60400000
[2022-12-12 04:36:30.145432: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 60400000
[2022-12-12 04:36:30.146541: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 60400000
[2022-12-12 04:36:30.146859: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 14996798 / 100000000 nodes ( 15.00 %~15.00 %) | remote 35827773 / 100000000 nodes ( 35.83 %) | cpu 49175429 / 100000000 nodes ( 49.18 %) | 7.16 GB | 2.70776 secs 
[2022-12-12 04:36:30.146988: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 14986499 / 100000000 nodes ( 14.99 %~15.00 %) | remote 35838072 / 100000000 nodes ( 35.84 %) | cpu 49175429 / 100000000 nodes ( 49.18 %) | 7.15 GB | 2.70829 secs 
[2022-12-12 04:36:30.147092: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 60400000
[2022-12-12 04:36:30.148038: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 14989469 / 100000000 nodes ( 14.99 %~15.00 %) | remote 35835102 / 100000000 nodes ( 35.84 %) | cpu 49175429 / 100000000 nodes ( 49.18 %) | 7.15 GB | 2.7271 secs 
[2022-12-12 04:36:30.149065: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 14986791 / 100000000 nodes ( 14.99 %~15.00 %) | remote 35837780 / 100000000 nodes ( 35.84 %) | cpu 49175429 / 100000000 nodes ( 49.18 %) | 7.15 GB | 2.71287 secs 
[2022-12-12 04:36:30.150541: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 15.47 GB
[2022-12-12 04:36:31.438268: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 15.73 GB
[2022-12-12 04:36:31.438486: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 15.73 GB
[2022-12-12 04:36:31.438684: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 15.73 GB
[2022-12-12 04:36:32.711108: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 16.00 GB
[2022-12-12 04:36:32.711260: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 16.00 GB
[2022-12-12 04:36:32.711587: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 16.00 GB
[2022-12-12 04:36:33.908090: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 16.21 GB
[2022-12-12 04:36:33.908223: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 16.21 GB
[2022-12-12 04:36:33.909369: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 16.21 GB
[2022-12-12 04:36:34.969215: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 16.43 GB
[2022-12-12 04:36:34.969773: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 16.43 GB
[2022-12-12 04:36:34.970874: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 16.43 GB
[2022-12-12 04:36:36.165228: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 16.88 GB
[2022-12-12 04:36:36.166196: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 16.88 GB
[2022-12-12 04:36:36.167179: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 16.88 GB
[2022-12-12 04:36:37.592490: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 17.08 GB
[2022-12-12 04:36:37.597150: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 17.08 GB
[HCTR][04:36:38.714][ERROR][RK0][tid #140290090309376]: replica 6 calling init per replica done, doing barrier
[HCTR][04:36:38.714][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier
[HCTR][04:36:38.714][ERROR][RK0][tid #140289561831168]: replica 0 calling init per replica done, doing barrier
[HCTR][04:36:38.714][ERROR][RK0][tid #140289687656192]: replica 4 calling init per replica done, doing barrier
[HCTR][04:36:38.714][ERROR][RK0][tid #140290367137536]: replica 5 calling init per replica done, doing barrier
[HCTR][04:36:38.714][ERROR][RK0][tid #140289561831168]: replica 2 calling init per replica done, doing barrier
[HCTR][04:36:38.714][ERROR][RK0][tid #140289964484352]: replica 1 calling init per replica done, doing barrier
[HCTR][04:36:38.714][ERROR][RK0][tid #140289561831168]: replica 7 calling init per replica done, doing barrier
[HCTR][04:36:38.714][ERROR][RK0][tid #140290367137536]: replica 5 calling init per replica done, doing barrier done
[HCTR][04:36:38.714][ERROR][RK0][tid #140289964484352]: replica 1 calling init per replica done, doing barrier done
[HCTR][04:36:38.714][ERROR][RK0][tid #140289687656192]: replica 4 calling init per replica done, doing barrier done
[HCTR][04:36:38.714][ERROR][RK0][tid #140290090309376]: replica 6 calling init per replica done, doing barrier done
[HCTR][04:36:38.714][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier done
[HCTR][04:36:38.714][ERROR][RK0][tid #140289561831168]: replica 2 calling init per replica done, doing barrier done
[HCTR][04:36:38.714][ERROR][RK0][tid #140289561831168]: replica 0 calling init per replica done, doing barrier done
[HCTR][04:36:38.714][ERROR][RK0][tid #140289561831168]: replica 7 calling init per replica done, doing barrier done
[HCTR][04:36:38.714][ERROR][RK0][main]: init per replica done
[HCTR][04:36:38.714][ERROR][RK0][tid #140290367137536]: init per replica done
[HCTR][04:36:38.714][ERROR][RK0][tid #140289964484352]: init per replica done
[HCTR][04:36:38.714][ERROR][RK0][tid #140289687656192]: init per replica done
[HCTR][04:36:38.714][ERROR][RK0][tid #140289561831168]: init per replica done
[HCTR][04:36:38.714][ERROR][RK0][tid #140290090309376]: init per replica done
[HCTR][04:36:38.714][ERROR][RK0][tid #140289561831168]: init per replica done
[HCTR][04:36:38.718][ERROR][RK0][tid #140289561831168]: init per replica done
[HCTR][04:36:38.753][ERROR][RK0][main]: 2 allocated 3276800 at 0x7f7a98238400
[HCTR][04:36:38.753][ERROR][RK0][main]: 2 allocated 6553600 at 0x7f7a98558400
[HCTR][04:36:38.753][ERROR][RK0][main]: 2 allocated 3276800 at 0x7f7a98b98400
[HCTR][04:36:38.753][ERROR][RK0][main]: 2 allocated 6553600 at 0x7f7a98eb8400
[HCTR][04:36:38.753][ERROR][RK0][main]: 1 allocated 3276800 at 0x7f7a98238400
[HCTR][04:36:38.753][ERROR][RK0][tid #140290367137536]: 5 allocated 3276800 at 0x7f7a98238400
[HCTR][04:36:38.753][ERROR][RK0][main]: 1 allocated 6553600 at 0x7f7a98558400
[HCTR][04:36:38.753][ERROR][RK0][tid #140290367137536]: 5 allocated 6553600 at 0x7f7a98558400
[HCTR][04:36:38.753][ERROR][RK0][main]: 1 allocated 3276800 at 0x7f7a98b98400
[HCTR][04:36:38.753][ERROR][RK0][tid #140290367137536]: 5 allocated 3276800 at 0x7f7a98b98400
[HCTR][04:36:38.753][ERROR][RK0][tid #140290367137536]: 5 allocated 6553600 at 0x7f7a98eb8400
[HCTR][04:36:38.753][ERROR][RK0][main]: 1 allocated 6553600 at 0x7f7a98eb8400
[HCTR][04:36:38.753][ERROR][RK0][main]: 7 allocated 3276800 at 0x7f7a8c238400
[HCTR][04:36:38.753][ERROR][RK0][tid #140289553438464]: 3 allocated 3276800 at 0x7f7a98238400
[HCTR][04:36:38.753][ERROR][RK0][main]: 7 allocated 6553600 at 0x7f7a8c558400
[HCTR][04:36:38.753][ERROR][RK0][main]: 7 allocated 3276800 at 0x7f7a8cb98400
[HCTR][04:36:38.753][ERROR][RK0][tid #140289553438464]: 3 allocated 6553600 at 0x7f7a98558400
[HCTR][04:36:38.753][ERROR][RK0][main]: 7 allocated 6553600 at 0x7f7a8ceb8400
[HCTR][04:36:38.753][ERROR][RK0][tid #140289553438464]: 3 allocated 3276800 at 0x7f7a98b98400
[HCTR][04:36:38.753][ERROR][RK0][tid #140289687656192]: 4 allocated 3276800 at 0x7f7a94238400
[HCTR][04:36:38.753][ERROR][RK0][tid #140289553438464]: 3 allocated 6553600 at 0x7f7a98eb8400
[HCTR][04:36:38.754][ERROR][RK0][tid #140289687656192]: 4 allocated 6553600 at 0x7f7a94558400
[HCTR][04:36:38.754][ERROR][RK0][main]: 6 allocated 3276800 at 0x7f7978238400
[HCTR][04:36:38.754][ERROR][RK0][tid #140289687656192]: 4 allocated 3276800 at 0x7f7a94b98400
[HCTR][04:36:38.754][ERROR][RK0][main]: 6 allocated 6553600 at 0x7f7978558400
[HCTR][04:36:38.754][ERROR][RK0][tid #140289687656192]: 4 allocated 6553600 at 0x7f7a94eb8400
[HCTR][04:36:38.754][ERROR][RK0][main]: 6 allocated 3276800 at 0x7f7978b98400
[HCTR][04:36:38.754][ERROR][RK0][main]: 6 allocated 6553600 at 0x7f7978eb8400
[HCTR][04:36:38.756][ERROR][RK0][tid #140289561831168]: 0 allocated 3276800 at 0x7f7a4c320000
[HCTR][04:36:38.756][ERROR][RK0][tid #140289561831168]: 0 allocated 6553600 at 0x7f7a4c640000
[HCTR][04:36:38.756][ERROR][RK0][tid #140289561831168]: 0 allocated 3276800 at 0x7f7a4cc80000
[HCTR][04:36:38.756][ERROR][RK0][tid #140289561831168]: 0 allocated 6553600 at 0x7f7a4cfa0000
