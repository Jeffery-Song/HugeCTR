2022-12-12 02:05:49.999089: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.007537: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.012701: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.018807: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.022879: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.036529: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.051070: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.056525: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.105634: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.111196: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.114123: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.115104: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.116457: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.117981: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.118655: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.119262: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.120347: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.120782: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.121998: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.122267: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.123730: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.123833: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.125196: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.125289: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.126765: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.126823: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.128436: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.128536: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.129927: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.130306: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.131415: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.132642: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.134309: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.135344: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.136264: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.137197: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.138150: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.139070: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.140142: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.141151: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.145212: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.146339: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.146475: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 02:05:50.147410: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.148437: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.149488: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.150523: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.151636: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.152730: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.156229: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.156345: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.157931: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.158108: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.159516: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.159730: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.161315: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.163182: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.165172: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.167649: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.169493: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.170510: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.172651: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.172830: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.175819: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.176036: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.176375: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.176992: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.179049: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.179435: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.179896: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.180047: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.180329: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.182275: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.182886: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.183301: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.183351: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.183609: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.188787: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.190631: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.190800: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.191021: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.191083: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.193322: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.194137: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.194305: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.194563: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.194727: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.197006: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.197506: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.202871: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.218408: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.231912: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.232180: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.232411: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.235004: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.235389: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.236241: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.236288: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.237203: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.237296: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.240122: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.240735: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.240747: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.242287: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.242483: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.245007: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.245373: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.245646: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.246656: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.248079: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.248487: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.248817: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.250974: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.251317: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.251569: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.253756: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.254031: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.254545: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.256226: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.256423: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.256958: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.258386: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.258680: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.259475: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.260629: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.260929: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.261840: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.263253: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.263456: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.264480: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.265652: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.266264: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.266865: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.267970: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.268683: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.269065: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.270382: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.270964: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.271616: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.272569: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.273471: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.274022: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.274737: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.275757: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.276516: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.278119: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.278140: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.279373: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.280276: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.280370: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.281982: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.282540: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.282578: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.284468: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.284835: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.284846: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.287203: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.287573: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.287706: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.287855: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.289315: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 02:05:50.290044: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.290308: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.290325: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.290424: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.290506: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.293250: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.293796: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.293961: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.293990: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.294017: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.294074: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.297946: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.298006: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.298020: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.298061: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.298113: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.298132: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.299428: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.302141: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.302417: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.302547: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.302557: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.302566: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.304095: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.306203: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.306389: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 02:05:50.306411: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.306528: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.306688: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.307812: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.309898: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 02:05:50.310018: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.310224: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.310296: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.310374: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.313485: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.313764: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.313801: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.313975: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.316206: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.316814: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.317084: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.317138: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.317641: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.319378: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.320120: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.321023: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.321377: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.321425: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.322058: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.324108: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.324808: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.325604: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.325929: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.326002: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.326568: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.328776: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.330226: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.330722: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.330848: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.331332: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.335075: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.335465: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.335643: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.336051: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.367987: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.368435: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.368516: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.369307: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.372982: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.373413: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.373456: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.374131: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.377793: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.378149: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.378240: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.380152: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.382989: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.383372: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.384160: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.389256: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.393491: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.394607: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.395237: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.396048: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.398644: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.400366: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.401135: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.401496: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.432317: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.432782: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.433627: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.433780: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.436790: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.437275: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.438704: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.438857: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.451362: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.451896: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.456312: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.456430: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.460385: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.460913: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.461709: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.461888: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.464959: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.494912: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.496335: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.500247: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 02:05:50.510074: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.525488: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.527669: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.529729: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 02:05:50.530092: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.530568: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.536378: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 02:05:50.536392: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.539916: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.540193: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 02:05:50.544440: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.546388: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.548261: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.550262: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.552287: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.554523: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.558409: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:50.560188: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:51.470784: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:51.471425: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:51.471952: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:51.472420: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 02:05:51.472479: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 02:05:51.490054: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:51.490729: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:51.491260: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:51.492027: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:51.492970: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:51.493458: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 02:05:51.537896: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:05:51.538086: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:05:51.592512: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-12 02:05:51.716040: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:51.716901: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:51.717456: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:51.717925: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 02:05:51.717983: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 02:05:51.720039: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:51.720650: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:51.721309: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:51.721768: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 02:05:51.721833: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 02:05:51.728477: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:51.729079: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:51.729616: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:51.730082: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 02:05:51.730135: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 02:05:51.736368: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:51.736992: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:51.737513: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:51.738099: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:51.738630: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:51.739034: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:51.739108: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 02:05:51.740258: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:51.740981: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:51.741777: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:51.742310: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:51.742777: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 02:05:51.747881: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:51.748492: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:51.748997: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:51.749573: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:51.750083: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:51.750553: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 02:05:51.794352: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:05:51.794536: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:05:51.796715: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
2022-12-12 02:05:51.811882: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:51.811882: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:51.812978: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:51.813000: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:51.814250: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:51.814279: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:51.815192: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 02:05:51.815230: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 02:05:51.815251: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 02:05:51.815278: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 02:05:51.818589: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:51.819171: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:51.819698: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:51.820449: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 02:05:51.820512: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 02:05:51.825431: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:05:51.825613: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:05:51.827024: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:05:51.827200: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:05:51.827459: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-12 02:05:51.829138: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
2022-12-12 02:05:51.833035: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:51.833201: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:51.834104: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:51.834279: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:51.835069: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:51.835342: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:51.836427: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:51.836618: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:51.837334: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:51.837635: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:51.837649: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:51.838598: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 02:05:51.839021: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 02:05:51.839161: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:51.839682: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:51.840264: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:51.840309: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:51.841305: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:51.841416: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:51.842225: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 02:05:51.842407: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:51.842873: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 02:05:51.842917: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 02:05:51.860676: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:51.861339: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:51.861843: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:51.862427: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:51.862938: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 02:05:51.863427: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 02:05:51.882503: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:05:51.882686: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:05:51.884520: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
2022-12-12 02:05:51.884729: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:05:51.884900: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:05:51.886339: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:05:51.886472: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:05:51.886837: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
2022-12-12 02:05:51.888263: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
2022-12-12 02:05:51.908545: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:05:51.908741: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 02:05:51.910723: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
[HCTR][02:05:53.180][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][02:05:53.180][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][02:05:53.180][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][02:05:53.181][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][02:05:53.181][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][02:05:53.181][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][02:05:53.181][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][02:05:53.181][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:01,  1.55s/it]warmup run: 1it [00:01,  1.52s/it]warmup run: 95it [00:01, 79.71it/s]warmup run: 1it [00:01,  1.52s/it]warmup run: 101it [00:01, 86.57it/s]warmup run: 193it [00:01, 176.21it/s]warmup run: 1it [00:01,  1.52s/it]warmup run: 1it [00:01,  1.50s/it]warmup run: 1it [00:01,  1.49s/it]warmup run: 1it [00:01,  1.51s/it]warmup run: 93it [00:01, 79.49it/s]warmup run: 203it [00:01, 188.58it/s]warmup run: 292it [00:01, 284.28it/s]warmup run: 92it [00:01, 78.75it/s]warmup run: 101it [00:01, 87.70it/s]warmup run: 95it [00:01, 82.71it/s]warmup run: 1it [00:01,  1.48s/it]warmup run: 100it [00:01, 86.37it/s]warmup run: 189it [00:01, 175.50it/s]warmup run: 305it [00:01, 300.57it/s]warmup run: 391it [00:01, 396.13it/s]warmup run: 185it [00:01, 171.72it/s]warmup run: 201it [00:01, 188.54it/s]warmup run: 193it [00:01, 182.13it/s]warmup run: 95it [00:01, 83.35it/s]warmup run: 198it [00:01, 184.75it/s]warmup run: 285it [00:01, 281.17it/s]warmup run: 407it [00:01, 416.37it/s]warmup run: 490it [00:02, 505.36it/s]warmup run: 284it [00:01, 281.93it/s]warmup run: 301it [00:01, 299.20it/s]warmup run: 290it [00:01, 289.47it/s]warmup run: 192it [00:01, 182.26it/s]warmup run: 298it [00:01, 295.35it/s]warmup run: 381it [00:01, 390.43it/s]warmup run: 508it [00:02, 527.03it/s]warmup run: 590it [00:02, 608.09it/s]warmup run: 381it [00:01, 393.01it/s]warmup run: 402it [00:01, 414.67it/s]warmup run: 388it [00:01, 401.74it/s]warmup run: 290it [00:01, 291.60it/s]warmup run: 397it [00:01, 408.16it/s]warmup run: 477it [00:02, 496.89it/s]warmup run: 609it [00:02, 628.46it/s]warmup run: 686it [00:02, 688.44it/s]warmup run: 482it [00:02, 508.05it/s]warmup run: 499it [00:01, 518.63it/s]warmup run: 486it [00:01, 510.87it/s]warmup run: 386it [00:01, 401.20it/s]warmup run: 496it [00:02, 517.06it/s]warmup run: 576it [00:02, 600.53it/s]warmup run: 712it [00:02, 720.73it/s]warmup run: 789it [00:02, 771.53it/s]warmup run: 583it [00:02, 613.06it/s]warmup run: 596it [00:02, 613.47it/s]warmup run: 583it [00:02, 608.40it/s]warmup run: 481it [00:01, 505.11it/s]warmup run: 595it [00:02, 616.61it/s]warmup run: 676it [00:02, 692.80it/s]warmup run: 814it [00:02, 794.50it/s]warmup run: 891it [00:02, 834.60it/s]warmup run: 684it [00:02, 703.51it/s]warmup run: 693it [00:02, 695.68it/s]warmup run: 680it [00:02, 691.58it/s]warmup run: 577it [00:02, 601.17it/s]warmup run: 697it [00:02, 709.53it/s]warmup run: 776it [00:02, 768.14it/s]warmup run: 915it [00:02, 848.86it/s]warmup run: 993it [00:02, 883.33it/s]warmup run: 785it [00:02, 777.88it/s]warmup run: 789it [00:02, 760.31it/s]warmup run: 777it [00:02, 760.39it/s]warmup run: 676it [00:02, 690.44it/s]warmup run: 798it [00:02, 783.95it/s]warmup run: 876it [00:02, 828.67it/s]warmup run: 1015it [00:02, 889.73it/s]warmup run: 1094it [00:02, 917.86it/s]warmup run: 886it [00:02, 837.99it/s]warmup run: 887it [00:02, 816.48it/s]warmup run: 873it [00:02, 797.76it/s]warmup run: 774it [00:02, 762.16it/s]warmup run: 899it [00:02, 842.03it/s]warmup run: 975it [00:02, 871.55it/s]warmup run: 1116it [00:02, 921.42it/s]warmup run: 1197it [00:02, 948.11it/s]warmup run: 986it [00:02, 881.24it/s]warmup run: 987it [00:02, 865.47it/s]warmup run: 967it [00:02, 816.57it/s]warmup run: 873it [00:02, 821.56it/s]warmup run: 999it [00:02, 883.10it/s]warmup run: 1074it [00:02, 902.23it/s]warmup run: 1219it [00:02, 951.62it/s]warmup run: 1299it [00:02, 967.58it/s]warmup run: 1086it [00:02, 912.08it/s]warmup run: 1084it [00:02, 893.09it/s]warmup run: 1068it [00:02, 867.91it/s]warmup run: 971it [00:02, 864.31it/s]warmup run: 1100it [00:02, 916.67it/s]warmup run: 1173it [00:02, 925.75it/s]warmup run: 1321it [00:02, 970.47it/s]warmup run: 1402it [00:02, 983.36it/s]warmup run: 1186it [00:02, 936.76it/s]warmup run: 1181it [00:02, 914.18it/s]warmup run: 1169it [00:02, 906.06it/s]warmup run: 1071it [00:02, 901.36it/s]warmup run: 1203it [00:02, 948.19it/s]warmup run: 1272it [00:02, 938.35it/s]warmup run: 1424it [00:02, 986.11it/s]warmup run: 1505it [00:03, 994.77it/s]warmup run: 1286it [00:02, 952.74it/s]warmup run: 1278it [00:02, 928.08it/s]warmup run: 1268it [00:02, 929.20it/s]warmup run: 1172it [00:02, 929.84it/s]warmup run: 1304it [00:02, 958.88it/s]warmup run: 1371it [00:02, 951.14it/s]warmup run: 1527it [00:03, 998.77it/s]warmup run: 1607it [00:03, 1001.36it/s]warmup run: 1386it [00:02, 963.95it/s]warmup run: 1376it [00:02, 942.06it/s]warmup run: 1367it [00:02, 945.74it/s]warmup run: 1271it [00:02, 939.01it/s]warmup run: 1408it [00:02, 980.39it/s]warmup run: 1470it [00:03, 962.42it/s]warmup run: 1630it [00:03, 1007.73it/s]warmup run: 1709it [00:03, 1006.67it/s]warmup run: 1487it [00:03, 974.94it/s]warmup run: 1473it [00:03, 950.15it/s]warmup run: 1467it [00:03, 959.38it/s]warmup run: 1369it [00:02, 947.54it/s]warmup run: 1510it [00:03, 991.86it/s]warmup run: 1569it [00:03, 963.82it/s]warmup run: 1733it [00:03, 1013.06it/s]warmup run: 1812it [00:03, 1011.99it/s]warmup run: 1587it [00:03, 977.87it/s]warmup run: 1570it [00:03, 950.47it/s]warmup run: 1567it [00:03, 970.93it/s]warmup run: 1467it [00:02, 951.16it/s]warmup run: 1614it [00:03, 1004.00it/s]warmup run: 1667it [00:03, 964.36it/s]warmup run: 1836it [00:03, 1011.63it/s]warmup run: 1914it [00:03, 1011.29it/s]warmup run: 1687it [00:03, 979.20it/s]warmup run: 1667it [00:03, 946.66it/s]warmup run: 1669it [00:03, 984.12it/s]warmup run: 1564it [00:03, 952.10it/s]warmup run: 1718it [00:03, 1012.35it/s]warmup run: 1765it [00:03, 959.04it/s]warmup run: 1938it [00:03, 1013.57it/s]warmup run: 2016it [00:03, 998.48it/s] warmup run: 1786it [00:03, 981.63it/s]warmup run: 1763it [00:03, 946.82it/s]warmup run: 1771it [00:03, 993.89it/s]warmup run: 1661it [00:03, 955.69it/s]warmup run: 1821it [00:03, 1014.04it/s]warmup run: 1862it [00:03, 955.95it/s]warmup run: 2046it [00:03, 1032.35it/s]warmup run: 2128it [00:03, 1034.02it/s]warmup run: 1885it [00:03, 982.39it/s]warmup run: 1859it [00:03, 947.45it/s]warmup run: 1873it [00:03, 1001.55it/s]warmup run: 1758it [00:03, 958.76it/s]warmup run: 1924it [00:03, 1013.39it/s]warmup run: 1959it [00:03, 959.86it/s]warmup run: 2167it [00:03, 1084.68it/s]warmup run: 2241it [00:03, 1061.85it/s]warmup run: 1985it [00:03, 985.90it/s]warmup run: 1958it [00:03, 957.71it/s]warmup run: 1975it [00:03, 1006.12it/s]warmup run: 1855it [00:03, 961.82it/s]warmup run: 2031it [00:03, 1029.83it/s]warmup run: 2068it [00:03, 997.02it/s]warmup run: 2289it [00:03, 1122.97it/s]warmup run: 2354it [00:03, 1081.59it/s]warmup run: 2099it [00:03, 1030.13it/s]warmup run: 2066it [00:03, 993.27it/s]warmup run: 2091it [00:03, 1049.97it/s]warmup run: 1955it [00:03, 972.55it/s]warmup run: 2154it [00:03, 1088.56it/s]warmup run: 2186it [00:03, 1050.39it/s]warmup run: 2411it [00:03, 1150.04it/s]warmup run: 2467it [00:03, 1095.91it/s]warmup run: 2217it [00:03, 1072.72it/s]warmup run: 2182it [00:03, 1041.73it/s]warmup run: 2212it [00:03, 1096.20it/s]warmup run: 2066it [00:03, 1013.22it/s]warmup run: 2277it [00:03, 1129.63it/s]warmup run: 2304it [00:03, 1088.87it/s]warmup run: 2531it [00:03, 1164.59it/s]warmup run: 2580it [00:04, 1105.35it/s]warmup run: 2334it [00:03, 1101.42it/s]warmup run: 2298it [00:03, 1076.92it/s]warmup run: 2333it [00:03, 1128.11it/s]warmup run: 2187it [00:03, 1070.25it/s]warmup run: 2400it [00:03, 1158.61it/s]warmup run: 2422it [00:03, 1116.00it/s]warmup run: 2652it [00:04, 1176.38it/s]warmup run: 2694it [00:04, 1113.30it/s]warmup run: 2451it [00:03, 1121.27it/s]warmup run: 2415it [00:03, 1102.00it/s]warmup run: 2454it [00:03, 1150.64it/s]warmup run: 2308it [00:03, 1110.53it/s]warmup run: 2523it [00:03, 1179.31it/s]warmup run: 2541it [00:04, 1135.49it/s]warmup run: 2773it [00:04, 1185.70it/s]warmup run: 2806it [00:04, 1115.21it/s]warmup run: 2568it [00:04, 1135.73it/s]warmup run: 2531it [00:04, 1119.06it/s]warmup run: 2575it [00:04, 1166.80it/s]warmup run: 2429it [00:03, 1138.13it/s]warmup run: 2646it [00:04, 1193.28it/s]warmup run: 2660it [00:04, 1150.91it/s]warmup run: 2896it [00:04, 1197.17it/s]warmup run: 2919it [00:04, 1119.25it/s]warmup run: 2686it [00:04, 1147.08it/s]warmup run: 2648it [00:04, 1131.51it/s]warmup run: 2696it [00:04, 1179.22it/s]warmup run: 2550it [00:03, 1157.40it/s]warmup run: 2768it [00:04, 1199.26it/s]warmup run: 2779it [00:04, 1161.57it/s]warmup run: 3000it [00:04, 674.11it/s] warmup run: 3000it [00:04, 694.00it/s] warmup run: 2801it [00:04, 1147.74it/s]warmup run: 2765it [00:04, 1140.97it/s]warmup run: 2815it [00:04, 1182.14it/s]warmup run: 2670it [00:04, 1169.88it/s]warmup run: 2888it [00:04, 1197.58it/s]warmup run: 2899it [00:04, 1172.46it/s]warmup run: 2919it [00:04, 1154.68it/s]warmup run: 2886it [00:04, 1159.69it/s]warmup run: 2935it [00:04, 1187.23it/s]warmup run: 2790it [00:04, 1177.12it/s]warmup run: 3000it [00:04, 693.77it/s] warmup run: 3000it [00:04, 675.87it/s] warmup run: 3000it [00:04, 679.86it/s] warmup run: 3000it [00:04, 684.71it/s] warmup run: 3000it [00:04, 679.13it/s] warmup run: 2908it [00:04, 1177.41it/s]warmup run: 3000it [00:04, 684.81it/s] 
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]




warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1645.83it/s]warmup should be done:   5%|▌         | 161/3000 [00:00<00:01, 1608.08it/s]warmup should be done:   5%|▌         | 164/3000 [00:00<00:01, 1637.15it/s]warmup should be done:   5%|▌         | 163/3000 [00:00<00:01, 1626.83it/s]warmup should be done:   5%|▌         | 164/3000 [00:00<00:01, 1634.08it/s]warmup should be done:   5%|▌         | 162/3000 [00:00<00:01, 1610.28it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1661.49it/s]warmup should be done:   4%|▍         | 117/3000 [00:00<00:02, 1161.15it/s]warmup should be done:  11%|█         | 330/3000 [00:00<00:01, 1647.11it/s]warmup should be done:  11%|█         | 324/3000 [00:00<00:01, 1620.06it/s]warmup should be done:  11%|█         | 328/3000 [00:00<00:01, 1635.67it/s]warmup should be done:  11%|█         | 335/3000 [00:00<00:01, 1672.26it/s]warmup should be done:  11%|█         | 330/3000 [00:00<00:01, 1644.83it/s]warmup should be done:  11%|█         | 325/3000 [00:00<00:01, 1618.48it/s]warmup should be done:  11%|█         | 326/3000 [00:00<00:01, 1621.12it/s]warmup should be done:   8%|▊         | 234/3000 [00:00<00:02, 1158.41it/s]warmup should be done:  16%|█▋        | 489/3000 [00:00<00:01, 1632.87it/s]warmup should be done:  16%|█▋        | 495/3000 [00:00<00:01, 1640.26it/s]warmup should be done:  16%|█▋        | 495/3000 [00:00<00:01, 1641.99it/s]warmup should be done:  16%|█▌        | 487/3000 [00:00<00:01, 1614.51it/s]warmup should be done:  16%|█▋        | 492/3000 [00:00<00:01, 1628.39it/s]warmup should be done:  17%|█▋        | 503/3000 [00:00<00:01, 1666.78it/s]warmup should be done:  13%|█▎        | 399/3000 [00:00<00:01, 1381.60it/s]warmup should be done:  16%|█▋        | 489/3000 [00:00<00:01, 1616.64it/s]warmup should be done:  22%|██▏       | 653/3000 [00:00<00:01, 1630.55it/s]warmup should be done:  22%|██▏       | 656/3000 [00:00<00:01, 1631.63it/s]warmup should be done:  22%|██▏       | 652/3000 [00:00<00:01, 1621.22it/s]warmup should be done:  22%|██▏       | 672/3000 [00:00<00:01, 1672.38it/s]warmup should be done:  22%|██▏       | 649/3000 [00:00<00:01, 1610.59it/s]warmup should be done:  22%|██▏       | 660/3000 [00:00<00:01, 1635.96it/s]warmup should be done:  19%|█▊        | 561/3000 [00:00<00:01, 1472.66it/s]warmup should be done:  22%|██▏       | 660/3000 [00:00<00:01, 1634.80it/s]warmup should be done:  27%|██▋       | 820/3000 [00:00<00:01, 1633.97it/s]warmup should be done:  27%|██▋       | 815/3000 [00:00<00:01, 1624.18it/s]warmup should be done:  28%|██▊       | 840/3000 [00:00<00:01, 1673.53it/s]warmup should be done:  27%|██▋       | 817/3000 [00:00<00:01, 1623.24it/s]warmup should be done:  24%|██▍       | 725/3000 [00:00<00:01, 1530.25it/s]warmup should be done:  27%|██▋       | 824/3000 [00:00<00:01, 1633.04it/s]warmup should be done:  27%|██▋       | 811/3000 [00:00<00:01, 1608.20it/s]warmup should be done:  27%|██▋       | 824/3000 [00:00<00:01, 1628.66it/s]warmup should be done:  33%|███▎      | 985/3000 [00:00<00:01, 1637.72it/s]warmup should be done:  33%|███▎      | 978/3000 [00:00<00:01, 1624.60it/s]warmup should be done:  34%|███▎      | 1008/3000 [00:00<00:01, 1672.20it/s]warmup should be done:  30%|██▉       | 890/3000 [00:00<00:01, 1567.83it/s]warmup should be done:  33%|███▎      | 988/3000 [00:00<00:01, 1628.36it/s]warmup should be done:  32%|███▏      | 972/3000 [00:00<00:01, 1602.97it/s]warmup should be done:  33%|███▎      | 980/3000 [00:00<00:01, 1616.41it/s]warmup should be done:  33%|███▎      | 987/3000 [00:00<00:01, 1622.32it/s]warmup should be done:  38%|███▊      | 1149/3000 [00:00<00:01, 1637.70it/s]warmup should be done:  38%|███▊      | 1141/3000 [00:00<00:01, 1621.41it/s]warmup should be done:  35%|███▌      | 1057/3000 [00:00<00:01, 1598.80it/s]warmup should be done:  39%|███▉      | 1176/3000 [00:00<00:01, 1669.03it/s]warmup should be done:  38%|███▊      | 1151/3000 [00:00<00:01, 1624.07it/s]warmup should be done:  38%|███▊      | 1133/3000 [00:00<00:01, 1598.72it/s]warmup should be done:  38%|███▊      | 1142/3000 [00:00<00:01, 1609.71it/s]warmup should be done:  38%|███▊      | 1150/3000 [00:00<00:01, 1613.96it/s]warmup should be done:  44%|████▍     | 1313/3000 [00:00<00:01, 1638.09it/s]warmup should be done:  41%|████      | 1223/3000 [00:00<00:01, 1615.78it/s]warmup should be done:  45%|████▍     | 1344/3000 [00:00<00:00, 1670.33it/s]warmup should be done:  43%|████▎     | 1304/3000 [00:00<00:01, 1616.09it/s]warmup should be done:  44%|████▍     | 1314/3000 [00:00<00:01, 1623.01it/s]warmup should be done:  43%|████▎     | 1293/3000 [00:00<00:01, 1597.90it/s]warmup should be done:  44%|████▎     | 1306/3000 [00:00<00:01, 1617.34it/s]warmup should be done:  44%|████▎     | 1312/3000 [00:00<00:01, 1610.67it/s]warmup should be done:  49%|████▉     | 1477/3000 [00:00<00:00, 1633.46it/s]warmup should be done:  46%|████▋     | 1390/3000 [00:00<00:00, 1629.89it/s]warmup should be done:  49%|████▉     | 1467/3000 [00:00<00:00, 1617.55it/s]warmup should be done:  50%|█████     | 1512/3000 [00:00<00:00, 1668.36it/s]warmup should be done:  49%|████▉     | 1478/3000 [00:00<00:00, 1626.74it/s]warmup should be done:  48%|████▊     | 1453/3000 [00:00<00:00, 1596.72it/s]warmup should be done:  49%|████▉     | 1471/3000 [00:00<00:00, 1626.00it/s]warmup should be done:  49%|████▉     | 1474/3000 [00:00<00:00, 1608.34it/s]warmup should be done:  55%|█████▍    | 1641/3000 [00:01<00:00, 1634.12it/s]warmup should be done:  52%|█████▏    | 1557/3000 [00:01<00:00, 1639.85it/s]warmup should be done:  54%|█████▍    | 1630/3000 [00:01<00:00, 1619.32it/s]warmup should be done:  56%|█████▌    | 1680/3000 [00:01<00:00, 1669.05it/s]warmup should be done:  55%|█████▍    | 1643/3000 [00:01<00:00, 1633.03it/s]warmup should be done:  54%|█████▍    | 1613/3000 [00:01<00:00, 1596.61it/s]warmup should be done:  55%|█████▍    | 1638/3000 [00:01<00:00, 1638.85it/s]warmup should be done:  55%|█████▍    | 1635/3000 [00:01<00:00, 1606.79it/s]warmup should be done:  60%|██████    | 1806/3000 [00:01<00:00, 1637.53it/s]warmup should be done:  57%|█████▋    | 1723/3000 [00:01<00:00, 1645.60it/s]warmup should be done:  62%|██████▏   | 1847/3000 [00:01<00:00, 1668.91it/s]warmup should be done:  60%|█████▉    | 1793/3000 [00:01<00:00, 1620.43it/s]warmup should be done:  60%|██████    | 1809/3000 [00:01<00:00, 1640.20it/s]warmup should be done:  59%|█████▉    | 1773/3000 [00:01<00:00, 1596.10it/s]warmup should be done:  60%|██████    | 1803/3000 [00:01<00:00, 1641.39it/s]warmup should be done:  60%|█████▉    | 1796/3000 [00:01<00:00, 1605.76it/s]warmup should be done:  66%|██████▌   | 1971/3000 [00:01<00:00, 1639.12it/s]warmup should be done:  63%|██████▎   | 1889/3000 [00:01<00:00, 1648.84it/s]warmup should be done:  67%|██████▋   | 2014/3000 [00:01<00:00, 1667.76it/s]warmup should be done:  66%|██████▌   | 1975/3000 [00:01<00:00, 1644.33it/s]warmup should be done:  66%|██████▌   | 1968/3000 [00:01<00:00, 1643.01it/s]warmup should be done:  64%|██████▍   | 1933/3000 [00:01<00:00, 1595.60it/s]warmup should be done:  65%|██████▌   | 1956/3000 [00:01<00:00, 1615.00it/s]warmup should be done:  65%|██████▌   | 1957/3000 [00:01<00:00, 1604.35it/s]warmup should be done:  71%|███████   | 2136/3000 [00:01<00:00, 1640.35it/s]warmup should be done:  68%|██████▊   | 2055/3000 [00:01<00:00, 1652.01it/s]warmup should be done:  70%|██████▉   | 2093/3000 [00:01<00:00, 1596.45it/s]warmup should be done:  71%|███████▏  | 2141/3000 [00:01<00:00, 1647.02it/s]warmup should be done:  71%|███████   | 2133/3000 [00:01<00:00, 1642.68it/s]warmup should be done:  71%|███████   | 2118/3000 [00:01<00:00, 1611.83it/s]warmup should be done:  71%|███████   | 2118/3000 [00:01<00:00, 1604.32it/s]warmup should be done:  73%|███████▎  | 2181/3000 [00:01<00:00, 1624.87it/s]warmup should be done:  74%|███████▍  | 2222/3000 [00:01<00:00, 1656.83it/s]warmup should be done:  77%|███████▋  | 2301/3000 [00:01<00:00, 1637.04it/s]warmup should be done:  77%|███████▋  | 2306/3000 [00:01<00:00, 1647.58it/s]warmup should be done:  75%|███████▌  | 2253/3000 [00:01<00:00, 1596.44it/s]warmup should be done:  77%|███████▋  | 2298/3000 [00:01<00:00, 1643.58it/s]warmup should be done:  76%|███████▌  | 2281/3000 [00:01<00:00, 1614.26it/s]warmup should be done:  76%|███████▌  | 2279/3000 [00:01<00:00, 1604.87it/s]warmup should be done:  78%|███████▊  | 2348/3000 [00:01<00:00, 1636.53it/s]warmup should be done:  80%|███████▉  | 2388/3000 [00:01<00:00, 1657.72it/s]warmup should be done:  82%|████████▏ | 2465/3000 [00:01<00:00, 1630.42it/s]warmup should be done:  82%|████████▏ | 2471/3000 [00:01<00:00, 1645.97it/s]warmup should be done:  80%|████████  | 2413/3000 [00:01<00:00, 1594.73it/s]warmup should be done:  82%|████████▏ | 2463/3000 [00:01<00:00, 1640.86it/s]warmup should be done:  81%|████████▏ | 2443/3000 [00:01<00:00, 1613.97it/s]warmup should be done:  81%|████████▏ | 2440/3000 [00:01<00:00, 1603.35it/s]warmup should be done:  84%|████████▍ | 2516/3000 [00:01<00:00, 1646.82it/s]warmup should be done:  85%|████████▌ | 2555/3000 [00:01<00:00, 1660.32it/s]warmup should be done:  88%|████████▊ | 2636/3000 [00:01<00:00, 1646.28it/s]warmup should be done:  86%|████████▌ | 2574/3000 [00:01<00:00, 1598.08it/s]warmup should be done:  88%|████████▊ | 2629/3000 [00:01<00:00, 1629.44it/s]warmup should be done:  88%|████████▊ | 2628/3000 [00:01<00:00, 1640.39it/s]warmup should be done:  87%|████████▋ | 2605/3000 [00:01<00:00, 1615.14it/s]warmup should be done:  87%|████████▋ | 2601/3000 [00:01<00:00, 1603.34it/s]warmup should be done:  89%|████████▉ | 2684/3000 [00:01<00:00, 1653.79it/s]warmup should be done:  91%|█████████ | 2722/3000 [00:01<00:00, 1661.46it/s]warmup should be done:  93%|█████████▎| 2802/3000 [00:01<00:00, 1647.47it/s]warmup should be done:  91%|█████████ | 2737/3000 [00:01<00:00, 1604.92it/s]warmup should be done:  93%|█████████▎| 2794/3000 [00:01<00:00, 1633.29it/s]warmup should be done:  93%|█████████▎| 2793/3000 [00:01<00:00, 1639.05it/s]warmup should be done:  92%|█████████▏| 2767/3000 [00:01<00:00, 1615.62it/s]warmup should be done:  92%|█████████▏| 2762/3000 [00:01<00:00, 1603.39it/s]warmup should be done:  95%|█████████▌| 2852/3000 [00:01<00:00, 1661.00it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1661.82it/s]warmup should be done:  96%|█████████▋| 2890/3000 [00:01<00:00, 1666.17it/s]warmup should be done:  99%|█████████▉| 2969/3000 [00:01<00:00, 1652.39it/s]warmup should be done:  97%|█████████▋| 2902/3000 [00:01<00:00, 1617.56it/s]warmup should be done:  99%|█████████▊| 2962/3000 [00:01<00:00, 1644.68it/s]warmup should be done:  99%|█████████▊| 2959/3000 [00:01<00:00, 1643.72it/s]warmup should be done:  98%|█████████▊| 2929/3000 [00:01<00:00, 1613.73it/s]warmup should be done:  98%|█████████▊| 2925/3000 [00:01<00:00, 1610.38it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1640.66it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1636.85it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1633.74it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1617.74it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1612.17it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1604.85it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1601.98it/s]







warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1677.49it/s]warmup should be done:   6%|▌         | 169/3000 [00:00<00:01, 1685.79it/s]warmup should be done:   6%|▌         | 169/3000 [00:00<00:01, 1684.90it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1654.78it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1651.53it/s]warmup should be done:   6%|▌         | 171/3000 [00:00<00:01, 1699.97it/s]warmup should be done:   6%|▌         | 173/3000 [00:00<00:01, 1719.79it/s]warmup should be done:   6%|▌         | 169/3000 [00:00<00:01, 1680.71it/s]warmup should be done:  11%|█▏        | 339/3000 [00:00<00:01, 1692.33it/s]warmup should be done:  11%|█▏        | 341/3000 [00:00<00:01, 1698.51it/s]warmup should be done:  11%|█         | 332/3000 [00:00<00:01, 1653.31it/s]warmup should be done:  12%|█▏        | 345/3000 [00:00<00:01, 1717.19it/s]warmup should be done:  11%|█▏        | 338/3000 [00:00<00:01, 1680.17it/s]warmup should be done:  11%|█         | 332/3000 [00:00<00:01, 1649.25it/s]warmup should be done:  11%|█▏        | 338/3000 [00:00<00:01, 1677.77it/s]warmup should be done:  11%|█▏        | 338/3000 [00:00<00:01, 1672.73it/s]warmup should be done:  17%|█▋        | 511/3000 [00:00<00:01, 1698.43it/s]warmup should be done:  17%|█▋        | 511/3000 [00:00<00:01, 1701.72it/s]warmup should be done:  17%|█▋        | 517/3000 [00:00<00:01, 1715.50it/s]warmup should be done:  17%|█▋        | 497/3000 [00:00<00:01, 1649.05it/s]warmup should be done:  17%|█▋        | 507/3000 [00:00<00:01, 1680.75it/s]warmup should be done:  17%|█▋        | 508/3000 [00:00<00:01, 1682.68it/s]warmup should be done:  17%|█▋        | 506/3000 [00:00<00:01, 1449.73it/s]warmup should be done:  17%|█▋        | 498/3000 [00:00<00:01, 1427.48it/s]warmup should be done:  23%|██▎       | 683/3000 [00:00<00:01, 1707.78it/s]warmup should be done:  23%|██▎       | 682/3000 [00:00<00:01, 1699.10it/s]warmup should be done:  22%|██▏       | 664/3000 [00:00<00:01, 1655.00it/s]warmup should be done:  23%|██▎       | 676/3000 [00:00<00:01, 1682.77it/s]warmup should be done:  23%|██▎       | 679/3000 [00:00<00:01, 1692.44it/s]warmup should be done:  23%|██▎       | 689/3000 [00:00<00:01, 1708.67it/s]warmup should be done:  23%|██▎       | 679/3000 [00:00<00:01, 1548.89it/s]warmup should be done:  22%|██▏       | 668/3000 [00:00<00:01, 1522.28it/s]warmup should be done:  28%|██▊       | 855/3000 [00:00<00:01, 1711.52it/s]warmup should be done:  28%|██▊       | 853/3000 [00:00<00:01, 1700.54it/s]warmup should be done:  28%|██▊       | 831/3000 [00:00<00:01, 1659.31it/s]warmup should be done:  28%|██▊       | 845/3000 [00:00<00:01, 1683.08it/s]warmup should be done:  28%|██▊       | 852/3000 [00:00<00:01, 1703.14it/s]warmup should be done:  29%|██▊       | 860/3000 [00:00<00:01, 1703.56it/s]warmup should be done:  28%|██▊       | 838/3000 [00:00<00:01, 1580.58it/s]warmup should be done:  28%|██▊       | 837/3000 [00:00<00:01, 1465.61it/s]warmup should be done:  34%|███▍      | 1027/3000 [00:00<00:01, 1712.17it/s]warmup should be done:  33%|███▎      | 997/3000 [00:00<00:01, 1656.72it/s]warmup should be done:  34%|███▍      | 1024/3000 [00:00<00:01, 1699.58it/s]warmup should be done:  34%|███▍      | 1014/3000 [00:00<00:01, 1683.78it/s]warmup should be done:  34%|███▍      | 1023/3000 [00:00<00:01, 1704.75it/s]warmup should be done:  34%|███▍      | 1031/3000 [00:00<00:01, 1695.88it/s]warmup should be done:  33%|███▎      | 1004/3000 [00:00<00:01, 1603.52it/s]warmup should be done:  33%|███▎      | 986/3000 [00:00<00:01, 1450.60it/s]warmup should be done:  40%|███▉      | 1199/3000 [00:00<00:01, 1710.93it/s]warmup should be done:  39%|███▉      | 1163/3000 [00:00<00:01, 1655.02it/s]warmup should be done:  39%|███▉      | 1184/3000 [00:00<00:01, 1686.25it/s]warmup should be done:  40%|███▉      | 1194/3000 [00:00<00:01, 1694.97it/s]warmup should be done:  40%|███▉      | 1194/3000 [00:00<00:01, 1703.32it/s]warmup should be done:  40%|████      | 1201/3000 [00:00<00:01, 1685.67it/s]warmup should be done:  39%|███▉      | 1174/3000 [00:00<00:01, 1633.20it/s]warmup should be done:  38%|███▊      | 1147/3000 [00:00<00:01, 1496.55it/s]warmup should be done:  46%|████▌     | 1372/3000 [00:00<00:00, 1715.10it/s]warmup should be done:  45%|████▌     | 1355/3000 [00:00<00:00, 1693.01it/s]warmup should be done:  44%|████▍     | 1329/3000 [00:00<00:01, 1651.55it/s]warmup should be done:  46%|████▌     | 1367/3000 [00:00<00:00, 1709.39it/s]warmup should be done:  45%|████▌     | 1364/3000 [00:00<00:00, 1691.27it/s]warmup should be done:  46%|████▌     | 1370/3000 [00:00<00:00, 1685.92it/s]warmup should be done:  45%|████▍     | 1345/3000 [00:00<00:00, 1655.95it/s]warmup should be done:  44%|████▎     | 1309/3000 [00:00<00:01, 1533.67it/s]warmup should be done:  51%|█████▏    | 1544/3000 [00:00<00:00, 1713.86it/s]warmup should be done:  51%|█████     | 1526/3000 [00:00<00:00, 1697.14it/s]warmup should be done:  51%|█████▏    | 1538/3000 [00:00<00:00, 1708.40it/s]warmup should be done:  50%|████▉     | 1498/3000 [00:00<00:00, 1661.46it/s]warmup should be done:  51%|█████     | 1536/3000 [00:00<00:00, 1697.14it/s]warmup should be done:  51%|█████▏    | 1539/3000 [00:00<00:00, 1684.11it/s]warmup should be done:  51%|█████     | 1517/3000 [00:00<00:00, 1672.51it/s]warmup should be done:  49%|████▉     | 1472/3000 [00:00<00:00, 1562.39it/s]warmup should be done:  57%|█████▋    | 1716/3000 [00:01<00:00, 1712.69it/s]warmup should be done:  57%|█████▋    | 1698/3000 [00:01<00:00, 1701.48it/s]warmup should be done:  56%|█████▌    | 1667/3000 [00:01<00:00, 1670.21it/s]warmup should be done:  57%|█████▋    | 1709/3000 [00:01<00:00, 1702.39it/s]warmup should be done:  57%|█████▋    | 1708/3000 [00:01<00:00, 1701.99it/s]warmup should be done:  57%|█████▋    | 1708/3000 [00:01<00:00, 1683.22it/s]warmup should be done:  56%|█████▋    | 1688/3000 [00:01<00:00, 1683.67it/s]warmup should be done:  54%|█████▍    | 1633/3000 [00:01<00:00, 1576.34it/s]warmup should be done:  62%|██████▏   | 1869/3000 [00:01<00:00, 1702.99it/s]warmup should be done:  63%|██████▎   | 1888/3000 [00:01<00:00, 1711.97it/s]warmup should be done:  61%|██████    | 1837/3000 [00:01<00:00, 1676.53it/s]warmup should be done:  63%|██████▎   | 1881/3000 [00:01<00:00, 1707.81it/s]warmup should be done:  63%|██████▎   | 1880/3000 [00:01<00:00, 1699.42it/s]warmup should be done:  63%|██████▎   | 1877/3000 [00:01<00:00, 1684.35it/s]warmup should be done:  62%|██████▏   | 1859/3000 [00:01<00:00, 1690.38it/s]warmup should be done:  60%|█████▉    | 1797/3000 [00:01<00:00, 1593.18it/s]warmup should be done:  68%|██████▊   | 2041/3000 [00:01<00:00, 1705.09it/s]warmup should be done:  69%|██████▊   | 2061/3000 [00:01<00:00, 1714.91it/s]warmup should be done:  67%|██████▋   | 2006/3000 [00:01<00:00, 1678.37it/s]warmup should be done:  68%|██████▊   | 2053/3000 [00:01<00:00, 1710.85it/s]warmup should be done:  68%|██████▊   | 2050/3000 [00:01<00:00, 1699.21it/s]warmup should be done:  68%|██████▊   | 2046/3000 [00:01<00:00, 1684.52it/s]warmup should be done:  68%|██████▊   | 2031/3000 [00:01<00:00, 1696.93it/s]warmup should be done:  65%|██████▌   | 1957/3000 [00:01<00:00, 1567.07it/s]warmup should be done:  74%|███████▎  | 2212/3000 [00:01<00:00, 1705.06it/s]warmup should be done:  72%|███████▏  | 2174/3000 [00:01<00:00, 1678.30it/s]warmup should be done:  74%|███████▍  | 2233/3000 [00:01<00:00, 1711.82it/s]warmup should be done:  74%|███████▍  | 2225/3000 [00:01<00:00, 1710.78it/s]warmup should be done:  74%|███████▍  | 2220/3000 [00:01<00:00, 1695.13it/s]warmup should be done:  74%|███████▍  | 2215/3000 [00:01<00:00, 1682.35it/s]warmup should be done:  73%|███████▎  | 2201/3000 [00:01<00:00, 1687.61it/s]warmup should be done:  71%|███████   | 2122/3000 [00:01<00:00, 1589.22it/s]warmup should be done:  79%|███████▉  | 2383/3000 [00:01<00:00, 1705.81it/s]warmup should be done:  78%|███████▊  | 2343/3000 [00:01<00:00, 1681.66it/s]warmup should be done:  80%|████████  | 2405/3000 [00:01<00:00, 1708.29it/s]warmup should be done:  80%|███████▉  | 2397/3000 [00:01<00:00, 1711.14it/s]warmup should be done:  80%|███████▉  | 2390/3000 [00:01<00:00, 1692.95it/s]warmup should be done:  79%|███████▉  | 2384/3000 [00:01<00:00, 1678.81it/s]warmup should be done:  79%|███████▉  | 2373/3000 [00:01<00:00, 1695.10it/s]warmup should be done:  76%|███████▌  | 2286/3000 [00:01<00:00, 1604.27it/s]warmup should be done:  85%|████████▌ | 2555/3000 [00:01<00:00, 1707.86it/s]warmup should be done:  84%|████████▍ | 2514/3000 [00:01<00:00, 1687.91it/s]warmup should be done:  86%|████████▌ | 2569/3000 [00:01<00:00, 1711.13it/s]warmup should be done:  85%|████████▌ | 2560/3000 [00:01<00:00, 1694.31it/s]warmup should be done:  85%|████████▌ | 2553/3000 [00:01<00:00, 1681.08it/s]warmup should be done:  86%|████████▌ | 2576/3000 [00:01<00:00, 1681.74it/s]warmup should be done:  85%|████████▍ | 2545/3000 [00:01<00:00, 1699.75it/s]warmup should be done:  82%|████████▏ | 2452/3000 [00:01<00:00, 1618.12it/s]warmup should be done:  91%|█████████ | 2727/3000 [00:01<00:00, 1709.83it/s]warmup should be done:  90%|████████▉ | 2685/3000 [00:01<00:00, 1691.97it/s]warmup should be done:  91%|█████████ | 2730/3000 [00:01<00:00, 1694.96it/s]warmup should be done:  91%|█████████▏| 2741/3000 [00:01<00:00, 1706.76it/s]warmup should be done:  91%|█████████ | 2722/3000 [00:01<00:00, 1683.51it/s]warmup should be done:  92%|█████████▏| 2745/3000 [00:01<00:00, 1670.38it/s]warmup should be done:  91%|█████████ | 2717/3000 [00:01<00:00, 1703.25it/s]warmup should be done:  87%|████████▋ | 2618/3000 [00:01<00:00, 1627.84it/s]warmup should be done:  97%|█████████▋| 2898/3000 [00:01<00:00, 1709.52it/s]warmup should be done:  95%|█████████▌| 2855/3000 [00:01<00:00, 1694.09it/s]warmup should be done:  97%|█████████▋| 2900/3000 [00:01<00:00, 1694.25it/s]warmup should be done:  97%|█████████▋| 2912/3000 [00:01<00:00, 1703.12it/s]warmup should be done:  96%|█████████▋| 2891/3000 [00:01<00:00, 1681.96it/s]warmup should be done:  97%|█████████▋| 2913/3000 [00:01<00:00, 1664.81it/s]warmup should be done:  96%|█████████▋| 2888/3000 [00:01<00:00, 1705.01it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1702.57it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1698.92it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1696.74it/s]warmup should be done:  93%|█████████▎| 2784/3000 [00:01<00:00, 1637.32it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1695.56it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1688.32it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1674.61it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1658.79it/s]warmup should be done:  98%|█████████▊| 2950/3000 [00:01<00:00, 1642.93it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1578.83it/s]
WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f71481c40d0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f7148509d30>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f71481c40d0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f7148506b20>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f71481c4160>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f71481c21c0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f71481d22b0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f7148508a30>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
2022-12-12 02:07:22.353798: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f6c7f02d390 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 02:07:22.353863: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 02:07:22.364301: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 02:07:22.850501: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f6c86830790 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 02:07:22.850575: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 02:07:22.859515: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 02:07:22.984979: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f6c7e82c7a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 02:07:22.985042: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 02:07:22.995446: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 02:07:23.090702: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f6c8a799440 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 02:07:23.090763: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 02:07:23.098510: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 02:07:23.208263: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f6c8682c4e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 02:07:23.208330: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 02:07:23.216287: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 02:07:23.222293: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f6c7f02d8e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 02:07:23.222355: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 02:07:23.232380: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 02:07:23.233705: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f6c868384a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 02:07:23.233745: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 02:07:23.241483: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f6c86834b30 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 02:07:23.241549: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 02:07:23.243842: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 02:07:23.251396: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 02:07:29.565646: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 02:07:29.754630: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 02:07:29.804424: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 02:07:29.805718: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 02:07:30.052486: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 02:07:30.162416: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 02:07:30.319180: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 02:07:30.322831: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][02:08:26.783][ERROR][RK0][main]: replica 7 reaches 1000, calling init pre replica
[HCTR][02:08:26.783][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][02:08:26.791][ERROR][RK0][main]: coll ps creation done
[HCTR][02:08:26.791][ERROR][RK0][main]: replica 7 waits for coll ps creation barrier
[HCTR][02:08:27.106][ERROR][RK0][main]: replica 4 reaches 1000, calling init pre replica
[HCTR][02:08:27.106][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][02:08:27.113][ERROR][RK0][main]: coll ps creation done
[HCTR][02:08:27.113][ERROR][RK0][main]: replica 4 waits for coll ps creation barrier
[HCTR][02:08:27.153][ERROR][RK0][tid #140105071171328]: replica 1 reaches 1000, calling init pre replica
[HCTR][02:08:27.153][ERROR][RK0][tid #140105071171328]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][02:08:27.158][ERROR][RK0][tid #140105071171328]: coll ps creation done
[HCTR][02:08:27.158][ERROR][RK0][tid #140105071171328]: replica 1 waits for coll ps creation barrier
[HCTR][02:08:27.279][ERROR][RK0][main]: replica 6 reaches 1000, calling init pre replica
[HCTR][02:08:27.279][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][02:08:27.286][ERROR][RK0][main]: coll ps creation done
[HCTR][02:08:27.286][ERROR][RK0][main]: replica 6 waits for coll ps creation barrier
[HCTR][02:08:27.289][ERROR][RK0][main]: replica 5 reaches 1000, calling init pre replica
[HCTR][02:08:27.289][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][02:08:27.294][ERROR][RK0][main]: coll ps creation done
[HCTR][02:08:27.294][ERROR][RK0][main]: replica 5 waits for coll ps creation barrier
[HCTR][02:08:27.354][ERROR][RK0][tid #140105071171328]: replica 0 reaches 1000, calling init pre replica
[HCTR][02:08:27.355][ERROR][RK0][tid #140105071171328]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][02:08:27.360][ERROR][RK0][tid #140105071171328]: coll ps creation done
[HCTR][02:08:27.360][ERROR][RK0][tid #140105071171328]: replica 0 waits for coll ps creation barrier
[HCTR][02:08:27.422][ERROR][RK0][main]: replica 3 reaches 1000, calling init pre replica
[HCTR][02:08:27.422][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][02:08:27.427][ERROR][RK0][main]: coll ps creation done
[HCTR][02:08:27.427][ERROR][RK0][main]: replica 3 waits for coll ps creation barrier
[HCTR][02:08:27.605][ERROR][RK0][tid #140104182003456]: replica 2 reaches 1000, calling init pre replica
[HCTR][02:08:27.605][ERROR][RK0][tid #140104182003456]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][02:08:27.609][ERROR][RK0][tid #140104182003456]: coll ps creation done
[HCTR][02:08:27.609][ERROR][RK0][tid #140104182003456]: replica 2 waits for coll ps creation barrier
[HCTR][02:08:27.609][ERROR][RK0][tid #140105071171328]: replica 0 preparing frequency
[HCTR][02:08:28.464][ERROR][RK0][tid #140105071171328]: replica 0 preparing frequency done
[HCTR][02:08:28.511][ERROR][RK0][main]: replica 4 calling init per replica
[HCTR][02:08:28.511][ERROR][RK0][tid #140105071171328]: replica 0 calling init per replica
[HCTR][02:08:28.511][ERROR][RK0][main]: replica 3 calling init per replica
[HCTR][02:08:28.511][ERROR][RK0][tid #140105071171328]: replica 1 calling init per replica
[HCTR][02:08:28.511][ERROR][RK0][main]: replica 7 calling init per replica
[HCTR][02:08:28.511][ERROR][RK0][main]: replica 6 calling init per replica
[HCTR][02:08:28.511][ERROR][RK0][main]: replica 5 calling init per replica
[HCTR][02:08:28.511][ERROR][RK0][tid #140104182003456]: replica 2 calling init per replica
[HCTR][02:08:28.511][ERROR][RK0][main]: Calling build_v2
[HCTR][02:08:28.511][ERROR][RK0][tid #140105071171328]: Calling build_v2
[HCTR][02:08:28.511][ERROR][RK0][main]: Calling build_v2
[HCTR][02:08:28.511][ERROR][RK0][tid #140105071171328]: Calling build_v2
[HCTR][02:08:28.511][ERROR][RK0][main]: Calling build_v2
[HCTR][02:08:28.511][ERROR][RK0][main]: Calling build_v2
[HCTR][02:08:28.511][ERROR][RK0][main]: Calling build_v2
[HCTR][02:08:28.511][ERROR][RK0][tid #140104182003456]: Calling build_v2
[HCTR][02:08:28.511][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][02:08:28.511][ERROR][RK0][tid #140105071171328]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][02:08:28.511][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][02:08:28.511][ERROR][RK0][tid #140105071171328]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][02:08:28.511][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][02:08:28.511][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][02:08:28.511][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][02:08:28.511][ERROR][RK0][tid #140104182003456]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[[[[[[[2022-12-12 02:08:282022-12-12 02:08:28[2022-12-12 02:08:282022-12-12 02:08:282022-12-12 02:08:28.2022-12-12 02:08:28.2022-12-12 02:08:28...511325.5113252022-12-12 02:08:28.511333511333511333: 511333: .511333: : : E: E511358: EEE E : E   /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.ccE /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc: /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:::136:136/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:136136136] 136] :136] ] ] using concurrent impl MPSPhase] using concurrent impl MPSPhase136] using concurrent impl MPSPhaseusing concurrent impl MPSPhaseusing concurrent impl MPSPhase
using concurrent impl MPSPhase
] using concurrent impl MPSPhase



using concurrent impl MPSPhase

[2022-12-12 02:08:28.515708: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-12 02:08:28.515745: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:[1962022-12-12 02:08:28] .assigning 8 to cpu515754
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-12 02:08:28.515801: [[E2022-12-12 02:08:282022-12-12 02:08:28 ../hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc515804515811:: : [196EE2022-12-12 02:08:28]   .assigning 8 to cpu/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[515849
::2022-12-12 02:08:28: 178212.E] ] [515894 v100x8, slow pciebuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 82022-12-12 02:08:28: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc

[.E:2022-12-12 02:08:28[515942 [178.[2022-12-12 02:08:28: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 02:08:28] 5159762022-12-12 02:08:28.E[:.v100x8, slow pcie: .515992 2022-12-12 02:08:28178515993
E516006: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.] :  : [E:516040v100x8, slow pcieE/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE2022-12-12 02:08:28 178: 
 : ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc212[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc516118:v100x8, slow pcie :] 2022-12-12 02:08:28:: 196
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc178build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8.213E] :[] 
516176]  assigning 8 to cpu1782022-12-12 02:08:28v100x8, slow pcie: remote time is 8.68421/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
] [.
E
:v100x8, slow pcie2022-12-12 02:08:28516240 [196
[.: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 02:08:28] [2022-12-12 02:08:28[516283E:.assigning 8 to cpu2022-12-12 02:08:28.2022-12-12 02:08:28:  196516327
.516335.E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] : 516357: 516363 :assigning 8 to cpuE: E[: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc196
 E 2022-12-12 02:08:28E:] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc. 213assigning 8 to cpu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[:516474/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] 
196:2022-12-12 02:08:28214: :remote time is 8.68421] 212.] E196
assigning 8 to cpu] 516551cpu time is 97.0588[ ] 
build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8[: 
2022-12-12 02:08:28/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccassigning 8 to cpu
2022-12-12 02:08:28E.:
. 516640212[516656[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: ] 2022-12-12 02:08:28: 2022-12-12 02:08:28:Ebuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8.E[.212 
516715 2022-12-12 02:08:28516718] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.[: build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8:E:5167472022-12-12 02:08:28E
212 214: . ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[] E516793/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccbuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8:2022-12-12 02:08:28cpu time is 97.0588 : :
213.
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE212] 516858[: ] remote time is 8.68421: 2022-12-12 02:08:28212/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccbuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8
E.] :
 516931[build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8213/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 2022-12-12 02:08:28
] [:E.remote time is 8.684212022-12-12 02:08:28213[ 516983
.] 2022-12-12 02:08:28/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 517002[remote time is 8.68421.:E: 2022-12-12 02:08:28
517021213 E.: ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[ 517053Eremote time is 8.68421:2022-12-12 02:08:28/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:  
214.:E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] 517097[213 :cpu time is 97.0588: 2022-12-12 02:08:28] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc213
E.remote time is 8.68421:]  517148
214remote time is 8.68421/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: ] 
:[Ecpu time is 97.05882142022-12-12 02:08:28[ 
] .2022-12-12 02:08:28/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cccpu time is 97.0588517230.:
: 517244214E: ]  Ecpu time is 97.0588/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 
:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc214:] 214cpu time is 97.0588] 
cpu time is 97.0588
[2022-12-12 02:09:47. 20410: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-12 02:09:47. 60632: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 381.47 MB
[2022-12-12 02:09:47. 60737: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 381.47 MB
[2022-12-12 02:09:47. 61814: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:74] mapping nid to rank...
[2022-12-12 02:09:47.140455: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:91] counting slots...
[2022-12-12 02:09:47.522763: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:105] Final num slot is 49
[2022-12-12 02:09:47.522854: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:109] counting blocks...
[2022-12-12 02:09:54.496732: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:118] Final num block is 1023
[2022-12-12 02:09:54.496825: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:123] counting freq and density...
[2022-12-12 02:09:56.209668: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:154] averaging freq and density...
[2022-12-12 02:09:56.209777: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:155] 1023
[2022-12-12 02:09:56.212651: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
[2022-12-12 02:09:56.212711: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:354] constructing optimal solver, device=8, stream=1
1023 blocks, 8 devices
[2022-12-12 02:09:56.510722: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:527] Add Var...
[2022-12-12 02:09:56.538624: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:545] Capacity...
[2022-12-12 02:09:56.540093: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:548] Connect CPU...
[2022-12-12 02:09:56.561187: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:550] Connect Access To Storage...
[2022-12-12 02:09:57. 91265: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:554] Time...
[2022-12-12 02:09:57. 93527: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 0, total sm is 80
[2022-12-12 02:09:57. 96560: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 1, total sm is 80
[2022-12-12 02:09:57. 99553: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 2, total sm is 80
[2022-12-12 02:09:57.102493: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 3, total sm is 80
[2022-12-12 02:09:57.105506: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 4, total sm is 80
[2022-12-12 02:09:57.108460: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 5, total sm is 80
[2022-12-12 02:09:57.111462: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 6, total sm is 80
[2022-12-12 02:09:57.114405: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 7, total sm is 80
[2022-12-12 02:10:45.975051: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:569] Coll Cache init block placement array
[2022-12-12 02:10:45.983404: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:645] Coll Cache init block placement array done
[2022-12-12 02:10:45.985703: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:647] Coll Cache model reset done
[2022-12-12 02:10:46. 30875: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-12 02:10:46. 30975: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-12 02:10:46. 31016: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-12 02:10:46. 31048: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-12 02:10:46. 31629: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 02:10:46. 31684: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:10:46. 32916: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:10:46. 33602: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:10:46. 46285: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 2 solved
[2022-12-12 02:10:46. 46362: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 2 initing device 2
[2022-12-12 02:10:46. 46633: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 1 solved
[2022-12-12 02:10:46. 46699: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 1 initing device 1
[2022-12-12 02:10:46. 46756: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 5 solved[
2022-12-12 02:10:46. 46807: [E2022-12-12 02:10:46 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu 46821:: 1815E]  Building Coll Cache with ... num gpu device is 8/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc
:205] worker 0 thread 5 initing device 5
[2022-12-12 02:10:46. 46879: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:10:46. 47119: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 02:10:46. 47176: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:10:46. 47282: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 02:10:46. 47327: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:10:46. 48975: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:10:46. 49036: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:10:46. 49085: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:10:46. 50640: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 6 solved
[2022-12-12 02:10:46. 50695: E[ 2022-12-12 02:10:46/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc[.:2022-12-12 02:10:46 50684205.: ]  50725Eworker 0 thread 6 initing device 6: [ 
E2022-12-12 02:10:46/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc .:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu 50781202:: [] 1980E2022-12-12 02:10:467 solved]  .
eager alloc mem 381.47 MB/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu 50828
[:: 2022-12-12 02:10:461980E.]   50884eager alloc mem 381.47 MB/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: 
:E1980 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cceager alloc mem 381.47 MB:
205] worker 0 thread 7 initing device 7
[2022-12-12 02:10:46. 51211: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 02:10:46. 51256: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:10:46. 51393: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 02:10:46. 51445: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:10:46. 54032: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:10:46. 54085: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:10:46. 54556: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 3 solved
[2022-12-12 02:10:46. 54618: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:[2052022-12-12 02:10:46] .worker 0 thread 3 initing device 3 54620
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 4 solved
[2022-12-12 02:10:46. 54695: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 4 initing device 4
[2022-12-12 02:10:46. 55076: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 02:10:46. 55124: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] [eager alloc mem 381.47 MB2022-12-12 02:10:46
. 55148: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 02:10:46. 55205: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:10:46. 55296: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:10:46. 55353: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:10:46. 57815: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:10:46. 57881: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:10:46. 59156: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:10:46. 59235: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 02:10:46.111004: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1023.00 Bytes
[2022-12-12 02:10:46.116504: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1023
[2022-12-12 02:10:46.116644: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 02:10:46.117459: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 19.45 MB
[2022-12-12 02:10:46.118063: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:10:46.119078: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:10:46.119150: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 18.97 MB
[2022-12-12 02:10:46.122722: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 02:10:46.123512: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 02:10:46.123559: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.38 GB
[[[2022-12-12 02:10:462022-12-12 02:10:462022-12-12 02:10:46...130324130324130343: : : EEE   /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:::198019801980] ] ] eager alloc mem 1023.00 Byteseager alloc mem 1023.00 Byteseager alloc mem 1023.00 Bytes


[2022-12-12 02:10:46.131001: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1023.00 Bytes
[2022-12-12 02:10:46.131251: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1023.00 Bytes
[[2022-12-12 02:10:462022-12-12 02:10:46..131963131962: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 1023.00 Byteseager alloc mem 1023.00 Bytes

[2022-12-12 02:10:46.135837: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1023
[2022-12-12 02:10:46.135929: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 02:10:46.141618: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 19.45 MB
[2022-12-12 02:10:46.141892: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1023
[2022-12-12 02:10:46.141973: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 02:10:46.142103: E[ 2022-12-12 02:10:46/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.:1421141980: ] Eeager alloc mem 611.00 KB 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1023
[2022-12-12 02:10:46.[1422072022-12-12 02:10:46: .E142200 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE: 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] :eager release cuda mem 400000000638
] eager release cuda mem 1023
[2022-12-12 02:10:46.142295: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 02:10:46.142487: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1023
[2022-12-12 02:10:46.142565: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 02:10:46.142675: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1023[
2022-12-12 02:10:46.142711: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 19.45 MB
[2022-12-12 02:10:46.142757: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-12 02:10:46:.638142758] : eager release cuda mem 400000000E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1023
[2022-12-12 02:10:46.142848: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 02:10:46.143090: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:10:46.143149: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 19.03 MB
[2022-12-12 02:10:46.143892: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 19.45 MB
[2022-12-12 02:10:46.144540: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 19.45 MB
[2022-12-12 02:10:46.145056: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 19.45 MB
[2022-12-12 02:10:46.145803: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 19.45 MB
[2022-12-12 02:10:46.146460: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 19.45 MB
[2022-12-12 02:10:46.146717: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:10:46.147349: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:10:46.147466: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:10:46.147511: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:10:46.147630: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:10:46.147674: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] [eager alloc mem 611.00 KB2022-12-12 02:10:46
.147701: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:10:46.147759: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 19.06 MB
[2022-12-12 02:10:46.147872: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 02:10:46.148327: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:10:46.148371: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 19.02 MB
[2022-12-12 02:10:46.148447: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:10:46.148480: [E2022-12-12 02:10:46 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc148491:: 638W]  eager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc
:43] WORKER[0] alloc host memory 19.02 MB
[2022-12-12 02:10:46.148539: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 19.05 MB
[2022-12-12 02:10:46.148590: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638[] 2022-12-12 02:10:46eager release cuda mem 25855.
148608: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663[
2022-12-12 02:10:46.148651: [E2022-12-12 02:10:46 [./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-12 02:10:46148664:.: 1980148678E] :  eager alloc mem 2.38 GBW/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
 :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc638:] 43eager release cuda mem 625663] 
WORKER[0] alloc host memory 19.05 MB
[2022-12-12 02:10:46.148789: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 19.07 MB
[2022-12-12 02:10:46.152346: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 02:10:46.153062: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 02:10:46.153106: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.39 GB
[2022-12-12 02:10:46.153461: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 02:10:46.154174: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 02:10:46.154215: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.39 GB
[2022-12-12 02:10:46.158736: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 02:10:46.159341: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 02:10:46.159384: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.38 GB
[2022-12-12 02:10:46.161494: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 02:10:46.161989: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 02:10:46.162095: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 02:10:46.162139: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.39 GB
[2022-12-12 02:10:46.162219: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 02:10:46.162584: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 02:10:46.162625: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.39 GB
[2022-12-12 02:10:46.162808: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 02:10:46.162848: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.38 GB
[[[[[[[[2022-12-12 02:10:462022-12-12 02:10:462022-12-12 02:10:462022-12-12 02:10:462022-12-12 02:10:462022-12-12 02:10:462022-12-12 02:10:462022-12-12 02:10:46........932535932534932534932536932534932534932534932534: : : : : : : : EEEEEEEE        /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::::::19261926192619261926192619261926] ] ] ] ] ] ] ] Device 7 init p2p of link 4Device 2 init p2p of link 1Device 1 init p2p of link 7Device 5 init p2p of link 6Device 4 init p2p of link 5Device 0 init p2p of link 3Device 6 init p2p of link 0Device 3 init p2p of link 2







[2022-12-12 02:10:46[.[2022-12-12 02:10:46[9331372022-12-12 02:10:46.2022-12-12 02:10:46: [[.933140.E2022-12-12 02:10:462022-12-12 02:10:46933142: 933148 [..: E: /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-12 02:10:46933157933157E E:.: : [ /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu 1980933186EE2022-12-12 02:10:46/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] :   :9332231980:eager alloc mem 611.00 KBE/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980: ] 1980
 ::] Eeager alloc mem 611.00 KB] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu19801980eager alloc mem 611.00 KB 
eager alloc mem 611.00 KB:] ] 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
1980eager alloc mem 611.00 KBeager alloc mem 611.00 KB:] 

1980eager alloc mem 611.00 KB] 
eager alloc mem 611.00 KB
[2022-12-12 02:10:46.934141: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:10:46.934188: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] [eager release cuda mem 625663[2022-12-12 02:10:46
[2022-12-12 02:10:46.2022-12-12 02:10:46.934208[.[934212: 2022-12-12 02:10:469342172022-12-12 02:10:46: E.: .E [934236E934252 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-12 02:10:46:  : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:.E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE:638934287 : 638] : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] eager release cuda mem 625663E:] :eager release cuda mem 625663
 638eager release cuda mem 625663638
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] 
] :eager release cuda mem 625663eager release cuda mem 625663638

] eager release cuda mem 625663
[2022-12-12 02:10:46.948372: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 2
[2022-12-12 02:10:46.948534: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:10:46.949063: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 3
[2022-12-12 02:10:46.949169: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 5
[2022-12-12 02:10:46.949212: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:10:46.949310: E[ 2022-12-12 02:10:46/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.:9493351926: ] E[Device 4 init p2p of link 7 2022-12-12 02:10:46
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.:9493551980: ] Eeager alloc mem 611.00 KB 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:10:46.949485: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:10:46.949627: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 1
[2022-12-12 02:10:46.949780: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:10:46.949967: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 6
[2022-12-12 02:10:46.950043: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[[2022-12-12 02:10:462022-12-12 02:10:46..950111950124: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19261980] ] Device 3 init p2p of link 0eager alloc mem 611.00 KB

[2022-12-12 02:10:46.950202: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:10:46.950290: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[:2022-12-12 02:10:46638.] 950305eager release cuda mem 625663: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:10:46.950405: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 4
[2022-12-12 02:10:46.950579: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:10:46.950613: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:10:46.950943: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:10:46.951122: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:10:46.951516: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:10:46.961666: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 3
[2022-12-12 02:10:46.961787: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:10:46.962604: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:10:46.963245: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 0
[2022-12-12 02:10:46.963365: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:10:46.963434: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 4
[2022-12-12 02:10:46.963506: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 6
[2022-12-12 02:10:46[.2022-12-12 02:10:46963546.: 963565E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1926:] 1980Device 0 init p2p of link 1] 
eager alloc mem 611.00 KB
[2022-12-12 02:10:46.963621: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:10:46.963665: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] [Device 4 init p2p of link 22022-12-12 02:10:46
.963700: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:10:46.963788: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:10:46.964170: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:10:46.964356: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 5
[2022-12-12 02:10:46.964398: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:10:46.964425: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:10:46.964477: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:10:46.964529: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:10:46.964592: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:10:46.964727: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 7
[2022-12-12 02:10:46.964852: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:10:46.965267: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:10:46.965652: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:10:46.976197: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 0
[2022-12-12 02:10:46.976315: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:10:46.977117: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:10:46.980167: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 6
[2022-12-12 02:10:46.980260: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-12 02:10:46:.1926980284] : Device 2 init p2p of link 4E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:10:46.980387: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:10:46.980637: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 7
[2022-12-12 02:10:46.980753: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:10:46.980941: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 5
[2022-12-12 02:10:46.981103: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:10:46.981168: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:10:46.981256: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB[
2022-12-12 02:10:46[.2022-12-12 02:10:46981285.: 981295E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1926:] 1926Device 0 init p2p of link 2] 
Device 3 init p2p of link 1
[2022-12-12 02:10:46.[9814372022-12-12 02:10:46: .E981441 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: 1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] :eager alloc mem 611.00 KB1980
] eager alloc mem 611.00 KB
[2022-12-12 02:10:46.981568: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:10:46.982148: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:10:46.982264: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638[] 2022-12-12 02:10:46eager release cuda mem 625663.
982281: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:10:46.982469: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 3
[2022-12-12 02:10:46.982609: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 02:10:46.983399: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 02:10:46.984218: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 20400000
[2022-12-12 02:10:46.988763: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 4988993 / 100000000 nodes ( 4.99 %~5.00 %) | remote 14968500 / 100000000 nodes ( 14.97 %) | cpu 80042507 / 100000000 nodes ( 80.04 %) | 2.38 GB | 0.941594 secs 
[2022-12-12 02:10:46.997004: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 20400000
[2022-12-12 02:10:46.998076: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 20400000
[2022-12-12 02:10:46.998451: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 20400000
[2022-12-12 02:10:46.998739: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 4997234 / 100000000 nodes ( 5.00 %~5.00 %) | remote 14960259 / 100000000 nodes ( 14.96 %) | cpu 80042507 / 100000000 nodes ( 80.04 %) | 2.39 GB | 0.951875 secs 
[2022-12-12 02:10:46.998886: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 20400000
[2022-12-12 02:10:46.999109: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 4984926 / 100000000 nodes ( 4.98 %~5.00 %) | remote 14972567 / 100000000 nodes ( 14.97 %) | cpu 80042507 / 100000000 nodes ( 80.04 %) | 2.38 GB | 0.943912 secs 
[2022-12-12 02:10:46.999450: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 4992847 / 100000000 nodes ( 4.99 %~5.00 %) | remote 14964646 / 100000000 nodes ( 14.96 %) | cpu 80042507 / 100000000 nodes ( 80.04 %) | 2.39 GB | 0.948202 secs 
[2022-12-12 02:10:46.999755[: 2022-12-12 02:10:46E. 999770/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: :E1955 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:Asymm Coll cache (policy: coll_cache_asymm_link) | local 4994154 / 100000000 nodes ( 4.99 %~5.00 %) | remote 14963339 / 100000000 nodes ( 14.96 %) | cpu 80042507 / 100000000 nodes ( 80.04 %) | 2.39 GB | 0.948323 secs 638
] eager release cuda mem 20400000
[2022-12-12 02:10:47.     2: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 20400000
[2022-12-12 02:10:47.    52: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 20400000
[2022-12-12 02:10:47.  2766: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 4999428 / 100000000 nodes ( 5.00 %~5.00 %) | remote 14958065 / 100000000 nodes ( 14.96 %) | cpu 80042507 / 100000000 nodes ( 80.04 %) | 2.39 GB | 0.947651 secs 
[2022-12-12 02:10:47.  3518: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 4985566 / 100000000 nodes ( 4.99 %~5.00 %) | remote 14971927 / 100000000 nodes ( 14.97 %) | cpu 80042507 / 100000000 nodes ( 80.04 %) | 2.38 GB | 0.956196 secs 
[2022-12-12 02:10:47.  4009: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 4971838 / 100000000 nodes ( 4.97 %~5.00 %) | remote 14985655 / 100000000 nodes ( 14.99 %) | cpu 80042507 / 100000000 nodes ( 80.04 %) | 2.38 GB | 0.97234 secs 
[2022-12-12 02:10:47.  5336: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2213] before create ctx, mem is 10.70 GB
[2022-12-12 02:10:48.314355: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2219] after create ctx, mem is 10.96 GB
[2022-12-12 02:10:48.315119: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2226] after create stream, mem is 10.96 GB
[2022-12-12 02:10:48.315468: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2213] before create ctx, mem is 10.96 GB
[2022-12-12 02:10:49.578642: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2219] after create ctx, mem is 11.22 GB
[2022-12-12 02:10:49.579260: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2226] after create stream, mem is 11.22 GB
[2022-12-12 02:10:49.580305: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2213] before create ctx, mem is 11.22 GB
[2022-12-12 02:10:50.838173: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2219] after create ctx, mem is 11.44 GB
[2022-12-12 02:10:50.838305: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2226] after create stream, mem is 11.44 GB
[2022-12-12 02:10:50.838648: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2213] before create ctx, mem is 11.44 GB
[2022-12-12 02:10:52. 39391: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2219] after create ctx, mem is 11.65 GB
[2022-12-12 02:10:52. 40700: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2226] after create stream, mem is 11.65 GB
[2022-12-12 02:10:52. 41454: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2243] before create stream, mem is 11.65 GB
[2022-12-12 02:10:52. 42024: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2249] after create stream, mem is 11.65 GB
[2022-12-12 02:10:52. 43457: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2213] before create ctx, mem is 11.65 GB
[2022-12-12 02:10:53.108135: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2219] after create ctx, mem is 11.85 GB
[2022-12-12 02:10:53.134410: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2226] after create stream, mem is 11.85 GB
[HCTR][02:10:54.222][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier
[HCTR][02:10:54.222][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier
[HCTR][02:10:54.222][ERROR][RK0][tid #140104182003456]: replica 2 calling init per replica done, doing barrier
[HCTR][02:10:54.222][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier
[HCTR][02:10:54.222][ERROR][RK0][tid #140105071171328]: replica 1 calling init per replica done, doing barrier
[HCTR][02:10:54.222][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier
[HCTR][02:10:54.222][ERROR][RK0][tid #140105071171328]: replica 0 calling init per replica done, doing barrier
[HCTR][02:10:54.222][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier
[HCTR][02:10:54.222][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier done
[HCTR][02:10:54.222][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier done
[HCTR][02:10:54.222][ERROR][RK0][tid #140104182003456]: replica 2 calling init per replica done, doing barrier done
[HCTR][02:10:54.222][ERROR][RK0][tid #140105071171328]: replica 0 calling init per replica done, doing barrier done
[HCTR][02:10:54.222][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier done
[HCTR][02:10:54.222][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier done
[HCTR][02:10:54.222][ERROR][RK0][tid #140105071171328]: replica 1 calling init per replica done, doing barrier done
[HCTR][02:10:54.222][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier done
[HCTR][02:10:54.222][ERROR][RK0][main]: init per replica done
[HCTR][02:10:54.222][ERROR][RK0][main]: init per replica done
[HCTR][02:10:54.222][ERROR][RK0][tid #140104182003456]: init per replica done
[HCTR][02:10:54.222][ERROR][RK0][main]: init per replica done
[HCTR][02:10:54.222][ERROR][RK0][main]: init per replica done
[HCTR][02:10:54.222][ERROR][RK0][tid #140105071171328]: init per replica done
[HCTR][02:10:54.222][ERROR][RK0][main]: init per replica done
[HCTR][02:10:54.225][ERROR][RK0][tid #140105071171328]: init per replica done
[HCTR][02:10:54.261][ERROR][RK0][tid #140104668518144]: 7 allocated 3276800 at 0x7f50b0238400
[HCTR][02:10:54.261][ERROR][RK0][main]: 1 allocated 3276800 at 0x7f5028238400
[HCTR][02:10:54.261][ERROR][RK0][tid #140104668518144]: 7 allocated 6553600 at 0x7f50b0558400
[HCTR][02:10:54.261][ERROR][RK0][main]: 1 allocated 6553600 at 0x7f5028558400
[HCTR][02:10:54.261][ERROR][RK0][tid #140104668518144]: 7 allocated 3276800 at 0x7f50b0b98400
[HCTR][02:10:54.261][ERROR][RK0][main]: 1 allocated 3276800 at 0x7f5028b98400
[HCTR][02:10:54.261][ERROR][RK0][tid #140104668518144]: 7 allocated 6553600 at 0x7f50b0eb8400
[HCTR][02:10:54.261][ERROR][RK0][main]: 1 allocated 6553600 at 0x7f5028eb8400
[HCTR][02:10:54.261][ERROR][RK0][main]: 6 allocated 3276800 at 0x7f4fb8238400
[HCTR][02:10:54.261][ERROR][RK0][main]: 6 allocated 6553600 at 0x7f4fb8558400
[HCTR][02:10:54.261][ERROR][RK0][main]: 6 allocated 3276800 at 0x7f4fb8b98400
[HCTR][02:10:54.261][ERROR][RK0][main]: 6 allocated 6553600 at 0x7f4fb8eb8400
[HCTR][02:10:54.261][ERROR][RK0][main]: 4 allocated 3276800 at 0x7f50bc238400
[HCTR][02:10:54.261][ERROR][RK0][tid #140104802735872]: 5 allocated 3276800 at 0x7f5010238400
[HCTR][02:10:54.261][ERROR][RK0][main]: 4 allocated 6553600 at 0x7f50bc558400
[HCTR][02:10:54.261][ERROR][RK0][tid #140104802735872]: 5 allocated 6553600 at 0x7f5010558400
[HCTR][02:10:54.261][ERROR][RK0][main]: 4 allocated 3276800 at 0x7f50bcb98400
[HCTR][02:10:54.261][ERROR][RK0][tid #140104802735872]: 5 allocated 3276800 at 0x7f5010b98400
[HCTR][02:10:54.261][ERROR][RK0][main]: 4 allocated 6553600 at 0x7f50bceb8400
[HCTR][02:10:54.261][ERROR][RK0][tid #140104802735872]: 5 allocated 6553600 at 0x7f5010eb8400
[HCTR][02:10:54.261][ERROR][RK0][main]: 3 allocated 3276800 at 0x7f4ff0238400
[HCTR][02:10:54.262][ERROR][RK0][main]: 3 allocated 6553600 at 0x7f4ff0558400
[HCTR][02:10:54.262][ERROR][RK0][main]: 3 allocated 3276800 at 0x7f4ff0b98400
[HCTR][02:10:54.262][ERROR][RK0][main]: 3 allocated 6553600 at 0x7f4ff0eb8400
[HCTR][02:10:54.262][ERROR][RK0][main]: 2 allocated 3276800 at 0x7f4fc4238400
[HCTR][02:10:54.262][ERROR][RK0][main]: 2 allocated 6553600 at 0x7f4fc4558400
[HCTR][02:10:54.262][ERROR][RK0][main]: 2 allocated 3276800 at 0x7f4fc4b98400
[HCTR][02:10:54.262][ERROR][RK0][main]: 2 allocated 6553600 at 0x7f4fc4eb8400
[HCTR][02:10:54.264][ERROR][RK0][main]: 0 allocated 3276800 at 0x7f5090320000
[HCTR][02:10:54.264][ERROR][RK0][main]: 0 allocated 6553600 at 0x7f5090640000
[HCTR][02:10:54.264][ERROR][RK0][main]: 0 allocated 3276800 at 0x7f5090c80000
[HCTR][02:10:54.264][ERROR][RK0][main]: 0 allocated 6553600 at 0x7f5090fa0000
