2022-12-11 21:11:46.719341: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.727066: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.734014: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.739545: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.745711: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.756979: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.764251: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.776383: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.825699: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.837486: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.841513: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.841968: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.842749: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.843650: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.844338: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.845405: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.846049: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.847108: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.847483: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.848815: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.849026: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.850552: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.850596: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.852224: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.852264: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.854175: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.854277: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.855632: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.856684: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.857631: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.858537: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.859468: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.861244: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.862382: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.863333: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.864334: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.865350: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.866371: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.867436: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.868364: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.873505: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 21:11:46.875245: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.876360: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.877526: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.879149: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.880947: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.881103: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.883005: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.883039: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.883074: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.883575: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.885487: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.885571: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.885617: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.885898: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.888325: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.888448: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.888470: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.888517: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.891038: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.891188: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.893251: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.893419: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.895915: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.896128: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.898176: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.898916: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.899120: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.899673: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.901817: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.902445: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.902752: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.903315: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.905394: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.906170: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.906603: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.906747: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.908398: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.908935: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.909369: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.909665: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.911325: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.911848: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.912067: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.912552: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.914126: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.914598: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.914684: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.915310: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.916732: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.916907: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.917115: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.923578: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.926022: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.926118: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.926451: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.927281: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.928364: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.929185: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.929850: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.930751: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.935140: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.945963: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.949874: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.966703: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.967686: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.967763: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.967833: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.967896: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.972150: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.972208: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.972295: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.972344: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.972344: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.976842: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.976898: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.976948: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.976993: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.977057: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.980047: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.980140: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.980230: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.980279: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.980327: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.983674: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.983833: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.983860: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.983912: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.983957: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.987567: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.987668: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.987711: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.987766: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.987804: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.991044: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.991091: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.991197: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.991246: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.991690: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.994010: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.994190: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.994306: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.994392: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.995014: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.997308: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.997446: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.997553: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.997586: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:46.998515: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.001280: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.001884: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.001932: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.002089: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.003449: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.005407: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.005591: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.005703: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.005765: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.007050: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.009186: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.009384: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.009433: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.009560: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.009589: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.010999: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.013023: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.013162: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.013465: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.013636: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.013713: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.013953: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.015100: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.017213: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.017490: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.017775: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.017898: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.018131: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.018469: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.019617: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.021683: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.022030: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.022232: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.022376: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.022608: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.023451: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.027588: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.027642: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.027793: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.027799: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.028313: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.028393: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.028781: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 21:11:47.031552: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.031607: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.031702: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.031893: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.031990: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.032051: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.035552: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.035599: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.036094: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.036183: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.036310: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.037394: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.038429: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.039338: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.039341: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.039981: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.040135: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.040142: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.041777: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.043421: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.044011: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.044013: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.044600: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.045552: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.045651: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.047881: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.049525: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.050172: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.050247: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.050821: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.050838: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.051083: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.052916: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.054930: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.055899: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.055921: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.055969: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.056197: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.057482: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.059366: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.060437: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.060490: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.060667: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.060799: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.062287: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.063978: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.065142: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.066429: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.069080: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 21:11:47.069321: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 21:11:47.069321: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 21:11:47.070792: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.071924: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.073454: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.074968: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.075661: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.077769: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.078525: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.079166: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.079204: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.080103: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.080537: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 21:11:47.081126: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.081659: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.082903: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.082919: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.084199: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.085169: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.087756: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.088281: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.088354: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.089723: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.089959: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.090880: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.092933: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.093966: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.095586: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.098265: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.100212: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.101834: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.103929: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.137174: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.139578: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.170948: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.172865: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.177338: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.180181: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.186372: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.188283: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 21:11:47.196434: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 21:11:47.197782: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.202248: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.205912: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.207168: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.214442: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:47.218998: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:48.201984: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:48.202589: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:48.203138: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:48.203869: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 21:11:48.203929: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-11 21:11:48.222381: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:48.223888: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:48.224407: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:48.224992: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:48.225624: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:48.226437: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-11 21:11:48.271102: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:11:48.271323: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:11:48.312460: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-11 21:11:48.437860: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:48.438481: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:48.439176: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:48.439653: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 21:11:48.439706: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-11 21:11:48.457485: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:48.458286: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:48.458816: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:48.459420: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:48.459959: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:48.460428: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-11 21:11:48.509794: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:48.510586: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:48.511327: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:48.511949: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 21:11:48.512005: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-11 21:11:48.515986: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:48.516589: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:48.517128: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:48.517805: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 21:11:48.517861: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-11 21:11:48.524533: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:48.525146: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:48.525660: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:48.526352: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 21:11:48.526408: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-11 21:11:48.529015: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:48.529656: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:48.530285: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:48.530873: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:48.531409: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:48.531888: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-11 21:11:48.534759: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:48.535844: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:48.536302: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:48.536439: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:48.537289: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:48.537340: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 21:11:48.537392: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-11 21:11:48.538182: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:48.538758: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:48.539830: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:48.539983: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:48.540812: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-11 21:11:48.540969: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:48.541509: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:48.542133: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 21:11:48.542181: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-11 21:11:48.543302: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:48.543900: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:48.544401: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:48.544962: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:48.545464: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:48.545933: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-11 21:11:48.553156: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:11:48.553339: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:11:48.554995: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:48.555148: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-11 21:11:48.555643: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:48.556174: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:48.556756: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:48.557295: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:48.557766: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-11 21:11:48.559507: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:48.560121: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:48.560625: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:48.561203: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:48.561712: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:48.562184: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-11 21:11:48.571590: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:48.572218: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:48.572735: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:48.573206: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 21:11:48.573261: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-11 21:11:48.585313: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:11:48.585509: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:11:48.587318: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
2022-12-11 21:11:48.590742: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:48.591442: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:48.591587: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:11:48.591746: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:11:48.591990: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:48.592624: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:48.593265: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:11:48.593536: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
2022-12-11 21:11:48.593822: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-11 21:11:48.602988: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:11:48.603205: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:11:48.605039: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
2022-12-11 21:11:48.607991: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:11:48.608153: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:11:48.609977: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
2022-12-11 21:11:48.627100: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:11:48.627305: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:11:48.629106: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
2022-12-11 21:11:48.642831: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:11:48.643019: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:11:48.644795: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
[HCTR][21:11:49.915][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][21:11:49.915][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][21:11:49.915][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][21:11:49.915][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][21:11:49.915][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][21:11:49.915][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][21:11:49.915][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][21:11:49.915][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:01,  1.53s/it]warmup run: 96it [00:01, 81.73it/s]warmup run: 1it [00:01,  1.55s/it]warmup run: 193it [00:01, 178.13it/s]warmup run: 1it [00:01,  1.50s/it]warmup run: 96it [00:01, 80.93it/s]warmup run: 289it [00:01, 283.10it/s]warmup run: 1it [00:01,  1.53s/it]warmup run: 1it [00:01,  1.49s/it]warmup run: 1it [00:01,  1.55s/it]warmup run: 98it [00:01, 84.83it/s]warmup run: 192it [00:01, 175.58it/s]warmup run: 387it [00:01, 394.61it/s]warmup run: 100it [00:01, 85.32it/s]warmup run: 100it [00:01, 87.08it/s]warmup run: 100it [00:01, 84.02it/s]warmup run: 195it [00:01, 182.47it/s]warmup run: 287it [00:01, 278.82it/s]warmup run: 484it [00:02, 501.78it/s]warmup run: 1it [00:01,  1.49s/it]warmup run: 203it [00:01, 188.08it/s]warmup run: 199it [00:01, 186.74it/s]warmup run: 1it [00:01,  1.47s/it]warmup run: 200it [00:01, 182.34it/s]warmup run: 291it [00:01, 288.14it/s]warmup run: 382it [00:01, 386.11it/s]warmup run: 583it [00:02, 603.91it/s]warmup run: 90it [00:01, 78.47it/s]warmup run: 306it [00:01, 301.22it/s]warmup run: 99it [00:01, 87.21it/s]warmup run: 298it [00:01, 296.16it/s]warmup run: 299it [00:01, 289.50it/s]warmup run: 387it [00:01, 397.47it/s]warmup run: 474it [00:02, 485.01it/s]warmup run: 682it [00:02, 692.53it/s]warmup run: 178it [00:01, 167.39it/s]warmup run: 409it [00:01, 418.32it/s]warmup run: 199it [00:01, 189.45it/s]warmup run: 397it [00:01, 408.85it/s]warmup run: 396it [00:01, 397.96it/s]warmup run: 484it [00:02, 504.52it/s]warmup run: 573it [00:02, 589.99it/s]warmup run: 779it [00:02, 759.79it/s]warmup run: 266it [00:01, 264.63it/s]warmup run: 511it [00:02, 530.81it/s]warmup run: 299it [00:01, 301.07it/s]warmup run: 496it [00:01, 518.28it/s]warmup run: 494it [00:02, 505.49it/s]warmup run: 582it [00:02, 604.22it/s]warmup run: 672it [00:02, 681.77it/s]warmup run: 876it [00:02, 812.85it/s]warmup run: 355it [00:01, 366.25it/s]warmup run: 615it [00:02, 636.99it/s]warmup run: 597it [00:02, 622.45it/s]warmup run: 400it [00:01, 417.18it/s]warmup run: 594it [00:02, 608.16it/s]warmup run: 678it [00:02, 686.31it/s]warmup run: 769it [00:02, 752.30it/s]warmup run: 973it [00:02, 853.70it/s]warmup run: 449it [00:01, 474.53it/s]warmup run: 718it [00:02, 727.51it/s]warmup run: 500it [00:01, 527.97it/s]warmup run: 696it [00:02, 707.25it/s]warmup run: 696it [00:02, 702.47it/s]warmup run: 773it [00:02, 750.42it/s]warmup run: 865it [00:02, 806.18it/s]warmup run: 1069it [00:02, 880.23it/s]warmup run: 548it [00:02, 583.55it/s]warmup run: 820it [00:02, 799.96it/s]warmup run: 601it [00:02, 630.53it/s]warmup run: 795it [00:02, 776.57it/s]warmup run: 798it [00:02, 780.81it/s]warmup run: 869it [00:02, 802.86it/s]warmup run: 963it [00:02, 851.24it/s]warmup run: 648it [00:02, 678.83it/s]warmup run: 1165it [00:02, 887.30it/s]warmup run: 921it [00:02, 850.34it/s]warmup run: 703it [00:02, 720.30it/s]warmup run: 897it [00:02, 839.67it/s]warmup run: 900it [00:02, 843.00it/s]warmup run: 964it [00:02, 840.43it/s]warmup run: 1061it [00:02, 885.62it/s]warmup run: 746it [00:02, 753.82it/s]warmup run: 1263it [00:02, 912.60it/s]warmup run: 1021it [00:02, 886.58it/s]warmup run: 805it [00:02, 793.58it/s]warmup run: 996it [00:02, 873.14it/s]warmup run: 1002it [00:02, 890.03it/s]warmup run: 1059it [00:02, 869.65it/s]warmup run: 1164it [00:02, 925.31it/s]warmup run: 844it [00:02, 811.16it/s]warmup run: 1363it [00:02, 936.88it/s]warmup run: 1121it [00:02, 918.00it/s]warmup run: 1098it [00:02, 911.59it/s]warmup run: 904it [00:02, 836.94it/s]warmup run: 1105it [00:02, 928.99it/s]warmup run: 1156it [00:02, 896.93it/s]warmup run: 1269it [00:02, 958.88it/s]warmup run: 942it [00:02, 855.43it/s]warmup run: 1466it [00:03, 963.84it/s]warmup run: 1222it [00:02, 942.43it/s]warmup run: 1202it [00:02, 945.61it/s]warmup run: 1208it [00:02, 957.27it/s]warmup run: 1002it [00:02, 823.63it/s]warmup run: 1252it [00:02, 914.60it/s]warmup run: 1373it [00:02, 982.11it/s]warmup run: 1039it [00:02, 885.58it/s]warmup run: 1567it [00:03, 976.58it/s]warmup run: 1322it [00:02, 958.59it/s]warmup run: 1303it [00:02, 958.28it/s]warmup run: 1310it [00:02, 969.32it/s]warmup run: 1102it [00:02, 870.56it/s]warmup run: 1350it [00:02, 933.67it/s]warmup run: 1477it [00:03, 998.78it/s]warmup run: 1667it [00:03, 955.93it/s]warmup run: 1424it [00:02, 973.62it/s]warmup run: 1135it [00:02, 865.85it/s]warmup run: 1403it [00:02, 962.27it/s]warmup run: 1412it [00:02, 973.47it/s]warmup run: 1203it [00:02, 908.89it/s]warmup run: 1448it [00:03, 945.32it/s]warmup run: 1581it [00:03, 1009.86it/s]warmup run: 1525it [00:03, 984.08it/s]warmup run: 1764it [00:03, 950.59it/s]warmup run: 1514it [00:03, 985.18it/s]warmup run: 1227it [00:02, 820.57it/s]warmup run: 1502it [00:03, 958.86it/s]warmup run: 1304it [00:02, 936.43it/s]warmup run: 1545it [00:03, 950.42it/s]warmup run: 1685it [00:03, 1017.18it/s]warmup run: 1626it [00:03, 985.64it/s]warmup run: 1860it [00:03, 948.14it/s]warmup run: 1615it [00:03, 992.02it/s]warmup run: 1319it [00:02, 847.21it/s]warmup run: 1600it [00:03, 951.76it/s]warmup run: 1406it [00:02, 958.95it/s]warmup run: 1642it [00:03, 951.60it/s]warmup run: 1789it [00:03, 1021.37it/s]warmup run: 1726it [00:03, 986.67it/s]warmup run: 1956it [00:03, 940.53it/s]warmup run: 1413it [00:03, 872.67it/s]warmup run: 1716it [00:03, 993.30it/s]warmup run: 1701it [00:03, 967.13it/s]warmup run: 1509it [00:03, 977.45it/s]warmup run: 1739it [00:03, 954.03it/s]warmup run: 1892it [00:03, 1018.80it/s]warmup run: 1827it [00:03, 992.25it/s]warmup run: 2062it [00:03, 973.62it/s]warmup run: 1817it [00:03, 997.23it/s]warmup run: 1507it [00:03, 890.40it/s]warmup run: 1803it [00:03, 982.13it/s]warmup run: 1611it [00:03, 989.44it/s]warmup run: 1995it [00:03, 1017.87it/s]warmup run: 1836it [00:03, 935.28it/s]warmup run: 1929it [00:03, 999.37it/s]warmup run: 2174it [00:03, 1015.74it/s]warmup run: 1919it [00:03, 1001.31it/s]warmup run: 1603it [00:03, 908.40it/s]warmup run: 1905it [00:03, 991.97it/s]warmup run: 1714it [00:03, 999.80it/s]warmup run: 2114it [00:03, 1068.53it/s]warmup run: 1931it [00:03, 892.62it/s]warmup run: 2035it [00:03, 1016.11it/s]warmup run: 2284it [00:03, 1039.73it/s]warmup run: 2023it [00:03, 1012.18it/s]warmup run: 1698it [00:03, 919.70it/s]warmup run: 2009it [00:03, 1003.90it/s]warmup run: 1817it [00:03, 1005.92it/s]warmup run: 2236it [00:03, 1111.02it/s]warmup run: 2035it [00:03, 932.36it/s]warmup run: 2155it [00:03, 1070.52it/s]warmup run: 2396it [00:03, 1061.71it/s]warmup run: 1793it [00:03, 928.06it/s]warmup run: 2143it [00:03, 1065.95it/s]warmup run: 2131it [00:03, 1067.00it/s]warmup run: 1919it [00:03, 1008.74it/s]warmup run: 2358it [00:03, 1140.96it/s]warmup run: 2157it [00:03, 1014.93it/s]warmup run: 2276it [00:03, 1109.86it/s]warmup run: 2508it [00:04, 1076.54it/s]warmup run: 2263it [00:03, 1104.32it/s]warmup run: 1889it [00:03, 935.15it/s]warmup run: 2253it [00:03, 1111.44it/s]warmup run: 2025it [00:03, 1021.55it/s]warmup run: 2480it [00:03, 1162.51it/s]warmup run: 2280it [00:03, 1075.88it/s]warmup run: 2396it [00:03, 1136.37it/s]warmup run: 2618it [00:04, 1080.70it/s]warmup run: 1985it [00:03, 942.06it/s]warmup run: 2384it [00:03, 1133.88it/s]warmup run: 2375it [00:03, 1142.90it/s]warmup run: 2146it [00:03, 1076.74it/s]warmup run: 2602it [00:04, 1176.82it/s]warmup run: 2402it [00:03, 1118.13it/s]warmup run: 2516it [00:03, 1154.20it/s]warmup run: 2729it [00:04, 1087.95it/s]warmup run: 2505it [00:03, 1156.29it/s]warmup run: 2100it [00:03, 1002.37it/s]warmup run: 2498it [00:03, 1166.12it/s]warmup run: 2267it [00:03, 1115.28it/s]warmup run: 2724it [00:04, 1187.06it/s]warmup run: 2524it [00:04, 1148.11it/s]warmup run: 2637it [00:04, 1168.83it/s]warmup run: 2839it [00:04, 1090.52it/s]warmup run: 2219it [00:03, 1058.10it/s]warmup run: 2626it [00:04, 1171.81it/s]warmup run: 2620it [00:04, 1181.35it/s]warmup run: 2388it [00:03, 1142.67it/s]warmup run: 2844it [00:04, 1189.96it/s]warmup run: 2647it [00:04, 1170.11it/s]warmup run: 2757it [00:04, 1177.27it/s]warmup run: 2949it [00:04, 1088.82it/s]warmup run: 2747it [00:04, 1181.61it/s]warmup run: 2339it [00:03, 1097.73it/s]warmup run: 2742it [00:04, 1191.08it/s]warmup run: 2509it [00:03, 1162.12it/s]warmup run: 2966it [00:04, 1197.50it/s]warmup run: 3000it [00:04, 663.36it/s] warmup run: 2768it [00:04, 1181.87it/s]warmup run: 3000it [00:04, 682.36it/s] warmup run: 2877it [00:04, 1181.42it/s]warmup run: 2453it [00:04, 1109.48it/s]warmup run: 2866it [00:04, 1179.38it/s]warmup run: 2862it [00:04, 1192.04it/s]warmup run: 2629it [00:04, 1172.34it/s]warmup run: 2890it [00:04, 1190.73it/s]warmup run: 2998it [00:04, 1187.35it/s]warmup run: 3000it [00:04, 689.51it/s] warmup run: 2568it [00:04, 1119.22it/s]warmup run: 2985it [00:04, 1178.87it/s]warmup run: 2984it [00:04, 1198.12it/s]warmup run: 2750it [00:04, 1180.71it/s]warmup run: 3000it [00:04, 683.39it/s] warmup run: 3000it [00:04, 690.72it/s] warmup run: 3000it [00:04, 675.07it/s] warmup run: 2688it [00:04, 1141.03it/s]warmup run: 2869it [00:04, 1181.81it/s]warmup run: 2806it [00:04, 1150.05it/s]warmup run: 2990it [00:04, 1189.11it/s]warmup run: 3000it [00:04, 693.19it/s] warmup run: 2923it [00:04, 1153.51it/s]warmup run: 3000it [00:04, 663.57it/s] 
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]




warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   5%|         | 157/3000 [00:00<00:01, 1569.59it/s]warmup should be done:   5%|         | 158/3000 [00:00<00:01, 1576.63it/s]warmup should be done:   5%|         | 153/3000 [00:00<00:01, 1521.38it/s]warmup should be done:   5%|         | 162/3000 [00:00<00:01, 1614.31it/s]warmup should be done:   5%|         | 162/3000 [00:00<00:01, 1611.29it/s]warmup should be done:   5%|         | 164/3000 [00:00<00:01, 1631.57it/s]warmup should be done:   6%|         | 165/3000 [00:00<00:01, 1640.19it/s]warmup should be done:   5%|         | 162/3000 [00:00<00:01, 1611.54it/s]warmup should be done:  11%|         | 320/3000 [00:00<00:01, 1601.66it/s]warmup should be done:  10%|         | 313/3000 [00:00<00:01, 1565.53it/s]warmup should be done:  11%|         | 323/3000 [00:00<00:01, 1616.49it/s]warmup should be done:  11%|         | 331/3000 [00:00<00:01, 1650.34it/s]warmup should be done:  11%|         | 326/3000 [00:00<00:01, 1625.34it/s]warmup should be done:  11%|         | 326/3000 [00:00<00:01, 1619.67it/s]warmup should be done:  11%|         | 328/3000 [00:00<00:01, 1623.72it/s]warmup should be done:  11%|         | 324/3000 [00:00<00:01, 1573.94it/s]warmup should be done:  16%|        | 483/3000 [00:00<00:01, 1613.62it/s]warmup should be done:  16%|        | 487/3000 [00:00<00:01, 1624.39it/s]warmup should be done:  16%|        | 472/3000 [00:00<00:01, 1573.20it/s]warmup should be done:  17%|        | 497/3000 [00:00<00:01, 1648.45it/s]warmup should be done:  16%|        | 491/3000 [00:00<00:01, 1618.57it/s]warmup should be done:  16%|        | 488/3000 [00:00<00:01, 1601.19it/s]warmup should be done:  16%|        | 489/3000 [00:00<00:01, 1584.86it/s]warmup should be done:  16%|        | 482/3000 [00:00<00:01, 1545.98it/s]warmup should be done:  22%|       | 645/3000 [00:00<00:01, 1615.44it/s]warmup should be done:  21%|        | 630/3000 [00:00<00:01, 1573.24it/s]warmup should be done:  22%|       | 662/3000 [00:00<00:01, 1647.22it/s]warmup should be done:  22%|       | 650/3000 [00:00<00:01, 1620.20it/s]warmup should be done:  22%|       | 656/3000 [00:00<00:01, 1627.80it/s]warmup should be done:  22%|       | 651/3000 [00:00<00:01, 1610.31it/s]warmup should be done:  22%|       | 648/3000 [00:00<00:01, 1582.56it/s]warmup should be done:  21%|       | 644/3000 [00:00<00:01, 1573.95it/s]warmup should be done:  27%|       | 809/3000 [00:00<00:01, 1622.58it/s]warmup should be done:  26%|       | 788/3000 [00:00<00:01, 1574.60it/s]warmup should be done:  28%|       | 827/3000 [00:00<00:01, 1646.02it/s]warmup should be done:  27%|       | 814/3000 [00:00<00:01, 1615.00it/s]warmup should be done:  27%|       | 813/3000 [00:00<00:01, 1606.87it/s]warmup should be done:  27%|       | 819/3000 [00:00<00:01, 1613.49it/s]warmup should be done:  27%|       | 811/3000 [00:00<00:01, 1598.25it/s]warmup should be done:  27%|       | 805/3000 [00:00<00:01, 1585.96it/s]warmup should be done:  32%|      | 973/3000 [00:00<00:01, 1627.94it/s]warmup should be done:  32%|      | 946/3000 [00:00<00:01, 1572.02it/s]warmup should be done:  33%|      | 992/3000 [00:00<00:01, 1644.19it/s]warmup should be done:  33%|      | 976/3000 [00:00<00:01, 1616.61it/s]warmup should be done:  33%|      | 984/3000 [00:00<00:01, 1625.45it/s]warmup should be done:  32%|      | 974/3000 [00:00<00:01, 1593.75it/s]warmup should be done:  32%|      | 973/3000 [00:00<00:01, 1605.42it/s]warmup should be done:  32%|      | 967/3000 [00:00<00:01, 1596.96it/s]warmup should be done:  38%|      | 1136/3000 [00:00<00:01, 1626.88it/s]warmup should be done:  37%|      | 1104/3000 [00:00<00:01, 1572.43it/s]warmup should be done:  39%|      | 1157/3000 [00:00<00:01, 1637.34it/s]warmup should be done:  38%|      | 1138/3000 [00:00<00:01, 1612.14it/s]warmup should be done:  38%|      | 1148/3000 [00:00<00:01, 1628.70it/s]warmup should be done:  38%|      | 1135/3000 [00:00<00:01, 1608.99it/s]warmup should be done:  38%|      | 1136/3000 [00:00<00:01, 1599.57it/s]warmup should be done:  38%|      | 1130/3000 [00:00<00:01, 1604.90it/s]warmup should be done:  43%|     | 1300/3000 [00:00<00:01, 1629.29it/s]warmup should be done:  42%|     | 1264/3000 [00:00<00:01, 1578.00it/s]warmup should be done:  44%|     | 1321/3000 [00:00<00:01, 1636.07it/s]warmup should be done:  43%|     | 1300/3000 [00:00<00:01, 1612.13it/s]warmup should be done:  44%|     | 1313/3000 [00:00<00:01, 1633.51it/s]warmup should be done:  43%|     | 1298/3000 [00:00<00:01, 1614.21it/s]warmup should be done:  43%|     | 1299/3000 [00:00<00:01, 1606.15it/s]warmup should be done:  43%|     | 1294/3000 [00:00<00:01, 1613.26it/s]warmup should be done:  49%|     | 1464/3000 [00:00<00:00, 1629.88it/s]warmup should be done:  47%|     | 1423/3000 [00:00<00:00, 1581.63it/s]warmup should be done:  50%|     | 1485/3000 [00:00<00:00, 1636.18it/s]warmup should be done:  49%|     | 1462/3000 [00:00<00:00, 1613.49it/s]warmup should be done:  49%|     | 1478/3000 [00:00<00:00, 1636.15it/s]warmup should be done:  49%|     | 1461/3000 [00:00<00:00, 1618.12it/s]warmup should be done:  49%|     | 1464/3000 [00:00<00:00, 1616.74it/s]warmup should be done:  49%|     | 1456/3000 [00:00<00:00, 1565.87it/s]warmup should be done:  54%|    | 1628/3000 [00:01<00:00, 1631.19it/s]warmup should be done:  53%|    | 1582/3000 [00:01<00:00, 1579.79it/s]warmup should be done:  55%|    | 1649/3000 [00:01<00:00, 1635.77it/s]warmup should be done:  54%|    | 1624/3000 [00:01<00:00, 1614.05it/s]warmup should be done:  55%|    | 1643/3000 [00:01<00:00, 1639.97it/s]warmup should be done:  54%|    | 1623/3000 [00:01<00:00, 1617.04it/s]warmup should be done:  54%|    | 1626/3000 [00:01<00:00, 1613.85it/s]warmup should be done:  54%|    | 1613/3000 [00:01<00:00, 1483.14it/s]warmup should be done:  60%|    | 1793/3000 [00:01<00:00, 1636.50it/s]warmup should be done:  60%|    | 1813/3000 [00:01<00:00, 1635.13it/s]warmup should be done:  58%|    | 1740/3000 [00:01<00:00, 1570.79it/s]warmup should be done:  60%|    | 1787/3000 [00:01<00:00, 1616.19it/s]warmup should be done:  60%|    | 1809/3000 [00:01<00:00, 1644.29it/s]warmup should be done:  60%|    | 1785/3000 [00:01<00:00, 1617.76it/s]warmup should be done:  60%|    | 1789/3000 [00:01<00:00, 1616.65it/s]warmup should be done:  59%|    | 1766/3000 [00:01<00:00, 1495.11it/s]warmup should be done:  65%|   | 1957/3000 [00:01<00:00, 1634.14it/s]warmup should be done:  66%|   | 1977/3000 [00:01<00:00, 1634.02it/s]warmup should be done:  63%|   | 1898/3000 [00:01<00:00, 1569.86it/s]warmup should be done:  65%|   | 1949/3000 [00:01<00:00, 1615.63it/s]warmup should be done:  66%|   | 1975/3000 [00:01<00:00, 1647.87it/s]warmup should be done:  65%|   | 1953/3000 [00:01<00:00, 1623.00it/s]warmup should be done:  65%|   | 1947/3000 [00:01<00:00, 1604.81it/s]warmup should be done:  64%|   | 1928/3000 [00:01<00:00, 1530.73it/s]warmup should be done:  71%|   | 2121/3000 [00:01<00:00, 1629.36it/s]warmup should be done:  68%|   | 2055/3000 [00:01<00:00, 1568.18it/s]warmup should be done:  71%|  | 2141/3000 [00:01<00:00, 1626.87it/s]warmup should be done:  71%|  | 2141/3000 [00:01<00:00, 1649.85it/s]warmup should be done:  70%|   | 2111/3000 [00:01<00:00, 1613.84it/s]warmup should be done:  70%|   | 2108/3000 [00:01<00:00, 1602.56it/s]warmup should be done:  71%|   | 2116/3000 [00:01<00:00, 1614.45it/s]warmup should be done:  70%|   | 2091/3000 [00:01<00:00, 1558.76it/s]warmup should be done:  76%|  | 2284/3000 [00:01<00:00, 1628.96it/s]warmup should be done:  74%|  | 2212/3000 [00:01<00:00, 1568.04it/s]warmup should be done:  77%|  | 2307/3000 [00:01<00:00, 1650.41it/s]warmup should be done:  76%|  | 2273/3000 [00:01<00:00, 1613.78it/s]warmup should be done:  77%|  | 2304/3000 [00:01<00:00, 1621.40it/s]warmup should be done:  76%|  | 2269/3000 [00:01<00:00, 1601.35it/s]warmup should be done:  76%|  | 2278/3000 [00:01<00:00, 1613.98it/s]warmup should be done:  75%|  | 2254/3000 [00:01<00:00, 1577.71it/s]warmup should be done:  82%| | 2447/3000 [00:01<00:00, 1625.65it/s]warmup should be done:  79%|  | 2369/3000 [00:01<00:00, 1563.62it/s]warmup should be done:  81%|  | 2435/3000 [00:01<00:00, 1612.69it/s]warmup should be done:  82%| | 2473/3000 [00:01<00:00, 1641.57it/s]warmup should be done:  82%| | 2467/3000 [00:01<00:00, 1614.46it/s]warmup should be done:  81%|  | 2430/3000 [00:01<00:00, 1600.42it/s]warmup should be done:  81%| | 2440/3000 [00:01<00:00, 1612.42it/s]warmup should be done:  81%|  | 2416/3000 [00:01<00:00, 1589.09it/s]warmup should be done:  87%| | 2610/3000 [00:01<00:00, 1626.78it/s]warmup should be done:  84%| | 2526/3000 [00:01<00:00, 1563.31it/s]warmup should be done:  87%| | 2597/3000 [00:01<00:00, 1614.79it/s]warmup should be done:  88%| | 2639/3000 [00:01<00:00, 1644.57it/s]warmup should be done:  88%| | 2629/3000 [00:01<00:00, 1612.93it/s]warmup should be done:  87%| | 2602/3000 [00:01<00:00, 1612.60it/s]warmup should be done:  86%| | 2591/3000 [00:01<00:00, 1599.64it/s]warmup should be done:  86%| | 2576/3000 [00:01<00:00, 1591.30it/s]warmup should be done:  92%|| 2773/3000 [00:01<00:00, 1627.25it/s]warmup should be done:  89%| | 2683/3000 [00:01<00:00, 1563.69it/s]warmup should be done:  92%|| 2759/3000 [00:01<00:00, 1614.85it/s]warmup should be done:  93%|| 2804/3000 [00:01<00:00, 1645.88it/s]warmup should be done:  93%|| 2791/3000 [00:01<00:00, 1611.44it/s]warmup should be done:  92%|| 2766/3000 [00:01<00:00, 1619.35it/s]warmup should be done:  92%|| 2751/3000 [00:01<00:00, 1586.52it/s]warmup should be done:  91%|| 2739/3000 [00:01<00:00, 1601.06it/s]warmup should be done:  98%|| 2938/3000 [00:01<00:00, 1631.79it/s]warmup should be done:  95%|| 2841/3000 [00:01<00:00, 1566.66it/s]warmup should be done:  97%|| 2923/3000 [00:01<00:00, 1621.16it/s]warmup should be done:  99%|| 2970/3000 [00:01<00:00, 1649.97it/s]warmup should be done:  98%|| 2954/3000 [00:01<00:00, 1616.58it/s]warmup should be done:  98%|| 2932/3000 [00:01<00:00, 1629.99it/s]warmup should be done:  97%|| 2910/3000 [00:01<00:00, 1582.25it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1639.12it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1628.84it/s]warmup should be done:  97%|| 2900/3000 [00:01<00:00, 1599.83it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1626.58it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1616.07it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1615.06it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1599.81it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1575.42it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1571.55it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1569.96it/s]




warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]


warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|         | 166/3000 [00:00<00:01, 1658.82it/s]warmup should be done:   6%|         | 168/3000 [00:00<00:01, 1677.61it/s]warmup should be done:   6%|         | 167/3000 [00:00<00:01, 1666.14it/s]warmup should be done:   6%|         | 168/3000 [00:00<00:01, 1674.35it/s]warmup should be done:   5%|         | 159/3000 [00:00<00:01, 1583.43it/s]warmup should be done:   6%|         | 168/3000 [00:00<00:01, 1671.05it/s]warmup should be done:   6%|         | 167/3000 [00:00<00:01, 1660.44it/s]warmup should be done:   5%|         | 164/3000 [00:00<00:01, 1630.85it/s]warmup should be done:  11%|         | 336/3000 [00:00<00:01, 1678.86it/s]warmup should be done:  11%|         | 333/3000 [00:00<00:01, 1663.87it/s]warmup should be done:  11%|         | 337/3000 [00:00<00:01, 1680.54it/s]warmup should be done:  11%|         | 336/3000 [00:00<00:01, 1673.10it/s]warmup should be done:  11%|         | 329/3000 [00:00<00:01, 1638.50it/s]warmup should be done:  11%|         | 334/3000 [00:00<00:01, 1657.11it/s]warmup should be done:  11%|         | 334/3000 [00:00<00:01, 1653.41it/s]warmup should be done:  11%|         | 318/3000 [00:00<00:01, 1573.74it/s]warmup should be done:  17%|        | 501/3000 [00:00<00:01, 1668.77it/s]warmup should be done:  17%|        | 506/3000 [00:00<00:01, 1684.23it/s]warmup should be done:  17%|        | 504/3000 [00:00<00:01, 1674.81it/s]warmup should be done:  17%|        | 507/3000 [00:00<00:01, 1686.70it/s]warmup should be done:  16%|        | 494/3000 [00:00<00:01, 1642.87it/s]warmup should be done:  17%|        | 500/3000 [00:00<00:01, 1657.63it/s]warmup should be done:  16%|        | 478/3000 [00:00<00:01, 1583.68it/s]warmup should be done:  17%|        | 500/3000 [00:00<00:01, 1643.74it/s]warmup should be done:  22%|       | 669/3000 [00:00<00:01, 1673.07it/s]warmup should be done:  23%|       | 676/3000 [00:00<00:01, 1686.88it/s]warmup should be done:  23%|       | 678/3000 [00:00<00:01, 1694.16it/s]warmup should be done:  22%|       | 659/3000 [00:00<00:01, 1643.94it/s]warmup should be done:  22%|       | 673/3000 [00:00<00:01, 1678.15it/s]warmup should be done:  22%|       | 667/3000 [00:00<00:01, 1660.54it/s]warmup should be done:  21%|       | 641/3000 [00:00<00:01, 1600.35it/s]warmup should be done:  22%|       | 665/3000 [00:00<00:01, 1641.92it/s]warmup should be done:  28%|       | 837/3000 [00:00<00:01, 1674.48it/s]warmup should be done:  28%|       | 845/3000 [00:00<00:01, 1686.27it/s]warmup should be done:  28%|       | 842/3000 [00:00<00:01, 1680.01it/s]warmup should be done:  27%|       | 824/3000 [00:00<00:01, 1641.95it/s]warmup should be done:  28%|       | 848/3000 [00:00<00:01, 1691.07it/s]warmup should be done:  27%|       | 803/3000 [00:00<00:01, 1607.10it/s]warmup should be done:  28%|       | 834/3000 [00:00<00:01, 1657.15it/s]warmup should be done:  28%|       | 830/3000 [00:00<00:01, 1639.51it/s]warmup should be done:  34%|      | 1005/3000 [00:00<00:01, 1671.79it/s]warmup should be done:  34%|      | 1014/3000 [00:00<00:01, 1686.81it/s]warmup should be done:  34%|      | 1011/3000 [00:00<00:01, 1679.35it/s]warmup should be done:  32%|      | 965/3000 [00:00<00:01, 1609.76it/s]warmup should be done:  33%|      | 989/3000 [00:00<00:01, 1636.61it/s]warmup should be done:  33%|      | 1000/3000 [00:00<00:01, 1653.75it/s]warmup should be done:  34%|      | 1018/3000 [00:00<00:01, 1677.54it/s]warmup should be done:  33%|      | 994/3000 [00:00<00:01, 1636.14it/s]warmup should be done:  39%|      | 1183/3000 [00:00<00:01, 1687.12it/s]warmup should be done:  39%|      | 1173/3000 [00:00<00:01, 1668.67it/s]warmup should be done:  39%|      | 1179/3000 [00:00<00:01, 1677.50it/s]warmup should be done:  38%|      | 1128/3000 [00:00<00:01, 1614.45it/s]warmup should be done:  38%|      | 1153/3000 [00:00<00:01, 1632.72it/s]warmup should be done:  39%|      | 1166/3000 [00:00<00:01, 1651.90it/s]warmup should be done:  40%|      | 1188/3000 [00:00<00:01, 1682.13it/s]warmup should be done:  39%|      | 1158/3000 [00:00<00:01, 1635.88it/s]warmup should be done:  45%|     | 1352/3000 [00:00<00:00, 1687.71it/s]warmup should be done:  45%|     | 1348/3000 [00:00<00:00, 1679.69it/s]warmup should be done:  45%|     | 1340/3000 [00:00<00:00, 1664.59it/s]warmup should be done:  44%|     | 1317/3000 [00:00<00:01, 1631.98it/s]warmup should be done:  44%|     | 1332/3000 [00:00<00:01, 1648.74it/s]warmup should be done:  45%|     | 1359/3000 [00:00<00:00, 1690.23it/s]warmup should be done:  43%|     | 1290/3000 [00:00<00:01, 1599.41it/s]warmup should be done:  44%|     | 1322/3000 [00:00<00:01, 1631.90it/s]warmup should be done:  51%|     | 1521/3000 [00:00<00:00, 1687.32it/s]warmup should be done:  51%|     | 1517/3000 [00:00<00:00, 1681.22it/s]warmup should be done:  49%|     | 1483/3000 [00:00<00:00, 1640.34it/s]warmup should be done:  50%|     | 1507/3000 [00:00<00:00, 1657.71it/s]warmup should be done:  50%|     | 1497/3000 [00:00<00:00, 1648.26it/s]warmup should be done:  51%|     | 1530/3000 [00:00<00:00, 1694.56it/s]warmup should be done:  48%|     | 1455/3000 [00:00<00:00, 1614.34it/s]warmup should be done:  50%|     | 1486/3000 [00:00<00:00, 1633.53it/s]warmup should be done:  56%|    | 1691/3000 [00:01<00:00, 1689.55it/s]warmup should be done:  56%|    | 1686/3000 [00:01<00:00, 1682.08it/s]warmup should be done:  55%|    | 1649/3000 [00:01<00:00, 1645.29it/s]warmup should be done:  56%|    | 1676/3000 [00:01<00:00, 1664.87it/s]warmup should be done:  56%|    | 1665/3000 [00:01<00:00, 1655.73it/s]warmup should be done:  57%|    | 1702/3000 [00:01<00:00, 1699.41it/s]warmup should be done:  54%|    | 1618/3000 [00:01<00:00, 1618.71it/s]warmup should be done:  55%|    | 1651/3000 [00:01<00:00, 1635.76it/s]warmup should be done:  62%|   | 1861/3000 [00:01<00:00, 1691.33it/s]warmup should be done:  62%|   | 1855/3000 [00:01<00:00, 1683.13it/s]warmup should be done:  60%|    | 1815/3000 [00:01<00:00, 1646.91it/s]warmup should be done:  62%|   | 1845/3000 [00:01<00:00, 1670.99it/s]warmup should be done:  61%|    | 1832/3000 [00:01<00:00, 1658.07it/s]warmup should be done:  62%|   | 1874/3000 [00:01<00:00, 1704.04it/s]warmup should be done:  59%|    | 1781/3000 [00:01<00:00, 1619.44it/s]warmup should be done:  61%|    | 1816/3000 [00:01<00:00, 1637.66it/s]warmup should be done:  68%|   | 2031/3000 [00:01<00:00, 1690.03it/s]warmup should be done:  67%|   | 2024/3000 [00:01<00:00, 1682.39it/s]warmup should be done:  66%|   | 1980/3000 [00:01<00:00, 1645.30it/s]warmup should be done:  67%|   | 2014/3000 [00:01<00:00, 1674.14it/s]warmup should be done:  67%|   | 1998/3000 [00:01<00:00, 1656.52it/s]warmup should be done:  68%|   | 2046/3000 [00:01<00:00, 1707.03it/s]warmup should be done:  66%|   | 1980/3000 [00:01<00:00, 1636.40it/s]warmup should be done:  65%|   | 1943/3000 [00:01<00:00, 1607.52it/s]warmup should be done:  73%|  | 2201/3000 [00:01<00:00, 1687.61it/s]warmup should be done:  73%|  | 2193/3000 [00:01<00:00, 1679.67it/s]warmup should be done:  72%|  | 2145/3000 [00:01<00:00, 1646.23it/s]warmup should be done:  73%|  | 2182/3000 [00:01<00:00, 1674.90it/s]warmup should be done:  74%|  | 2217/3000 [00:01<00:00, 1707.59it/s]warmup should be done:  72%|  | 2165/3000 [00:01<00:00, 1657.86it/s]warmup should be done:  71%|  | 2144/3000 [00:01<00:00, 1635.92it/s]warmup should be done:  70%|   | 2106/3000 [00:01<00:00, 1613.96it/s]warmup should be done:  79%|  | 2370/3000 [00:01<00:00, 1687.99it/s]warmup should be done:  79%|  | 2361/3000 [00:01<00:00, 1677.99it/s]warmup should be done:  77%|  | 2311/3000 [00:01<00:00, 1647.84it/s]warmup should be done:  78%|  | 2351/3000 [00:01<00:00, 1676.86it/s]warmup should be done:  80%|  | 2388/3000 [00:01<00:00, 1707.56it/s]warmup should be done:  78%|  | 2333/3000 [00:01<00:00, 1661.50it/s]warmup should be done:  77%|  | 2310/3000 [00:01<00:00, 1641.51it/s]warmup should be done:  76%|  | 2270/3000 [00:01<00:00, 1618.75it/s]warmup should be done:  85%| | 2540/3000 [00:01<00:00, 1689.85it/s]warmup should be done:  84%| | 2530/3000 [00:01<00:00, 1678.98it/s]warmup should be done:  83%| | 2476/3000 [00:01<00:00, 1646.77it/s]warmup should be done:  84%| | 2520/3000 [00:01<00:00, 1678.45it/s]warmup should be done:  85%| | 2560/3000 [00:01<00:00, 1708.74it/s]warmup should be done:  83%| | 2501/3000 [00:01<00:00, 1665.41it/s]warmup should be done:  83%| | 2477/3000 [00:01<00:00, 1647.40it/s]warmup should be done:  81%|  | 2432/3000 [00:01<00:00, 1618.15it/s]warmup should be done:  90%| | 2710/3000 [00:01<00:00, 1690.69it/s]warmup should be done:  90%| | 2698/3000 [00:01<00:00, 1676.75it/s]warmup should be done:  90%| | 2689/3000 [00:01<00:00, 1679.89it/s]warmup should be done:  88%| | 2641/3000 [00:01<00:00, 1643.90it/s]warmup should be done:  91%| | 2732/3000 [00:01<00:00, 1710.98it/s]warmup should be done:  89%| | 2668/3000 [00:01<00:00, 1663.39it/s]warmup should be done:  88%| | 2644/3000 [00:01<00:00, 1651.95it/s]warmup should be done:  86%| | 2595/3000 [00:01<00:00, 1618.64it/s]warmup should be done:  96%|| 2880/3000 [00:01<00:00, 1687.05it/s]warmup should be done:  97%|| 2904/3000 [00:01<00:00, 1711.67it/s]warmup should be done:  95%|| 2858/3000 [00:01<00:00, 1680.44it/s]warmup should be done:  96%|| 2866/3000 [00:01<00:00, 1669.69it/s]warmup should be done:  94%|| 2809/3000 [00:01<00:00, 1651.79it/s]warmup should be done:  94%|| 2835/3000 [00:01<00:00, 1658.86it/s]warmup should be done:  94%|| 2812/3000 [00:01<00:00, 1658.05it/s]warmup should be done:  92%|| 2759/3000 [00:01<00:00, 1622.17it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1699.80it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1686.99it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1676.39it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1673.22it/s]warmup should be done:  99%|| 2978/3000 [00:01<00:00, 1661.56it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1657.57it/s]warmup should be done:  99%|| 2980/3000 [00:01<00:00, 1662.62it/s]warmup should be done:  97%|| 2923/3000 [00:01<00:00, 1624.75it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1646.11it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1645.28it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1612.51it/s]
WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f4226c03e80>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f42268900a0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f42268942e0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f4226891190>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f422689f2b0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f42268911f0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f4226c04730>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f422688f1c0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
2022-12-11 21:13:19.839560: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f3d5702d110 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 21:13:19.839632: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 21:13:19.850027: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 21:13:20.425471: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f3d5e834000 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 21:13:20.425530: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 21:13:20.434397: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 21:13:20.817456: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f3d5702d570 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 21:13:20.817526: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 21:13:20.826740: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 21:13:20.889031: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f3d5f028540 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 21:13:20.889100: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 21:13:20.890466: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f3d6282b9f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 21:13:20.890511: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 21:13:20.897754: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 21:13:20.898668: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f3d57031a80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 21:13:20.898719: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 21:13:20.899405: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 21:13:20.907793: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 21:13:20.991154: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f3d5ef92540 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 21:13:20.991215: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 21:13:20.999976: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 21:13:21.017415: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f3d4f030d90 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 21:13:21.017478: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 21:13:21.028452: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 21:13:27.169373: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 21:13:27.370644: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 21:13:27.573121: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 21:13:27.656220: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 21:13:27.777999: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 21:13:27.781124: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 21:13:27.940332: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 21:13:27.987888: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][21:14:32.736][ERROR][RK0][main]: replica 5 reaches 1000, calling init pre replica
[HCTR][21:14:32.736][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][21:14:32.742][ERROR][RK0][main]: coll ps creation done
[HCTR][21:14:32.742][ERROR][RK0][main]: replica 5 waits for coll ps creation barrier
[HCTR][21:14:32.881][ERROR][RK0][tid #139902133966592]: replica 3 reaches 1000, calling init pre replica
[HCTR][21:14:32.881][ERROR][RK0][tid #139902133966592]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][21:14:32.885][ERROR][RK0][tid #139902133966592]: replica 4 reaches 1000, calling init pre replica
[HCTR][21:14:32.885][ERROR][RK0][tid #139902133966592]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][21:14:32.892][ERROR][RK0][tid #139902133966592]: coll ps creation done
[HCTR][21:14:32.892][ERROR][RK0][tid #139902133966592]: replica 3 waits for coll ps creation barrier
[HCTR][21:14:32.892][ERROR][RK0][tid #139902133966592]: coll ps creation done
[HCTR][21:14:32.892][ERROR][RK0][tid #139902133966592]: replica 4 waits for coll ps creation barrier
[HCTR][21:14:32.911][ERROR][RK0][tid #139902268184320]: replica 2 reaches 1000, calling init pre replica
[HCTR][21:14:32.911][ERROR][RK0][tid #139902268184320]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][21:14:32.916][ERROR][RK0][tid #139902268184320]: coll ps creation done
[HCTR][21:14:32.916][ERROR][RK0][tid #139902268184320]: replica 2 waits for coll ps creation barrier
[HCTR][21:14:32.953][ERROR][RK0][tid #139902670837504]: replica 7 reaches 1000, calling init pre replica
[HCTR][21:14:32.953][ERROR][RK0][tid #139902670837504]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][21:14:32.958][ERROR][RK0][tid #139902670837504]: coll ps creation done
[HCTR][21:14:32.958][ERROR][RK0][tid #139902670837504]: replica 7 waits for coll ps creation barrier
[HCTR][21:14:33.050][ERROR][RK0][main]: replica 1 reaches 1000, calling init pre replica
[HCTR][21:14:33.050][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][21:14:33.054][ERROR][RK0][main]: coll ps creation done
[HCTR][21:14:33.054][ERROR][RK0][main]: replica 1 waits for coll ps creation barrier
[HCTR][21:14:33.137][ERROR][RK0][tid #139901706168064]: replica 6 reaches 1000, calling init pre replica
[HCTR][21:14:33.137][ERROR][RK0][tid #139901706168064]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][21:14:33.143][ERROR][RK0][tid #139902133966592]: replica 0 reaches 1000, calling init pre replica
[HCTR][21:14:33.143][ERROR][RK0][tid #139902133966592]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][21:14:33.145][ERROR][RK0][tid #139901706168064]: coll ps creation done
[HCTR][21:14:33.145][ERROR][RK0][tid #139901706168064]: replica 6 waits for coll ps creation barrier
[HCTR][21:14:33.150][ERROR][RK0][tid #139902133966592]: coll ps creation done
[HCTR][21:14:33.150][ERROR][RK0][tid #139902133966592]: replica 0 waits for coll ps creation barrier
[HCTR][21:14:33.150][ERROR][RK0][tid #139902133966592]: replica 0 preparing frequency
[HCTR][21:14:33.995][ERROR][RK0][tid #139902133966592]: replica 0 preparing frequency done
[HCTR][21:14:34.026][ERROR][RK0][tid #139902133966592]: replica 0 calling init per replica
[HCTR][21:14:34.026][ERROR][RK0][tid #139902268184320]: replica 2 calling init per replica
[HCTR][21:14:34.026][ERROR][RK0][main]: replica 1 calling init per replica
[HCTR][21:14:34.026][ERROR][RK0][tid #139901706168064]: replica 6 calling init per replica
[HCTR][21:14:34.026][ERROR][RK0][main]: replica 5 calling init per replica
[HCTR][21:14:34.026][ERROR][RK0][tid #139902133966592]: replica 3 calling init per replica
[HCTR][21:14:34.026][ERROR][RK0][tid #139902133966592]: replica 4 calling init per replica
[HCTR][21:14:34.026][ERROR][RK0][tid #139902670837504]: replica 7 calling init per replica
[HCTR][21:14:34.026][ERROR][RK0][tid #139902133966592]: Calling build_v2
[HCTR][21:14:34.026][ERROR][RK0][tid #139902268184320]: Calling build_v2
[HCTR][21:14:34.026][ERROR][RK0][main]: Calling build_v2
[HCTR][21:14:34.026][ERROR][RK0][tid #139901706168064]: Calling build_v2
[HCTR][21:14:34.026][ERROR][RK0][tid #139902670837504]: Calling build_v2
[HCTR][21:14:34.026][ERROR][RK0][main]: Calling build_v2
[HCTR][21:14:34.026][ERROR][RK0][tid #139902133966592]: Calling build_v2
[HCTR][21:14:34.026][ERROR][RK0][tid #139902133966592]: Calling build_v2
[HCTR][21:14:34.026][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][21:14:34.026][ERROR][RK0][tid #139902133966592]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][21:14:34.026][ERROR][RK0][tid #139902268184320]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][21:14:34.026][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][21:14:34.026][ERROR][RK0][tid #139901706168064]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][21:14:34.026][ERROR][RK0][tid #139902670837504]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][21:14:34.026][ERROR][RK0][tid #139902133966592]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][21:14:34.026][ERROR][RK0][tid #139902133966592]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[[[[[[[[2022-12-11 21:14:342022-12-11 21:14:342022-12-11 21:14:342022-12-11 21:14:342022-12-11 21:14:342022-12-11 21:14:34...2022-12-11 21:14:34.2022-12-11 21:14:34.. 26269 26280 26269. 26269. 26269 26277: : :  26283:  26288: : EEE: E: EE   E E  /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:::/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc::136136136:136:136136] ] ] 136] 136] ] using concurrent impl MPSusing concurrent impl MPSusing concurrent impl MPS] using concurrent impl MPS] using concurrent impl MPSusing concurrent impl MPS


using concurrent impl MPS
using concurrent impl MPS



[2022-12-11 21:14:34. 30779: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-11 21:14:34. 30818: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[:2022-12-11 21:14:34196.]  30825assigning 8 to cpu: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[[2022-12-11 21:14:342022-12-11 21:14:34.. 30877 30882: [: E2022-12-11 21:14:34E . [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 30909/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 21:14:34:: :.178E196 30926] [ ] : v100x8, slow pcie2022-12-11 21:14:34/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccassigning 8 to cpuE
.:
  30970212[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[: ] [2022-12-11 21:14:34:2022-12-11 21:14:34Ebuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 82022-12-11 21:14:34.178. [
. 31023]  31020/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 21:14:34 31050: v100x8, slow pcie: [:.: E[
2022-12-11 21:14:34E2022-12-11 21:14:34178 31064E . [.] :  /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 31120/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 21:14:34 31117v100x8, slow pcieE/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:: :.: 
 :196E178 31184E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc212[]  ] :  :] 2022-12-11 21:14:34assigning 8 to cpu/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccv100x8, slow pcieE/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc178build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8.
:
 :] 
 31280178/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc213[v100x8, slow pcie: [] :v100x8, slow pcie] [2022-12-11 21:14:34
E2022-12-11 21:14:34196
remote time is 8.684212022-12-11 21:14:34. .[] 
. 31375[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 313872022-12-11 21:14:34assigning 8 to cpu 31395: [2022-12-11 21:14:34:: .
: E2022-12-11 21:14:34.196E 31442E . 31464]  :  /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 31488: assigning 8 to cpu/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:: E
:2022-12-11 21:14:34 :196E 212./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc213]  [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc]  31594:] assigning 8 to cpu/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 21:14:34:build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8: 196remote time is 8.68421
:.196
E] 
214 31678]  assigning 8 to cpu] : [[assigning 8 to cpu[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
cpu time is 97.0588E2022-12-11 21:14:342022-12-11 21:14:34
2022-12-11 21:14:34:
 ...212/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 31793 31794 31803] :[: : : build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8212[2022-12-11 21:14:34EEE
] 2022-12-11 21:14:34.   build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8. 31891[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
 31906: 2022-12-11 21:14:34:::: E[.213214212E 2022-12-11 21:14:34 31958] ] ]  /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.: remote time is 8.68421cpu time is 97.0588build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 32013E


:212:  [[212] E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 21:14:342022-12-11 21:14:34] build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8 :..build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc213 32129 32117
:] : [: 213remote time is 8.68421E2022-12-11 21:14:34[E] 
 .2022-12-11 21:14:34 remote time is 8.68421/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 32201[./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
:: 2022-12-11 21:14:34 32220:213E[.: 214]  2022-12-11 21:14:34 32253E] remote time is 8.68421/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.:  cpu time is 97.0588
: 32294E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
213: [ :] E2022-12-11 21:14:34/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc213remote time is 8.68421 .:] 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 32371214remote time is 8.68421:[: ] 
2142022-12-11 21:14:34Ecpu time is 97.0588] . 
[cpu time is 97.0588 32451/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
2022-12-11 21:14:34: :.E214 32492 ] : /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cccpu time is 97.0588E:
 214/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] :cpu time is 97.0588214
] cpu time is 97.0588
[2022-12-11 21:15:52.258726: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-11 21:15:52.298567: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 381.47 MB
[2022-12-11 21:15:52.298646: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 381.47 MB
[2022-12-11 21:15:52.299811: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:74] mapping nid to rank...
[2022-12-11 21:15:52.376188: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:91] counting slots...
[2022-12-11 21:15:52.757160: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:105] Final num slot is 49
[2022-12-11 21:15:52.757244: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:109] counting blocks...
[2022-12-11 21:15:59.921771: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:118] Final num block is 1024
[2022-12-11 21:15:59.921864: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:123] counting freq and density...
[2022-12-11 21:16:01.687535: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:154] averaging freq and density...
[2022-12-11 21:16:01.687664: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:155] 1024
[2022-12-11 21:16:01.690314: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
[2022-12-11 21:16:01.690372: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:354] constructing optimal solver, device=8, stream=1
1024 blocks, 8 devices
[2022-12-11 21:16:01.963425: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:527] Add Var...
[2022-12-11 21:16:01.994046: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:545] Capacity...
[2022-12-11 21:16:01.995480: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:548] Connect CPU...
[2022-12-11 21:16:02. 17024: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:550] Connect Access To Storage...
[2022-12-11 21:16:02.547183: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:554] Time...
[2022-12-11 21:16:15.997883: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:569] Coll Cache init block placement array
[2022-12-11 21:16:16.  5299: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:645] Coll Cache init block placement array done
[2022-12-11 21:16:16.  5919: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:647] Coll Cache model reset done
[2022-12-11 21:16:16. 49486: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-11 21:16:16. 49586: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-11 21:16:16. 49617: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-11 21:16:16. 49645: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-11 21:16:16. 50167: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 21:16:16. 50218: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:16:16. 51098: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:16:16. 51858: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:16:16. 65032: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 4 solved
[2022-12-11 21:16:16. 65117: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 4 initing device 4
[2022-12-11 21:16:16. 65197: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 5 solved
[2022-12-11 21:16:16. 65275: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 5 initing device 5
[2022-12-11 21:16:16. 65290: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 2 solved
[2022-12-11 21:16:16. 65353: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 2 initing device 2
[2022-12-11 21:16:16. 65364: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 6 solved
[2022-12-11 21:16:16. 65424: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc[:2022-12-11 21:16:16205.]  65423worker 0 thread 6 initing device 6: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 7 solved
[2022-12-11 21:16:16. 65504: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 7 initing device 7
[2022-12-11 21:16:16. 65554: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 21:16:16. 65605: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:16:16. 65728: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[[2022-12-11 21:16:162022-12-11 21:16:16.. 65774 65777: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::18151980] ] Building Coll Cache with ... num gpu device is 8eager alloc mem 381.47 MB

[2022-12-11 21:16:16. 65840: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:16:16. 65872: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 21:16:16. 65917: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB[
2022-12-11 21:16:16. 65935: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 21:16:16. 65985: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[[2022-12-11 21:16:162022-12-11 21:16:16.. 66505 66513: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::202202] ] 1 solved3 solved

[[2022-12-11 21:16:162022-12-11 21:16:16.. 66624 66627: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::205205] ] worker 0 thread 1 initing device 1worker 0 thread 3 initing device 3

[2022-12-11 21:16:16. 67112: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-11 21:16:16:.1815 67125] : Building Coll Cache with ... num gpu device is 8E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 21:16:16. 67208[: 2022-12-11 21:16:16E.  67219/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: :E1980 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 381.47 MB:
1980] eager alloc mem 381.47 MB
[2022-12-11 21:16:16. 70093: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:16:16. 70159: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:16:16. 70218: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:16:16. 70431: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:16:16. 70506: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:16:16. 71473: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:16:16. 71987: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:16:16. 74522: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:16:16. 74587: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:16:16. 74706: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:16:16. 74818: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:16:16. 74882: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:16:16. 75393: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:16:16. 75902: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:16:16.128232: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-11 21:16:16.133663: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 21:16:16.133795: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 21:16:16.134648: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.20 MB
[2022-12-11 21:16:16.135252: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:16:16.136253: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:16:16.137359: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 21:16:16.138098: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 21:16:16.138143: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 493.16 MB
[[[[[[[2022-12-11 21:16:162022-12-11 21:16:162022-12-11 21:16:162022-12-11 21:16:162022-12-11 21:16:162022-12-11 21:16:162022-12-11 21:16:16.......155831155831155831155831155831155830155831: : : : : : : EEEEEEE       /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:::::::1980198019801980198019801980] ] ] ] ] ] ] eager alloc mem 1024.00 Byteseager alloc mem 1024.00 Byteseager alloc mem 1024.00 Byteseager alloc mem 1024.00 Byteseager alloc mem 1024.00 Byteseager alloc mem 1024.00 Byteseager alloc mem 1024.00 Bytes






[2022-12-11 21:16:16.178178: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 21:16:16.178264: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 21:16:16.178362: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 21:16:16.178445: E[ 2022-12-11 21:16:16/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.:178442638: ] Eeager release cuda mem 400000000 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 21:16:16.178534: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 21:16:16.178547: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[[2022-12-11 21:16:162022-12-11 21:16:16..178623178639: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 1024eager release cuda mem 400000000

[2022-12-11 21:16:16[.2022-12-11 21:16:16178729.: 178721E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638:] 638eager release cuda mem 400000000] 
eager release cuda mem 1024
[2022-12-11 21:16:16.178779: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 21:16:16.178824: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 21:16:16.178855: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 21:16:16.179052: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.20 MB
[2022-12-11 21:16:16.180556: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.20 MB
[2022-12-11 21:16:16.181340: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.20 MB
[2022-12-11 21:16:16.186063: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.20 MB
[2022-12-11 21:16:16.186749: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.20 MB
[2022-12-11 21:16:16.187285: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.20 MB
[2022-12-11 21:16:16.187792: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 4.20 MB
[2022-12-11 21:16:16.188361: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:16:16.189131: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:16:16.189193: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:16:16.189255: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB[
2022-12-11 21:16:16[.2022-12-11 21:16:16189287.: 189291E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu638:] [1980eager release cuda mem 6256632022-12-11 21:16:16] 
.eager alloc mem 611.00 KB189351[
: 2022-12-11 21:16:16E. 189384/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: :E1980 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 611.00 KB:
1980] eager alloc mem 611.00 KB
[2022-12-11 21:16:16.190079: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:16:16.190153: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:16:16.190218: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:16:16.190310: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[:2022-12-11 21:16:16638.] 190326eager release cuda mem 625663: 
E[ 2022-12-11 21:16:16/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.:190346638: ] Eeager release cuda mem 625663 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:16:16.191331: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 21:16:16.191385: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 21:16:16.192045: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[[2022-12-11 21:16:162022-12-11 21:16:16..192093192097: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::6381980] ] eager release cuda mem 25855eager alloc mem 492.29 MB

[2022-12-11 21:16:16.192160: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 492.77 MB
[2022-12-11 21:16:16.192864: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 21:16:16.193461: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 21:16:16.193503: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 485.81 MB
[2022-12-11 21:16:16.193581: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 21:16:16.193706: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 21:16:16.193825: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980[] 2022-12-11 21:16:16eager alloc mem 25.25 KB.
193848: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 21:16:16.194179: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 21:16:16.194223: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 492.95 MB
[2022-12-11 21:16:16.194306: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 21:16:16.194350: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 490.67 MB
[2022-12-11 21:16:16.194541: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855[
2022-12-11 21:16:16.194565: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] [eager release cuda mem 258552022-12-11 21:16:16
.194589: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 493.16 MB[
2022-12-11 21:16:16.194614: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 490.95 MB
[[[[[[[[2022-12-11 21:16:162022-12-11 21:16:162022-12-11 21:16:162022-12-11 21:16:162022-12-11 21:16:162022-12-11 21:16:162022-12-11 21:16:162022-12-11 21:16:16........321341321341321341321342321341321342321342321342: : : : : : : : EEEEEEEE        /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::::::19261926192619261926192619261926] ] ] ] ] ] ] ] Device 6 init p2p of link 0Device 5 init p2p of link 6Device 1 init p2p of link 7Device 3 init p2p of link 2Device 7 init p2p of link 4Device 4 init p2p of link 5Device 2 init p2p of link 1Device 0 init p2p of link 3







[[[[2022-12-11 21:16:162022-12-11 21:16:16[[[2022-12-11 21:16:162022-12-11 21:16:16..2022-12-11 21:16:162022-12-11 21:16:162022-12-11 21:16:16..321858321860...321865321863: : 321868[321867321871: : EE: 2022-12-11 21:16:16: : EE  E.EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu 321932  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980::/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:] 19801980:E::1980eager alloc mem 611.00 KB] ] 1980 19801980] 
eager alloc mem 611.00 KBeager alloc mem 611.00 KB] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] ] eager alloc mem 611.00 KB

eager alloc mem 611.00 KB:eager alloc mem 611.00 KBeager alloc mem 611.00 KB

1980

] eager alloc mem 611.00 KB
[2022-12-11 21:16:16.322906: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:16:16.322946: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663[
[2022-12-11 21:16:16[2022-12-11 21:16:16[.[2022-12-11 21:16:16[.2022-12-11 21:16:163229672022-12-11 21:16:16.2022-12-11 21:16:16322970.: .322975: .: 322982E322983E322990E:  :  :  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc : : :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638:] :] :] 638eager release cuda mem 625663638eager release cuda mem 625663638eager release cuda mem 625663] 
] 
] 
eager release cuda mem 625663eager release cuda mem 625663eager release cuda mem 625663


[2022-12-11 21:16:16.335460: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 7
[2022-12-11 21:16:16.335576[: 2022-12-11 21:16:16E. 335573/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: :E1980 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 611.00 KB:
1926] Device 6 init p2p of link 5
[2022-12-11 21:16:16.335709: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:16:16.335777: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 2
[2022-12-11 21:16:16.335904: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:16:16.336164: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 0
[2022-12-11 21:16:16.336290: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:16:16.336403: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663[
2022-12-11 21:16:16.336416: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 1
[2022-12-11 21:16:16.336525: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:16:16.336554: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-11 21:16:16:.1980336559] : eager alloc mem 611.00 KBE
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 4
[2022-12-11 21:16:16.336701: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:16:16.336733: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:16:16.336835: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 3
[2022-12-11 21:16:16.336953: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:16:16.336976: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 6
[2022-12-11 21:16:16.337097: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:16:16.337130: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:16:16.337376: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:16:16.337546: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:16:16.337787: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:16:16.337935: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:16:16.362471: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 2
[2022-12-11 21:16:16.362586: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:16:16.362872: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 4
[2022-12-11 21:16:16.362985: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:16:16.363058: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 3
[2022-12-11 21:16:16.363185: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:16:16.363225: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 5
[2022-12-11 21:16:16.363297: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 6
[2022-12-11 21:16:16.363349: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:16:16.363388: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:16:16.363414: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:16:16.363644: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 7
[2022-12-11 21:16:16.363754: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-11 21:16:16[:.2022-12-11 21:16:161980363763.] : 363767eager alloc mem 611.00 KBE: 
 E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu638:] 1926eager release cuda mem 625663] 
Device 2 init p2p of link 0
[2022-12-11 21:16:16.363937: E[ 2022-12-11 21:16:16/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.:3639611926: ] EDevice 0 init p2p of link 1 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:16:16.364008: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:16:16.364075: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:16:16.364158: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:16:16.364224: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:16:16.364611: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:16:16.364769: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:16:16.364882: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:16:16.377789: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 6
[2022-12-11 21:16:16.377900: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:16:16.378533: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 7
[2022-12-11 21:16:16.378647: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:16:16.378683: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-11 21:16:16:.1926378709] : Device 1 init p2p of link 0E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:16:16.378823: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:16:16.378893: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 1
[2022-12-11 21:16:16.379013: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:16:16.379352: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 5
[2022-12-11 21:16:16.379425: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:16:16.379468: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:16:16.379647: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:16:16.379682: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 3
[2022-12-11 21:16:16.379793: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:16:16.379828: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:16:16.379953: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 4
[2022-12-11 21:16:16.380072: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:16:16.380278: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:16:16.380300: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 2
[2022-12-11 21:16:16.380428: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:16:16.380582: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:16:16.380861: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:16:16.381234: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:16:16.394315: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 4399996
[2022-12-11 21:16:16.394767: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 4399996
[2022-12-11 21:16:16.395164: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 4399996
[2022-12-11 21:16:16.395535: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 4399996
[2022-12-11 21:16:16.395833: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 4399996
[2022-12-11 21:16:16.395989: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 4399996
[2022-12-11 21:16:16.396389: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 4399996
[2022-12-11 21:16:16.396826: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 4399996
[2022-12-11 21:16:16.397369: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 999562 / 100000000 nodes ( 1.00 %~1.00 %) | remote 2983365 / 100000000 nodes ( 2.98 %) | cpu 96017073 / 100000000 nodes ( 96.02 %) | 492.95 MB | 0.331776 secs 
[2022-12-11 21:16:16.397585: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 984943 / 100000000 nodes ( 0.98 %~1.00 %) | remote 2997984 / 100000000 nodes ( 3.00 %) | cpu 96017073 / 100000000 nodes ( 96.02 %) | 485.81 MB | 0.331676 secs 
[2022-12-11 21:16:16.397989: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 998218 / 100000000 nodes ( 1.00 %~1.00 %) | remote 2984709 / 100000000 nodes ( 2.98 %) | cpu 96017073 / 100000000 nodes ( 96.02 %) | 492.29 MB | 0.330776 secs 
[2022-12-11 21:16:16.398078: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 994898 / 100000000 nodes ( 0.99 %~1.00 %) | remote 2988029 / 100000000 nodes ( 2.99 %) | cpu 96017073 / 100000000 nodes ( 96.02 %) | 490.67 MB | 0.330879 secs 
[2022-12-11 21:16:16.398481: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 1000000 / 100000000 nodes ( 1.00 %~1.00 %) | remote 2982927 / 100000000 nodes ( 2.98 %) | cpu 96017073 / 100000000 nodes ( 96.02 %) | 493.16 MB | 0.332503 secs 
[2022-12-11 21:16:16.398879: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 999188 / 100000000 nodes ( 1.00 %~1.00 %) | remote 2983739 / 100000000 nodes ( 2.98 %) | cpu 96017073 / 100000000 nodes ( 96.02 %) | 492.77 MB | 0.333114 secs 
[2022-12-11 21:16:16.399296: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 995472 / 100000000 nodes ( 1.00 %~1.00 %) | remote 2987455 / 100000000 nodes ( 2.99 %) | cpu 96017073 / 100000000 nodes ( 96.02 %) | 490.95 MB | 0.333466 secs 
[2022-12-11 21:16:16.399877: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 1000000 / 100000000 nodes ( 1.00 %~1.00 %) | remote 2982927 / 100000000 nodes ( 2.98 %) | cpu 96017073 / 100000000 nodes ( 96.02 %) | 493.16 MB | 0.349674 secs 
[2022-12-11 21:16:16.401603: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 8.80 GB
[2022-12-11 21:16:17.708859: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 9.06 GB
[2022-12-11 21:16:17.709034: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 9.06 GB
[2022-12-11 21:16:17.732295: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 9.06 GB
[2022-12-11 21:16:19. 82039: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 9.33 GB
[2022-12-11 21:16:19. 82343: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 9.33 GB
[2022-12-11 21:16:19. 83626: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 9.33 GB
[2022-12-11 21:16:20.330351: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 9.54 GB
[2022-12-11 21:16:20.330639: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 9.54 GB
[2022-12-11 21:16:20.331048: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 9.54 GB
[2022-12-11 21:16:21.589509: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 9.76 GB
[2022-12-11 21:16:21.590211: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 9.76 GB
[2022-12-11 21:16:21.591568: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 9.76 GB
[2022-12-11 21:16:22.783588: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 10.21 GB
[2022-12-11 21:16:22.784259: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 10.21 GB
[2022-12-11 21:16:22.785901: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 10.21 GB
[2022-12-11 21:16:24.160715: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 10.41 GB
[2022-12-11 21:16:24.160966: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 10.41 GB
[HCTR][21:16:25.607][ERROR][RK0][tid #139902268184320]: replica 2 calling init per replica done, doing barrier
[HCTR][21:16:25.607][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier
[HCTR][21:16:25.607][ERROR][RK0][tid #139902670837504]: replica 7 calling init per replica done, doing barrier
[HCTR][21:16:25.607][ERROR][RK0][tid #139902133966592]: replica 3 calling init per replica done, doing barrier
[HCTR][21:16:25.607][ERROR][RK0][tid #139901706168064]: replica 6 calling init per replica done, doing barrier
[HCTR][21:16:25.607][ERROR][RK0][tid #139902133966592]: replica 4 calling init per replica done, doing barrier
[HCTR][21:16:25.607][ERROR][RK0][main]: replica 1 calling init per replica done, doing barrier
[HCTR][21:16:25.607][ERROR][RK0][tid #139902133966592]: replica 0 calling init per replica done, doing barrier
[HCTR][21:16:25.607][ERROR][RK0][tid #139902133966592]: replica 3 calling init per replica done, doing barrier done
[HCTR][21:16:25.607][ERROR][RK0][main]: replica 1 calling init per replica done, doing barrier done
[HCTR][21:16:25.607][ERROR][RK0][tid #139902670837504]: replica 7 calling init per replica done, doing barrier done
[HCTR][21:16:25.607][ERROR][RK0][tid #139902268184320]: replica 2 calling init per replica done, doing barrier done
[HCTR][21:16:25.607][ERROR][RK0][tid #139902133966592]: init per replica done
[HCTR][21:16:25.607][ERROR][RK0][tid #139901706168064]: replica 6 calling init per replica done, doing barrier done
[HCTR][21:16:25.607][ERROR][RK0][tid #139902133966592]: replica 4 calling init per replica done, doing barrier done
[HCTR][21:16:25.607][ERROR][RK0][tid #139902133966592]: replica 0 calling init per replica done, doing barrier done
[HCTR][21:16:25.607][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier done
[HCTR][21:16:25.607][ERROR][RK0][main]: init per replica done
[HCTR][21:16:25.607][ERROR][RK0][tid #139902670837504]: init per replica done
[HCTR][21:16:25.607][ERROR][RK0][tid #139902268184320]: init per replica done
[HCTR][21:16:25.607][ERROR][RK0][tid #139901706168064]: init per replica done
[HCTR][21:16:25.607][ERROR][RK0][main]: init per replica done
[HCTR][21:16:25.607][ERROR][RK0][tid #139902133966592]: init per replica done
[HCTR][21:16:25.610][ERROR][RK0][tid #139902133966592]: init per replica done
[HCTR][21:16:25.646][ERROR][RK0][main]: 5 allocated 3276800 at 0x7f219c238400
[HCTR][21:16:25.646][ERROR][RK0][tid #139901706168064]: 6 allocated 3276800 at 0x7f2210238400
[HCTR][21:16:25.646][ERROR][RK0][main]: 5 allocated 6553600 at 0x7f219c558400
[HCTR][21:16:25.646][ERROR][RK0][tid #139901706168064]: 6 allocated 6553600 at 0x7f2210558400
[HCTR][21:16:25.646][ERROR][RK0][main]: 5 allocated 3276800 at 0x7f219cb98400
[HCTR][21:16:25.646][ERROR][RK0][tid #139901706168064]: 6 allocated 3276800 at 0x7f2210b98400
[HCTR][21:16:25.646][ERROR][RK0][main]: 5 allocated 6553600 at 0x7f219ceb8400
[HCTR][21:16:25.646][ERROR][RK0][tid #139901706168064]: 6 allocated 6553600 at 0x7f2210eb8400
[HCTR][21:16:25.646][ERROR][RK0][tid #139902670837504]: 7 allocated 3276800 at 0x7f2170238400
[HCTR][21:16:25.646][ERROR][RK0][tid #139902670837504]: 7 allocated 6553600 at 0x7f2170558400
[HCTR][21:16:25.646][ERROR][RK0][tid #139902670837504]: 7 allocated 3276800 at 0x7f2170b98400
[HCTR][21:16:25.646][ERROR][RK0][tid #139902670837504]: 7 allocated 6553600 at 0x7f2170eb8400
[HCTR][21:16:25.646][ERROR][RK0][tid #139901647451904]: 1 allocated 3276800 at 0x7f21b8238400
[HCTR][21:16:25.646][ERROR][RK0][tid #139901647451904]: 1 allocated 6553600 at 0x7f21b8558400
[HCTR][21:16:25.646][ERROR][RK0][tid #139901647451904]: 1 allocated 3276800 at 0x7f21b8b98400
[HCTR][21:16:25.646][ERROR][RK0][tid #139901647451904]: 1 allocated 6553600 at 0x7f21b8eb8400
[HCTR][21:16:25.646][ERROR][RK0][tid #139902268184320]: 2 allocated 3276800 at 0x7f2210238400
[HCTR][21:16:25.646][ERROR][RK0][tid #139902268184320]: 2 allocated 6553600 at 0x7f2210558400
[HCTR][21:16:25.646][ERROR][RK0][tid #139902268184320]: 2 allocated 3276800 at 0x7f2210b98400
[HCTR][21:16:25.646][ERROR][RK0][tid #139902268184320]: 2 allocated 6553600 at 0x7f2210eb8400
[HCTR][21:16:25.646][ERROR][RK0][tid #139902133966592]: 4 allocated 3276800 at 0x7f21fc238400
[HCTR][21:16:25.646][ERROR][RK0][tid #139902133966592]: 4 allocated 6553600 at 0x7f21fc558400
[HCTR][21:16:25.646][ERROR][RK0][tid #139902133966592]: 4 allocated 3276800 at 0x7f21fcb98400
[HCTR][21:16:25.646][ERROR][RK0][tid #139902133966592]: 4 allocated 6553600 at 0x7f21fceb8400
[HCTR][21:16:25.646][ERROR][RK0][main]: 3 allocated 3276800 at 0x7f20f8238400
[HCTR][21:16:25.646][ERROR][RK0][main]: 3 allocated 6553600 at 0x7f20f8558400
[HCTR][21:16:25.646][ERROR][RK0][main]: 3 allocated 3276800 at 0x7f20f8b98400
[HCTR][21:16:25.646][ERROR][RK0][main]: 3 allocated 6553600 at 0x7f20f8eb8400
[HCTR][21:16:25.649][ERROR][RK0][tid #139902133966592]: 0 allocated 3276800 at 0x7f21c4320000
[HCTR][21:16:25.649][ERROR][RK0][tid #139902133966592]: 0 allocated 6553600 at 0x7f21c4640000
[HCTR][21:16:25.649][ERROR][RK0][tid #139902133966592]: 0 allocated 3276800 at 0x7f21c4c80000
[HCTR][21:16:25.649][ERROR][RK0][tid #139902133966592]: 0 allocated 6553600 at 0x7f21c4fa0000
