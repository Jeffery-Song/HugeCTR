2022-12-12 05:16:07.813815: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:07.818117: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:07.823484: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:07.835064: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:07.842760: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:07.854434: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:07.861686: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:07.866266: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:07.920645: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:07.921691: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:07.922356: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:07.923046: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:07.923819: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:07.924907: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:07.925340: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:07.926715: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:07.926818: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:07.928426: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:07.928479: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:07.930133: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:07.930175: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:07.931807: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:07.931919: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:07.933384: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:07.934436: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:07.935395: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:07.936354: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:07.937241: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:07.938106: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:07.939032: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:07.940165: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:07.941316: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:07.943386: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:07.944441: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:07.944650: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:07.946222: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:07.946267: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:07.947852: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:07.947886: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:07.949435: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:07.949528: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:07.951071: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:07.951261: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:07.952633: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:07.952737: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:07.954373: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:07.954440: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:07.956122: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:07.959236: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:07.960480: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 05:16:07.960776: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:07.962147: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:07.962176: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:07.964257: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:07.964365: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:07.965938: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:07.965989: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:07.967575: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:07.967596: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:07.969030: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:07.969178: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:07.970237: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:07.971214: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:07.971392: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:07.972125: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:07.973411: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:07.973736: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:07.975157: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:07.978402: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:07.980144: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:07.980379: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:07.980542: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:07.982594: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:07.983076: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:07.983369: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:07.983523: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:07.984929: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:07.985738: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:07.986109: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:07.986278: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:07.987425: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:07.988495: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:07.988910: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:07.989004: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:07.989942: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:07.991247: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:07.991655: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:07.991698: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:07.992342: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:07.994036: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:07.994347: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:07.994390: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:07.994847: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:07.997107: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:07.997306: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:07.997340: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:07.997594: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.001848: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.010365: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.010443: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.010460: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.010768: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.013327: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.013361: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.013545: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.017891: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.021062: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.049173: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.049499: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.050993: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.051061: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.051480: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.052219: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.054481: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.054733: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.054774: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.055296: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.055708: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.057487: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.059161: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.059291: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.059334: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.060661: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.061273: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.062135: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.063658: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.063819: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.063913: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.064764: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.065368: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.066124: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.068624: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.068767: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.068879: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.069437: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.069945: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.070743: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.073241: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.073370: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.073607: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.074496: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.074936: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.075401: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.077313: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.077451: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.077569: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.078707: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.079234: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.079478: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.081379: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.081476: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.081520: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.082881: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.083468: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.083609: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.085378: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.085440: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.085549: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.086964: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.087792: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.087844: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.089781: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.089830: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.090057: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.092155: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.093213: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.093257: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.093480: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.094914: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.094947: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.095137: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.096728: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.097814: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.097891: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.098012: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.100052: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.100140: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.100320: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.101718: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.102486: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.102671: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.102892: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.104318: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.104404: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.104585: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.107388: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.107396: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.107504: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.107604: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.109305: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.109447: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.109487: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.111972: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.112092: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.112205: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.113552: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.113696: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.113737: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.115982: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.116177: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.116178: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.116647: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 05:16:08.117722: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.118647: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.119238: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.120910: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.121057: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.121160: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.122257: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.123368: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.123777: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.124932: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.125386: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.125491: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.126341: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.127437: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.127502: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.127812: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.129898: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.129972: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.130191: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.131101: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.132164: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.132275: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.132770: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.134416: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.134620: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.134819: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.135755: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.136841: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.136990: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.137302: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.139447: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.140191: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.140298: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.143040: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.143225: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.143736: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.145181: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.145780: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.147287: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.148072: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.148375: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.149206: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.149614: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.149913: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 05:16:08.151071: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.151814: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.152280: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.153015: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.153516: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.155081: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.157029: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.157321: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.158652: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.158691: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.159591: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 05:16:08.160020: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 05:16:08.161495: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.161869: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.163084: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.165922: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.165952: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.166574: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 05:16:08.168404: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.168675: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 05:16:08.168743: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.169162: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.171749: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.172113: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.172645: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.175500: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.176457: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.176719: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.177534: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.177595: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.190794: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.191637: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.192736: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.195180: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.196260: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.197392: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.229735: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.262976: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.272724: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 05:16:08.281594: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.300430: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:08.305879: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:09.371495: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:09.372132: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:09.373148: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:09.373623: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 05:16:09.373678: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 05:16:09.391754: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:09.392403: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:09.393040: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:09.393834: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:09.394698: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:09.395545: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 05:16:09.441751: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:16:09.441957: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:16:09.495790: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-12 05:16:09.575156: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:09.575765: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:09.576299: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:09.576766: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 05:16:09.576827: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 05:16:09.595777: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:09.596419: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:09.596934: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:09.597716: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:09.598657: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:09.599151: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 05:16:09.627516: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:09.628133: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:09.628658: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:09.629368: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 05:16:09.629424: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 05:16:09.647840: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:09.648662: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:09.649391: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:09.649970: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:09.650959: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:09.651440: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 05:16:09.658084: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:09.658774: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:09.659348: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:09.659832: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 05:16:09.659883: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 05:16:09.663693: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:09.664301: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:09.664830: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:09.665662: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 05:16:09.665742: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 05:16:09.670145: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:09.670735: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:09.671280: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:09.671750: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 05:16:09.671808: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 05:16:09.672799: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:09.673619: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:09.674170: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:09.674634: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 05:16:09.674677: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 05:16:09.678120: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:09.678728: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:09.679258: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:09.679862: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:09.680378: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:09.680844: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 05:16:09.681407: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:16:09.681590: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:16:09.683388: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-12 05:16:09.684593: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:09.685219: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:09.685728: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:09.686313: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:09.686837: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:09.687325: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 05:16:09.688615: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:09.689215: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:09.689735: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:09.690207: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 05:16:09.690248: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 05:16:09.690268: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:09.691225: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:09.691768: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:09.692343: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:09.692862: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:09.693022: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:09.693652: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 05:16:09.693978: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:09.694504: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:09.695075: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:09.695610: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:09.696082: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 05:16:09.708826: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:09.709508: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:09.710024: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:09.710601: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:09.711123: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 05:16:09.711614: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 05:16:09.725341: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:16:09.725551: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:16:09.727433: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
2022-12-12 05:16:09.732529: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:16:09.732705: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:16:09.734477: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
2022-12-12 05:16:09.738935: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:16:09.739103: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:16:09.741101: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
2022-12-12 05:16:09.741391: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:16:09.741519: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:16:09.743474: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
2022-12-12 05:16:09.747844: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:16:09.747975: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:16:09.749764: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
2022-12-12 05:16:09.756550: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:16:09.756687: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 05:16:09.758529: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
[HCTR][05:16:10.995][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][05:16:10.996][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][05:16:11.018][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][05:16:11.018][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][05:16:11.018][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][05:16:11.019][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][05:16:11.019][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][05:16:11.019][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:01,  1.54s/it]warmup run: 101it [00:01, 85.61it/s]warmup run: 1it [00:01,  1.53s/it]warmup run: 201it [00:01, 184.59it/s]warmup run: 99it [00:01, 84.28it/s]warmup run: 302it [00:01, 294.86it/s]warmup run: 1it [00:01,  1.53s/it]warmup run: 197it [00:01, 181.50it/s]warmup run: 1it [00:01,  1.48s/it]warmup run: 402it [00:01, 407.97it/s]warmup run: 99it [00:01, 84.40it/s]warmup run: 1it [00:01,  1.52s/it]warmup run: 1it [00:01,  1.49s/it]warmup run: 295it [00:01, 288.54it/s]warmup run: 1it [00:01,  1.51s/it]warmup run: 93it [00:01, 81.65it/s]warmup run: 503it [00:02, 519.47it/s]warmup run: 199it [00:01, 184.06it/s]warmup run: 99it [00:01, 84.49it/s]warmup run: 98it [00:01, 85.44it/s]warmup run: 393it [00:01, 399.74it/s]warmup run: 101it [00:01, 87.05it/s]warmup run: 184it [00:01, 174.18it/s]warmup run: 604it [00:02, 621.67it/s]warmup run: 292it [00:01, 284.18it/s]warmup run: 197it [00:01, 181.86it/s]warmup run: 1it [00:01,  1.50s/it]warmup run: 196it [00:01, 184.57it/s]warmup run: 490it [00:02, 505.87it/s]warmup run: 202it [00:01, 188.42it/s]warmup run: 275it [00:01, 275.08it/s]warmup run: 705it [00:02, 710.27it/s]warmup run: 390it [00:01, 396.18it/s]warmup run: 293it [00:01, 286.56it/s]warmup run: 99it [00:01, 85.94it/s]warmup run: 296it [00:01, 295.87it/s]warmup run: 586it [00:02, 601.20it/s]warmup run: 303it [00:01, 299.63it/s]warmup run: 365it [00:01, 377.44it/s]warmup run: 807it [00:02, 785.46it/s]warmup run: 488it [00:02, 504.93it/s]warmup run: 389it [00:01, 394.74it/s]warmup run: 185it [00:01, 171.38it/s]warmup run: 396it [00:01, 410.77it/s]warmup run: 682it [00:02, 683.92it/s]warmup run: 404it [00:01, 414.59it/s]warmup run: 448it [00:01, 454.13it/s]warmup run: 908it [00:02, 842.34it/s]warmup run: 588it [00:02, 608.71it/s]warmup run: 488it [00:02, 505.21it/s]warmup run: 265it [00:01, 256.96it/s]warmup run: 496it [00:01, 521.67it/s]warmup run: 778it [00:02, 752.18it/s]warmup run: 505it [00:02, 526.46it/s]warmup run: 544it [00:02, 560.12it/s]warmup run: 1010it [00:02, 888.19it/s]warmup run: 688it [00:02, 698.65it/s]warmup run: 586it [00:02, 605.16it/s]warmup run: 366it [00:01, 379.40it/s]warmup run: 596it [00:02, 623.24it/s]warmup run: 873it [00:02, 803.87it/s]warmup run: 607it [00:02, 630.95it/s]warmup run: 645it [00:02, 662.73it/s]warmup run: 1112it [00:02, 924.30it/s]warmup run: 786it [00:02, 768.37it/s]warmup run: 685it [00:02, 693.32it/s]warmup run: 468it [00:01, 499.98it/s]warmup run: 696it [00:02, 710.44it/s]warmup run: 968it [00:02, 842.89it/s]warmup run: 710it [00:02, 722.55it/s]warmup run: 746it [00:02, 747.38it/s]warmup run: 1213it [00:02, 937.02it/s]warmup run: 885it [00:02, 824.93it/s]warmup run: 784it [00:02, 765.82it/s]warmup run: 570it [00:02, 609.54it/s]warmup run: 795it [00:02, 779.51it/s]warmup run: 1064it [00:02, 873.94it/s]warmup run: 813it [00:02, 797.19it/s]warmup run: 844it [00:02, 807.63it/s]warmup run: 1313it [00:02, 938.81it/s]warmup run: 983it [00:02, 852.21it/s]warmup run: 883it [00:02, 824.14it/s]warmup run: 671it [00:02, 701.92it/s]warmup run: 895it [00:02, 836.51it/s]warmup run: 1159it [00:02, 892.21it/s]warmup run: 915it [00:02, 854.20it/s]warmup run: 944it [00:02, 859.78it/s]warmup run: 1411it [00:02, 940.90it/s]warmup run: 983it [00:02, 870.29it/s]warmup run: 773it [00:02, 781.06it/s]warmup run: 1079it [00:02, 853.60it/s]warmup run: 994it [00:02, 876.13it/s]warmup run: 1255it [00:02, 909.87it/s]warmup run: 1018it [00:02, 900.57it/s]warmup run: 1045it [00:02, 899.41it/s]warmup run: 1511it [00:03, 957.56it/s]warmup run: 1083it [00:02, 904.90it/s]warmup run: 1178it [00:02, 890.60it/s]warmup run: 871it [00:02, 832.37it/s]warmup run: 1093it [00:02, 907.36it/s]warmup run: 1356it [00:02, 936.73it/s]warmup run: 1122it [00:02, 938.19it/s]warmup run: 1143it [00:02, 908.09it/s]warmup run: 1609it [00:03, 963.14it/s]warmup run: 1182it [00:02, 928.76it/s]warmup run: 1275it [00:02, 911.11it/s]warmup run: 969it [00:02, 869.75it/s]warmup run: 1192it [00:02, 930.88it/s]warmup run: 1456it [00:03, 952.64it/s]warmup run: 1225it [00:02, 961.57it/s]warmup run: 1707it [00:03, 954.06it/s]warmup run: 1282it [00:02, 948.66it/s]warmup run: 1240it [00:02, 872.21it/s]warmup run: 1370it [00:02, 922.19it/s]warmup run: 1068it [00:02, 903.05it/s]warmup run: 1292it [00:02, 950.79it/s]warmup run: 1555it [00:03, 962.33it/s]warmup run: 1328it [00:02, 979.03it/s]warmup run: 1804it [00:03, 956.21it/s]warmup run: 1381it [00:02, 960.56it/s]warmup run: 1333it [00:02, 887.06it/s]warmup run: 1466it [00:03, 932.66it/s]warmup run: 1172it [00:02, 941.09it/s]warmup run: 1393it [00:02, 965.52it/s]warmup run: 1655it [00:03, 970.69it/s]warmup run: 1432it [00:02, 994.40it/s]warmup run: 1901it [00:03, 956.51it/s]warmup run: 1480it [00:03, 967.61it/s]warmup run: 1429it [00:03, 906.95it/s]warmup run: 1563it [00:03, 942.43it/s]warmup run: 1273it [00:02, 959.13it/s]warmup run: 1493it [00:02, 973.94it/s]warmup run: 1754it [00:03, 976.14it/s]warmup run: 1535it [00:03, 1002.89it/s]warmup run: 1998it [00:03, 952.84it/s]warmup run: 1579it [00:03, 971.51it/s]warmup run: 1530it [00:03, 936.28it/s]warmup run: 1662it [00:03, 956.07it/s]warmup run: 1374it [00:02, 972.24it/s]warmup run: 1594it [00:03, 983.08it/s]warmup run: 1853it [00:03, 979.58it/s]warmup run: 1638it [00:03, 1008.28it/s]warmup run: 2116it [00:03, 1019.78it/s]warmup run: 1679it [00:03, 979.62it/s]warmup run: 1626it [00:03, 939.41it/s]warmup run: 1762it [00:03, 968.91it/s]warmup run: 1475it [00:03, 981.05it/s]warmup run: 1694it [00:03, 986.55it/s]warmup run: 1954it [00:03, 987.05it/s]warmup run: 1741it [00:03, 1012.34it/s]warmup run: 2236it [00:03, 1071.07it/s]warmup run: 1780it [00:03, 987.16it/s]warmup run: 1725it [00:03, 952.49it/s]warmup run: 1864it [00:03, 983.15it/s]warmup run: 1579it [00:03, 995.86it/s]warmup run: 1795it [00:03, 990.43it/s]warmup run: 2065it [00:03, 1021.40it/s]warmup run: 1844it [00:03, 1013.76it/s]warmup run: 2356it [00:03, 1107.83it/s]warmup run: 1882it [00:03, 994.74it/s]warmup run: 1824it [00:03, 961.45it/s]warmup run: 1968it [00:03, 997.73it/s]warmup run: 1683it [00:03, 1007.64it/s]warmup run: 1895it [00:03, 989.83it/s]warmup run: 2182it [00:03, 1065.04it/s]warmup run: 1947it [00:03, 1014.88it/s]warmup run: 2475it [00:03, 1130.02it/s]warmup run: 1982it [00:03, 995.02it/s]warmup run: 1922it [00:03, 964.94it/s]warmup run: 2082it [00:03, 1039.04it/s]warmup run: 1787it [00:03, 1016.10it/s]warmup run: 1995it [00:03, 992.36it/s]warmup run: 2300it [00:03, 1097.15it/s]warmup run: 2058it [00:03, 1040.69it/s]warmup run: 2595it [00:04, 1149.38it/s]warmup run: 2099it [00:03, 1046.27it/s]warmup run: 2203it [00:03, 1089.91it/s]warmup run: 2027it [00:03, 988.76it/s]warmup run: 1892it [00:03, 1024.29it/s]warmup run: 2112it [00:03, 1045.19it/s]warmup run: 2418it [00:03, 1121.64it/s]warmup run: 2177it [00:03, 1083.72it/s]warmup run: 2714it [00:04, 1160.39it/s]warmup run: 2221it [00:03, 1095.55it/s]warmup run: 2147it [00:03, 1050.87it/s]warmup run: 2325it [00:03, 1126.43it/s]warmup run: 1997it [00:03, 1029.23it/s]warmup run: 2232it [00:03, 1088.69it/s]warmup run: 2536it [00:04, 1137.19it/s]warmup run: 2294it [00:03, 1108.42it/s]warmup run: 2834it [00:04, 1171.51it/s]warmup run: 2343it [00:03, 1130.54it/s]warmup run: 2267it [00:03, 1094.56it/s]warmup run: 2447it [00:03, 1152.98it/s]warmup run: 2119it [00:03, 1083.62it/s]warmup run: 2351it [00:03, 1118.78it/s]warmup run: 2653it [00:04, 1144.30it/s]warmup run: 2410it [00:03, 1121.09it/s]warmup run: 2953it [00:04, 1176.36it/s]warmup run: 2465it [00:03, 1154.91it/s]warmup run: 2386it [00:03, 1122.97it/s]warmup run: 2569it [00:04, 1170.87it/s]warmup run: 2242it [00:03, 1125.47it/s]warmup run: 2470it [00:03, 1139.56it/s]warmup run: 2771it [00:04, 1153.94it/s]warmup run: 2527it [00:03, 1134.85it/s]warmup run: 3000it [00:04, 678.89it/s] warmup run: 2586it [00:04, 1171.30it/s]warmup run: 2506it [00:04, 1144.01it/s]warmup run: 2688it [00:04, 1174.05it/s]warmup run: 2364it [00:03, 1153.60it/s]warmup run: 2587it [00:04, 1147.70it/s]warmup run: 2890it [00:04, 1162.26it/s]warmup run: 2644it [00:04, 1144.13it/s]warmup run: 2707it [00:04, 1180.54it/s]warmup run: 2626it [00:04, 1154.20it/s]warmup run: 2806it [00:04, 1172.43it/s]warmup run: 2486it [00:03, 1172.87it/s]warmup run: 2702it [00:04, 1146.11it/s]warmup run: 3000it [00:04, 675.24it/s] warmup run: 2759it [00:04, 1145.00it/s]warmup run: 2827it [00:04, 1184.75it/s]warmup run: 2927it [00:04, 1183.60it/s]warmup run: 2745it [00:04, 1163.59it/s]warmup run: 2608it [00:04, 1184.36it/s]warmup run: 2818it [00:04, 1150.14it/s]warmup run: 2878it [00:04, 1155.51it/s]warmup run: 3000it [00:04, 678.30it/s] warmup run: 2947it [00:04, 1186.94it/s]warmup run: 2863it [00:04, 1168.25it/s]warmup run: 2940it [00:04, 1169.12it/s]warmup run: 2728it [00:04, 1186.25it/s]warmup run: 2996it [00:04, 1161.60it/s]warmup run: 3000it [00:04, 691.69it/s] warmup run: 3000it [00:04, 683.42it/s] warmup run: 3000it [00:04, 688.13it/s] warmup run: 2981it [00:04, 1170.14it/s]warmup run: 2850it [00:04, 1193.45it/s]warmup run: 3000it [00:04, 673.72it/s] warmup run: 2971it [00:04, 1198.36it/s]warmup run: 3000it [00:04, 690.73it/s] 

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]



warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   5%|         | 163/3000 [00:00<00:01, 1625.77it/s]warmup should be done:   5%|         | 161/3000 [00:00<00:01, 1606.45it/s]warmup should be done:   5%|         | 160/3000 [00:00<00:01, 1597.83it/s]warmup should be done:   5%|         | 159/3000 [00:00<00:01, 1585.15it/s]warmup should be done:   5%|         | 163/3000 [00:00<00:01, 1626.12it/s]warmup should be done:   5%|         | 163/3000 [00:00<00:01, 1623.12it/s]warmup should be done:   5%|         | 153/3000 [00:00<00:01, 1522.67it/s]warmup should be done:   5%|         | 163/3000 [00:00<00:01, 1623.49it/s]warmup should be done:  11%|         | 321/3000 [00:00<00:01, 1604.34it/s]warmup should be done:  11%|         | 328/3000 [00:00<00:01, 1636.32it/s]warmup should be done:  11%|         | 320/3000 [00:00<00:01, 1598.05it/s]warmup should be done:  11%|         | 324/3000 [00:00<00:01, 1615.09it/s]warmup should be done:  11%|         | 330/3000 [00:00<00:01, 1647.28it/s]warmup should be done:  11%|         | 327/3000 [00:00<00:01, 1631.10it/s]warmup should be done:  11%|         | 328/3000 [00:00<00:01, 1634.42it/s]warmup should be done:  11%|         | 319/3000 [00:00<00:01, 1597.75it/s]warmup should be done:  16%|        | 492/3000 [00:00<00:01, 1635.83it/s]warmup should be done:  16%|        | 481/3000 [00:00<00:01, 1599.94it/s]warmup should be done:  17%|        | 496/3000 [00:00<00:01, 1650.98it/s]warmup should be done:  16%|        | 482/3000 [00:00<00:01, 1601.66it/s]warmup should be done:  16%|        | 486/3000 [00:00<00:01, 1612.28it/s]warmup should be done:  16%|        | 492/3000 [00:00<00:01, 1631.45it/s]warmup should be done:  16%|        | 491/3000 [00:00<00:01, 1624.34it/s]warmup should be done:  16%|        | 479/3000 [00:00<00:01, 1582.56it/s]warmup should be done:  21%|       | 642/3000 [00:00<00:01, 1603.86it/s]warmup should be done:  22%|       | 656/3000 [00:00<00:01, 1633.52it/s]warmup should be done:  22%|       | 662/3000 [00:00<00:01, 1652.34it/s]warmup should be done:  21%|       | 643/3000 [00:00<00:01, 1598.86it/s]warmup should be done:  22%|       | 656/3000 [00:00<00:01, 1630.17it/s]warmup should be done:  22%|       | 648/3000 [00:00<00:01, 1608.20it/s]warmup should be done:  22%|       | 654/3000 [00:00<00:01, 1621.33it/s]warmup should be done:  21%|       | 638/3000 [00:00<00:01, 1572.95it/s]warmup should be done:  27%|       | 804/3000 [00:00<00:01, 1607.19it/s]warmup should be done:  28%|       | 828/3000 [00:00<00:01, 1651.38it/s]warmup should be done:  27%|       | 803/3000 [00:00<00:01, 1595.99it/s]warmup should be done:  27%|       | 820/3000 [00:00<00:01, 1628.23it/s]warmup should be done:  27%|       | 820/3000 [00:00<00:01, 1630.03it/s]warmup should be done:  27%|       | 809/3000 [00:00<00:01, 1606.04it/s]warmup should be done:  27%|       | 817/3000 [00:00<00:01, 1620.31it/s]warmup should be done:  27%|       | 796/3000 [00:00<00:01, 1574.63it/s]warmup should be done:  32%|      | 965/3000 [00:00<00:01, 1604.82it/s]warmup should be done:  33%|      | 994/3000 [00:00<00:01, 1648.87it/s]warmup should be done:  32%|      | 963/3000 [00:00<00:01, 1595.19it/s]warmup should be done:  33%|      | 983/3000 [00:00<00:01, 1623.66it/s]warmup should be done:  33%|      | 984/3000 [00:00<00:01, 1625.08it/s]warmup should be done:  32%|      | 970/3000 [00:00<00:01, 1599.11it/s]warmup should be done:  33%|      | 980/3000 [00:00<00:01, 1612.91it/s]warmup should be done:  32%|      | 958/3000 [00:00<00:01, 1587.47it/s]warmup should be done:  38%|      | 1127/3000 [00:00<00:01, 1607.73it/s]warmup should be done:  39%|      | 1160/3000 [00:00<00:01, 1649.67it/s]warmup should be done:  37%|      | 1123/3000 [00:00<00:01, 1592.29it/s]warmup should be done:  38%|      | 1146/3000 [00:00<00:01, 1623.24it/s]warmup should be done:  38%|      | 1130/3000 [00:00<00:01, 1599.18it/s]warmup should be done:  38%|      | 1148/3000 [00:00<00:01, 1626.92it/s]warmup should be done:  38%|      | 1142/3000 [00:00<00:01, 1612.52it/s]warmup should be done:  37%|      | 1119/3000 [00:00<00:01, 1592.28it/s]warmup should be done:  43%|     | 1289/3000 [00:00<00:01, 1609.18it/s]warmup should be done:  44%|     | 1326/3000 [00:00<00:01, 1650.16it/s]warmup should be done:  43%|     | 1283/3000 [00:00<00:01, 1592.95it/s]warmup should be done:  44%|     | 1309/3000 [00:00<00:01, 1623.28it/s]warmup should be done:  43%|     | 1290/3000 [00:00<00:01, 1597.89it/s]warmup should be done:  44%|     | 1311/3000 [00:00<00:01, 1627.49it/s]warmup should be done:  43%|     | 1304/3000 [00:00<00:01, 1611.77it/s]warmup should be done:  43%|     | 1281/3000 [00:00<00:01, 1599.05it/s]warmup should be done:  48%|     | 1451/3000 [00:00<00:00, 1610.59it/s]warmup should be done:  50%|     | 1492/3000 [00:00<00:00, 1651.73it/s]warmup should be done:  48%|     | 1444/3000 [00:00<00:00, 1597.56it/s]warmup should be done:  49%|     | 1474/3000 [00:00<00:00, 1630.77it/s]warmup should be done:  48%|     | 1450/3000 [00:00<00:00, 1596.64it/s]warmup should be done:  49%|     | 1474/3000 [00:00<00:00, 1626.42it/s]warmup should be done:  48%|     | 1446/3000 [00:00<00:00, 1614.38it/s]warmup should be done:  49%|     | 1466/3000 [00:00<00:00, 1609.68it/s]warmup should be done:  54%|    | 1613/3000 [00:01<00:00, 1608.01it/s]warmup should be done:  55%|    | 1658/3000 [00:01<00:00, 1653.43it/s]warmup should be done:  54%|    | 1605/3000 [00:01<00:00, 1600.77it/s]warmup should be done:  55%|    | 1640/3000 [00:01<00:00, 1636.74it/s]warmup should be done:  54%|    | 1610/3000 [00:01<00:00, 1596.16it/s]warmup should be done:  55%|    | 1637/3000 [00:01<00:00, 1625.58it/s]warmup should be done:  54%|    | 1611/3000 [00:01<00:00, 1625.09it/s]warmup should be done:  54%|    | 1628/3000 [00:01<00:00, 1612.45it/s]warmup should be done:  59%|    | 1767/3000 [00:01<00:00, 1604.98it/s]warmup should be done:  61%|    | 1824/3000 [00:01<00:00, 1652.69it/s]warmup should be done:  60%|    | 1805/3000 [00:01<00:00, 1639.64it/s]warmup should be done:  59%|    | 1770/3000 [00:01<00:00, 1596.92it/s]warmup should be done:  59%|    | 1774/3000 [00:01<00:00, 1602.18it/s]warmup should be done:  60%|    | 1800/3000 [00:01<00:00, 1626.70it/s]warmup should be done:  59%|    | 1776/3000 [00:01<00:00, 1631.19it/s]warmup should be done:  60%|    | 1791/3000 [00:01<00:00, 1617.75it/s]warmup should be done:  64%|   | 1929/3000 [00:01<00:00, 1608.10it/s]warmup should be done:  66%|   | 1990/3000 [00:01<00:00, 1652.01it/s]warmup should be done:  66%|   | 1970/3000 [00:01<00:00, 1640.71it/s]warmup should be done:  65%|   | 1963/3000 [00:01<00:00, 1627.28it/s]warmup should be done:  64%|   | 1932/3000 [00:01<00:00, 1601.04it/s]warmup should be done:  64%|   | 1935/3000 [00:01<00:00, 1596.30it/s]warmup should be done:  65%|   | 1941/3000 [00:01<00:00, 1634.40it/s]warmup should be done:  65%|   | 1953/3000 [00:01<00:00, 1618.16it/s]warmup should be done:  70%|   | 2091/3000 [00:01<00:00, 1609.60it/s]warmup should be done:  71%|   | 2126/3000 [00:01<00:00, 1627.09it/s]warmup should be done:  72%|  | 2156/3000 [00:01<00:00, 1651.15it/s]warmup should be done:  71%|   | 2135/3000 [00:01<00:00, 1641.44it/s]warmup should be done:  70%|   | 2097/3000 [00:01<00:00, 1613.32it/s]warmup should be done:  70%|   | 2115/3000 [00:01<00:00, 1618.53it/s]warmup should be done:  70%|   | 2106/3000 [00:01<00:00, 1636.54it/s]warmup should be done:  70%|   | 2095/3000 [00:01<00:00, 1590.56it/s]warmup should be done:  75%|  | 2253/3000 [00:01<00:00, 1609.87it/s]warmup should be done:  77%|  | 2300/3000 [00:01<00:00, 1640.53it/s]warmup should be done:  75%|  | 2261/3000 [00:01<00:00, 1619.73it/s]warmup should be done:  76%|  | 2289/3000 [00:01<00:00, 1623.64it/s]warmup should be done:  77%|  | 2322/3000 [00:01<00:00, 1648.77it/s]warmup should be done:  76%|  | 2270/3000 [00:01<00:00, 1635.81it/s]warmup should be done:  76%|  | 2277/3000 [00:01<00:00, 1614.93it/s]warmup should be done:  75%|  | 2255/3000 [00:01<00:00, 1584.31it/s]warmup should be done:  80%|  | 2414/3000 [00:01<00:00, 1608.09it/s]warmup should be done:  82%| | 2465/3000 [00:01<00:00, 1642.43it/s]warmup should be done:  83%| | 2488/3000 [00:01<00:00, 1651.55it/s]warmup should be done:  82%| | 2452/3000 [00:01<00:00, 1624.19it/s]warmup should be done:  81%|  | 2426/3000 [00:01<00:00, 1627.15it/s]warmup should be done:  81%|  | 2435/3000 [00:01<00:00, 1640.01it/s]warmup should be done:  81%| | 2439/3000 [00:01<00:00, 1615.96it/s]warmup should be done:  80%|  | 2414/3000 [00:01<00:00, 1583.37it/s]warmup should be done:  86%| | 2576/3000 [00:01<00:00, 1609.91it/s]warmup should be done:  88%| | 2630/3000 [00:01<00:00, 1643.98it/s]warmup should be done:  87%| | 2615/3000 [00:01<00:00, 1625.85it/s]warmup should be done:  86%| | 2591/3000 [00:01<00:00, 1632.63it/s]warmup should be done:  87%| | 2600/3000 [00:01<00:00, 1641.39it/s]warmup should be done:  87%| | 2602/3000 [00:01<00:00, 1617.35it/s]warmup should be done:  88%| | 2654/3000 [00:01<00:00, 1637.08it/s]warmup should be done:  86%| | 2573/3000 [00:01<00:00, 1568.65it/s]warmup should be done:  91%|| 2738/3000 [00:01<00:00, 1611.73it/s]warmup should be done:  93%|| 2778/3000 [00:01<00:00, 1626.96it/s]warmup should be done:  93%|| 2795/3000 [00:01<00:00, 1644.04it/s]warmup should be done:  92%|| 2756/3000 [00:01<00:00, 1636.92it/s]warmup should be done:  92%|| 2765/3000 [00:01<00:00, 1640.15it/s]warmup should be done:  92%|| 2765/3000 [00:01<00:00, 1618.52it/s]warmup should be done:  94%|| 2819/3000 [00:01<00:00, 1638.54it/s]warmup should be done:  91%| | 2734/3000 [00:01<00:00, 1579.53it/s]warmup should be done:  97%|| 2901/3000 [00:01<00:00, 1614.96it/s]warmup should be done:  99%|| 2961/3000 [00:01<00:00, 1648.53it/s]warmup should be done:  98%|| 2943/3000 [00:01<00:00, 1631.83it/s]warmup should be done:  97%|| 2922/3000 [00:01<00:00, 1642.46it/s]warmup should be done:  98%|| 2930/3000 [00:01<00:00, 1640.73it/s]warmup should be done:  98%|| 2929/3000 [00:01<00:00, 1622.57it/s]warmup should be done: 100%|| 2986/3000 [00:01<00:00, 1645.89it/s]warmup should be done:  97%|| 2897/3000 [00:01<00:00, 1592.56it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1647.89it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1637.84it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1627.70it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1618.74it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1618.12it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1616.96it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1605.43it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1596.14it/s]





warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|         | 165/3000 [00:00<00:01, 1647.55it/s]warmup should be done:   6%|         | 166/3000 [00:00<00:01, 1656.81it/s]warmup should be done:   6%|         | 170/3000 [00:00<00:01, 1693.78it/s]warmup should be done:   6%|         | 169/3000 [00:00<00:01, 1684.18it/s]warmup should be done:   6%|         | 165/3000 [00:00<00:01, 1642.89it/s]warmup should be done:   6%|         | 167/3000 [00:00<00:01, 1662.14it/s]warmup should be done:   6%|         | 169/3000 [00:00<00:01, 1681.73it/s]warmup should be done:   6%|         | 168/3000 [00:00<00:01, 1670.70it/s]warmup should be done:  11%|         | 331/3000 [00:00<00:01, 1652.55it/s]warmup should be done:  11%|         | 331/3000 [00:00<00:01, 1652.22it/s]warmup should be done:  11%|        | 340/3000 [00:00<00:01, 1694.29it/s]warmup should be done:  11%|        | 339/3000 [00:00<00:01, 1688.51it/s]warmup should be done:  11%|        | 338/3000 [00:00<00:01, 1682.58it/s]warmup should be done:  11%|         | 334/3000 [00:00<00:01, 1659.84it/s]warmup should be done:  11%|         | 336/3000 [00:00<00:01, 1664.04it/s]warmup should be done:  11%|         | 332/3000 [00:00<00:01, 1642.15it/s]warmup should be done:  17%|        | 497/3000 [00:00<00:01, 1655.38it/s]warmup should be done:  17%|        | 498/3000 [00:00<00:01, 1658.10it/s]warmup should be done:  17%|        | 509/3000 [00:00<00:01, 1692.52it/s]warmup should be done:  17%|        | 511/3000 [00:00<00:01, 1697.50it/s]warmup should be done:  17%|        | 501/3000 [00:00<00:01, 1661.13it/s]warmup should be done:  17%|        | 500/3000 [00:00<00:01, 1657.87it/s]warmup should be done:  17%|        | 503/3000 [00:00<00:01, 1660.51it/s]warmup should be done:  17%|        | 507/3000 [00:00<00:01, 1671.85it/s]warmup should be done:  22%|       | 663/3000 [00:00<00:01, 1655.33it/s]warmup should be done:  22%|       | 664/3000 [00:00<00:01, 1655.61it/s]warmup should be done:  23%|       | 679/3000 [00:00<00:01, 1692.77it/s]warmup should be done:  23%|       | 682/3000 [00:00<00:01, 1700.31it/s]warmup should be done:  22%|       | 668/3000 [00:00<00:01, 1662.73it/s]warmup should be done:  22%|       | 668/3000 [00:00<00:01, 1663.44it/s]warmup should be done:  22%|       | 670/3000 [00:00<00:01, 1663.15it/s]warmup should be done:  22%|       | 675/3000 [00:00<00:01, 1668.55it/s]warmup should be done:  28%|       | 829/3000 [00:00<00:01, 1655.79it/s]warmup should be done:  28%|       | 830/3000 [00:00<00:01, 1653.04it/s]warmup should be done:  28%|       | 853/3000 [00:00<00:01, 1700.43it/s]warmup should be done:  28%|       | 849/3000 [00:00<00:01, 1691.08it/s]warmup should be done:  28%|       | 835/3000 [00:00<00:01, 1663.61it/s]warmup should be done:  28%|       | 835/3000 [00:00<00:01, 1663.01it/s]warmup should be done:  28%|       | 838/3000 [00:00<00:01, 1666.26it/s]warmup should be done:  28%|       | 842/3000 [00:00<00:01, 1665.58it/s]warmup should be done:  33%|      | 996/3000 [00:00<00:01, 1658.96it/s]warmup should be done:  33%|      | 997/3000 [00:00<00:01, 1656.33it/s]warmup should be done:  34%|      | 1024/3000 [00:00<00:01, 1699.92it/s]warmup should be done:  34%|      | 1019/3000 [00:00<00:01, 1690.52it/s]warmup should be done:  33%|      | 1002/3000 [00:00<00:01, 1661.60it/s]warmup should be done:  33%|      | 1002/3000 [00:00<00:01, 1664.49it/s]warmup should be done:  34%|      | 1005/3000 [00:00<00:01, 1667.28it/s]warmup should be done:  34%|      | 1009/3000 [00:00<00:01, 1663.93it/s]warmup should be done:  39%|      | 1163/3000 [00:00<00:01, 1660.93it/s]warmup should be done:  39%|      | 1164/3000 [00:00<00:01, 1658.95it/s]warmup should be done:  40%|      | 1189/3000 [00:00<00:01, 1691.64it/s]warmup should be done:  40%|      | 1194/3000 [00:00<00:01, 1696.86it/s]warmup should be done:  39%|      | 1171/3000 [00:00<00:01, 1672.10it/s]warmup should be done:  39%|      | 1173/3000 [00:00<00:01, 1670.50it/s]warmup should be done:  39%|      | 1169/3000 [00:00<00:01, 1659.46it/s]warmup should be done:  39%|      | 1176/3000 [00:00<00:01, 1662.22it/s]warmup should be done:  44%|     | 1330/3000 [00:00<00:01, 1658.07it/s]warmup should be done:  45%|     | 1359/3000 [00:00<00:00, 1693.64it/s]warmup should be done:  44%|     | 1330/3000 [00:00<00:01, 1655.94it/s]warmup should be done:  46%|     | 1365/3000 [00:00<00:00, 1699.47it/s]warmup should be done:  45%|     | 1342/3000 [00:00<00:00, 1675.37it/s]warmup should be done:  45%|     | 1341/3000 [00:00<00:00, 1678.06it/s]warmup should be done:  45%|     | 1336/3000 [00:00<00:01, 1661.02it/s]warmup should be done:  45%|     | 1343/3000 [00:00<00:00, 1661.79it/s]warmup should be done:  50%|     | 1497/3000 [00:00<00:00, 1659.05it/s]warmup should be done:  50%|     | 1496/3000 [00:00<00:00, 1656.52it/s]warmup should be done:  51%|     | 1529/3000 [00:00<00:00, 1692.30it/s]warmup should be done:  51%|     | 1536/3000 [00:00<00:00, 1700.04it/s]warmup should be done:  50%|     | 1510/3000 [00:00<00:00, 1680.75it/s]warmup should be done:  50%|     | 1510/3000 [00:00<00:00, 1674.48it/s]warmup should be done:  50%|     | 1503/3000 [00:00<00:00, 1661.90it/s]warmup should be done:  50%|     | 1510/3000 [00:00<00:00, 1660.81it/s]warmup should be done:  55%|    | 1663/3000 [00:01<00:00, 1658.50it/s]warmup should be done:  55%|    | 1663/3000 [00:01<00:00, 1659.12it/s]warmup should be done:  56%|    | 1679/3000 [00:01<00:00, 1682.62it/s]warmup should be done:  56%|    | 1679/3000 [00:01<00:00, 1677.85it/s]warmup should be done:  57%|    | 1707/3000 [00:01<00:00, 1699.89it/s]warmup should be done:  56%|    | 1670/3000 [00:01<00:00, 1663.44it/s]warmup should be done:  57%|    | 1699/3000 [00:01<00:00, 1690.26it/s]warmup should be done:  56%|    | 1677/3000 [00:01<00:00, 1659.95it/s]warmup should be done:  61%|    | 1829/3000 [00:01<00:00, 1657.48it/s]warmup should be done:  61%|    | 1830/3000 [00:01<00:00, 1660.79it/s]warmup should be done:  62%|   | 1848/3000 [00:01<00:00, 1683.88it/s]warmup should be done:  61%|    | 1837/3000 [00:01<00:00, 1663.61it/s]warmup should be done:  62%|   | 1849/3000 [00:01<00:00, 1681.75it/s]warmup should be done:  63%|   | 1878/3000 [00:01<00:00, 1700.67it/s]warmup should be done:  62%|   | 1869/3000 [00:01<00:00, 1690.46it/s]warmup should be done:  61%|   | 1844/3000 [00:01<00:00, 1659.92it/s]warmup should be done:  66%|   | 1995/3000 [00:01<00:00, 1657.52it/s]warmup should be done:  67%|   | 2017/3000 [00:01<00:00, 1685.62it/s]warmup should be done:  67%|   | 2018/3000 [00:01<00:00, 1683.77it/s]warmup should be done:  67%|   | 1997/3000 [00:01<00:00, 1657.99it/s]warmup should be done:  68%|   | 2049/3000 [00:01<00:00, 1701.37it/s]warmup should be done:  68%|   | 2039/3000 [00:01<00:00, 1689.44it/s]warmup should be done:  67%|   | 2004/3000 [00:01<00:00, 1660.63it/s]warmup should be done:  67%|   | 2011/3000 [00:01<00:00, 1660.59it/s]warmup should be done:  72%|  | 2161/3000 [00:01<00:00, 1658.18it/s]warmup should be done:  73%|  | 2186/3000 [00:01<00:00, 1685.39it/s]warmup should be done:  72%|  | 2163/3000 [00:01<00:00, 1656.23it/s]warmup should be done:  74%|  | 2220/3000 [00:01<00:00, 1700.25it/s]warmup should be done:  72%|  | 2171/3000 [00:01<00:00, 1659.68it/s]warmup should be done:  73%|  | 2187/3000 [00:01<00:00, 1672.67it/s]warmup should be done:  74%|  | 2208/3000 [00:01<00:00, 1680.71it/s]warmup should be done:  73%|  | 2178/3000 [00:01<00:00, 1660.71it/s]warmup should be done:  78%|  | 2328/3000 [00:01<00:00, 1660.02it/s]warmup should be done:  79%|  | 2356/3000 [00:01<00:00, 1688.03it/s]warmup should be done:  78%|  | 2330/3000 [00:01<00:00, 1658.53it/s]warmup should be done:  80%|  | 2391/3000 [00:01<00:00, 1699.66it/s]warmup should be done:  78%|  | 2338/3000 [00:01<00:00, 1661.82it/s]warmup should be done:  79%|  | 2357/3000 [00:01<00:00, 1677.93it/s]warmup should be done:  78%|  | 2345/3000 [00:01<00:00, 1662.39it/s]warmup should be done:  79%|  | 2377/3000 [00:01<00:00, 1673.49it/s]warmup should be done:  83%| | 2495/3000 [00:01<00:00, 1660.45it/s]warmup should be done:  84%| | 2526/3000 [00:01<00:00, 1690.50it/s]warmup should be done:  83%| | 2497/3000 [00:01<00:00, 1660.79it/s]warmup should be done:  85%| | 2562/3000 [00:01<00:00, 1701.60it/s]warmup should be done:  84%| | 2506/3000 [00:01<00:00, 1664.22it/s]warmup should be done:  84%| | 2526/3000 [00:01<00:00, 1681.32it/s]warmup should be done:  84%| | 2512/3000 [00:01<00:00, 1664.54it/s]warmup should be done:  85%| | 2545/3000 [00:01<00:00, 1670.44it/s]warmup should be done:  89%| | 2662/3000 [00:01<00:00, 1659.03it/s]warmup should be done:  90%| | 2696/3000 [00:01<00:00, 1691.33it/s]warmup should be done:  89%| | 2664/3000 [00:01<00:00, 1659.04it/s]warmup should be done:  91%| | 2733/3000 [00:01<00:00, 1702.08it/s]warmup should be done:  89%| | 2673/3000 [00:01<00:00, 1663.24it/s]warmup should be done:  90%| | 2695/3000 [00:01<00:00, 1683.83it/s]warmup should be done:  89%| | 2680/3000 [00:01<00:00, 1666.46it/s]warmup should be done:  90%| | 2713/3000 [00:01<00:00, 1667.59it/s]warmup should be done:  94%|| 2828/3000 [00:01<00:00, 1658.71it/s]warmup should be done:  96%|| 2866/3000 [00:01<00:00, 1690.08it/s]warmup should be done:  94%|| 2830/3000 [00:01<00:00, 1656.69it/s]warmup should be done:  95%|| 2864/3000 [00:01<00:00, 1684.62it/s]warmup should be done:  97%|| 2904/3000 [00:01<00:00, 1699.06it/s]warmup should be done:  95%|| 2840/3000 [00:01<00:00, 1661.69it/s]warmup should be done:  95%|| 2848/3000 [00:01<00:00, 1667.93it/s]warmup should be done:  96%|| 2880/3000 [00:01<00:00, 1662.66it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1698.57it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1680.10it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1677.45it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1676.96it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1665.70it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1661.88it/s]warmup should be done: 100%|| 2995/3000 [00:01<00:00, 1660.52it/s]warmup should be done: 100%|| 2996/3000 [00:01<00:00, 1657.56it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1658.13it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1656.97it/s]2022-12-12 05:17:46.929836: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fddaf82c610 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 05:17:46.929895: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 05:17:46.982883: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fbfc002a490 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 05:17:46.982951: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 05:17:47.661077: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fbfc802d520 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 05:17:47.661156: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 05:17:47.708924: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fddb3797180 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 05:17:47.708989: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 05:17:47.757347: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fddaf82fd30 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 05:17:47.757412: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 05:17:47.760615: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fddaf834a90 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 05:17:47.760657: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 05:17:47.762555: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fdda315fb00 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 05:17:47.762594: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 05:17:47.809453: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fbfa402dcc0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 05:17:47.809536: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 05:17:49.183254: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 05:17:49.259512: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 05:17:49.970034: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 05:17:50.017152: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 05:17:50.064778: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 05:17:50.066460: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 05:17:50.075710: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 05:17:50.081701: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 05:17:52.083494: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 05:17:52.189751: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 05:17:52.883284: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 05:17:52.980387: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 05:17:52.986841: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 05:17:53.004470: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 05:17:53.056008: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 05:17:53.198677: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][05:18:17.799][ERROR][RK0][main]: replica 3 reaches 1000, calling init pre replica
[HCTR][05:18:17.799][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][05:18:17.799][ERROR][RK0][main]: replica 6 reaches 1000, calling init pre replica
[HCTR][05:18:17.799][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][05:18:17.804][ERROR][RK0][main]: replica 7 reaches 1000, calling init pre replica
[HCTR][05:18:17.804][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][05:18:17.808][ERROR][RK0][tid #140591375570688]: replica 0 reaches 1000, calling init pre replica
[HCTR][05:18:17.808][ERROR][RK0][tid #140591375570688]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][05:18:17.809][ERROR][RK0][main]: coll ps creation done
[HCTR][05:18:17.809][ERROR][RK0][main]: replica 7 waits for coll ps creation barrier
[HCTR][05:18:17.810][ERROR][RK0][main]: coll ps creation done
[HCTR][05:18:17.810][ERROR][RK0][main]: replica 3 waits for coll ps creation barrier
[HCTR][05:18:17.811][ERROR][RK0][main]: coll ps creation done
[HCTR][05:18:17.811][ERROR][RK0][main]: replica 6 waits for coll ps creation barrier
[HCTR][05:18:17.813][ERROR][RK0][tid #140591375570688]: coll ps creation done
[HCTR][05:18:17.813][ERROR][RK0][tid #140591375570688]: replica 0 waits for coll ps creation barrier
[HCTR][05:18:17.819][ERROR][RK0][main]: replica 2 reaches 1000, calling init pre replica
[HCTR][05:18:17.819][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][05:18:17.827][ERROR][RK0][main]: coll ps creation done
[HCTR][05:18:17.827][ERROR][RK0][main]: replica 2 waits for coll ps creation barrier
[HCTR][05:18:17.846][ERROR][RK0][main]: replica 1 reaches 1000, calling init pre replica
[HCTR][05:18:17.847][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][05:18:17.854][ERROR][RK0][main]: coll ps creation done
[HCTR][05:18:17.854][ERROR][RK0][main]: replica 1 waits for coll ps creation barrier
[HCTR][05:18:17.905][ERROR][RK0][main]: replica 4 reaches 1000, calling init pre replica
[HCTR][05:18:17.905][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][05:18:17.910][ERROR][RK0][main]: coll ps creation done
[HCTR][05:18:17.910][ERROR][RK0][main]: replica 4 waits for coll ps creation barrier
[HCTR][05:18:17.940][ERROR][RK0][main]: replica 5 reaches 1000, calling init pre replica
[HCTR][05:18:17.940][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][05:18:17.945][ERROR][RK0][main]: coll ps creation done
[HCTR][05:18:17.945][ERROR][RK0][main]: replica 5 waits for coll ps creation barrier
[HCTR][05:18:17.945][ERROR][RK0][tid #140591375570688]: replica 0 preparing frequency
[HCTR][05:18:18.969][ERROR][RK0][tid #140591375570688]: replica 0 preparing frequency done
[HCTR][05:18:19.010][ERROR][RK0][tid #140591375570688]: replica 0 calling init per replica
[HCTR][05:18:19.010][ERROR][RK0][main]: replica 4 calling init per replica
[HCTR][05:18:19.010][ERROR][RK0][main]: replica 2 calling init per replica
[HCTR][05:18:19.010][ERROR][RK0][main]: replica 3 calling init per replica
[HCTR][05:18:19.010][ERROR][RK0][main]: Calling build_v2
[HCTR][05:18:19.010][ERROR][RK0][main]: replica 7 calling init per replica
[HCTR][05:18:19.010][ERROR][RK0][main]: replica 6 calling init per replica
[HCTR][05:18:19.010][ERROR][RK0][main]: replica 1 calling init per replica
[HCTR][05:18:19.010][ERROR][RK0][main]: replica 5 calling init per replica
[HCTR][05:18:19.011][ERROR][RK0][main]: Calling build_v2
[HCTR][05:18:19.010][ERROR][RK0][tid #140591375570688]: Calling build_v2
[HCTR][05:18:19.010][ERROR][RK0][main]: Calling build_v2
[HCTR][05:18:19.011][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][05:18:19.011][ERROR][RK0][main]: Calling build_v2
[HCTR][05:18:19.011][ERROR][RK0][main]: Calling build_v2
[HCTR][05:18:19.011][ERROR][RK0][main]: Calling build_v2
[HCTR][05:18:19.011][ERROR][RK0][tid #140591375570688]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][05:18:19.011][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][05:18:19.011][ERROR][RK0][main]: Calling build_v2
[HCTR][05:18:19.011][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][05:18:19.011][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][05:18:19.011][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][05:18:19.011][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][05:18:19.011][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[2022-12-12 05:18:19[[[[[.[[2022-12-12 05:18:192022-12-12 05:18:192022-12-12 05:18:19 111172022-12-12 05:18:19.2022-12-12 05:18:192022-12-12 05:18:19.2022-12-12 05:18:19.: . 11152.. 11149. 11149E 11149:  11157 11152:  11162:  : E: : E: E/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.ccE EE E : /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc  /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc136/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:] :136::136:136using concurrent impl MPSPhase136] 136136] 136] 
] using concurrent impl MPSPhase] ] using concurrent impl MPSPhase] using concurrent impl MPSPhaseusing concurrent impl MPSPhase
using concurrent impl MPSPhaseusing concurrent impl MPSPhase
using concurrent impl MPSPhase




[2022-12-12 05:18:19. 15394: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-12 05:18:19. 15434: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 05:18:19:.196 15439] : assigning 8 to cpuE
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[[2022-12-12 05:18:192022-12-12 05:18:19.. 15490[ 15486: 2022-12-12 05:18:19: E.E  15503 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:E:196[ 178] 2022-12-12 05:18:19/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] assigning 8 to cpu.:v100x8, slow pcie
 15530212
: ] Ebuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8 [
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 05:18:19[:.2022-12-12 05:18:19178 15575.] : [ 15576v100x8, slow pcieE[2022-12-12 05:18:19: 
 2022-12-12 05:18:19.E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc. 15600[ : 15606: [2022-12-12 05:18:19/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc196: E2022-12-12 05:18:19.:] E . 15626178assigning 8 to cpu /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[ 15627: ] 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:2022-12-12 05:18:19: Ev100x8, slow pcie:213[.E 
212] 2022-12-12 05:18:19 15673 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] remote time is 8.68421.[[: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8
 157142022-12-12 05:18:192022-12-12 05:18:19E:196
: .[. [178] E 157562022-12-12 05:18:19 15756/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 05:18:19] assigning 8 to cpu : .: :.v100x8, slow pcie
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE 15804E178 15834
: :  ] : 178/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccv100x8, slow pcieE] [:2022-12-12 05:18:19 :
 v100x8, slow pcie2022-12-12 05:18:19196./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc212/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[
.] [ 15934:] :2022-12-12 05:18:19 15955assigning 8 to cpu2022-12-12 05:18:19: 214build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8213.: 
.E] 
]  16006E 16039 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cccpu time is 97.0588remote time is 8.68421: [ : :[

E2022-12-12 05:18:19/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE1962022-12-12 05:18:19 .[: ] ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 161142022-12-12 05:18:19212/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccassigning 8 to cpu 16124:: .] :
: 196E 16167build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8196E]  : 
[]  assigning 8 to cpu/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE2022-12-12 05:18:19assigning 8 to cpu/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[
: .
:2022-12-12 05:18:19213/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 16282212.] :: ]  16296[remote time is 8.68421214[Ebuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8: 2022-12-12 05:18:19
] 2022-12-12 05:18:19 
E.cpu time is 97.0588[.[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc  16366
2022-12-12 05:18:19 163942022-12-12 05:18:19:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: .: .212:E 16441E 16423] 213 :  : build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE
remote time is 8.68421: : 
212/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc212[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] [:] 2022-12-12 05:18:19:build asymm link desc with 8X Tesla V100-SXM2-32GB out of 82022-12-12 05:18:19213build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8.214
.] 
 16602]  16617remote time is 8.68421: [cpu time is 97.0588: [
E2022-12-12 05:18:19
E2022-12-12 05:18:19 .[ ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 166912022-12-12 05:18:19/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc 16698:: .:: 213E 16727214E]  : ]  remote time is 8.68421/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccEcpu time is 97.0588/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
: 
:213/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[213] :2022-12-12 05:18:19] remote time is 8.68421214.remote time is 8.68421
]  16836
cpu time is 97.0588: [
E[2022-12-12 05:18:19 2022-12-12 05:18:19./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc. 16890: 16899: 214: E] E cpu time is 97.0588 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc::214214] ] cpu time is 97.0588cpu time is 97.0588

[2022-12-12 05:19:36.248820: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-12 05:19:36.288755: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 381.47 MB
[2022-12-12 05:19:36.288842: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 381.47 MB
[2022-12-12 05:19:36.289928: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:74] mapping nid to rank...
[2022-12-12 05:19:36.364884: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:91] counting slots...
[2022-12-12 05:19:36.758626: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:105] Final num slot is 49
[2022-12-12 05:19:36.758711: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:109] counting blocks...
[2022-12-12 05:19:43.596656: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:118] Final num block is 1024
[2022-12-12 05:19:43.596748: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:123] counting freq and density...
[2022-12-12 05:19:45.255983: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:154] averaging freq and density...
[2022-12-12 05:19:45.256081: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:155] 1024
[2022-12-12 05:19:45.258913: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
[2022-12-12 05:19:45.258971: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:354] constructing optimal solver, device=8, stream=1
1024 blocks, 8 devices
[2022-12-12 05:19:45.563715: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:527] Add Var...
[2022-12-12 05:19:45.591852: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:545] Capacity...
[2022-12-12 05:19:45.593283: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:548] Connect CPU...
[2022-12-12 05:19:45.613652: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:550] Connect Access To Storage...
[2022-12-12 05:19:46.135013: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:554] Time...
[2022-12-12 05:19:46.137261: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 0, total sm is 80
[2022-12-12 05:19:46.140232: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 1, total sm is 80
[2022-12-12 05:19:46.143113: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 2, total sm is 80
[2022-12-12 05:19:46.146028: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 3, total sm is 80
[2022-12-12 05:19:46.149009: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 4, total sm is 80
[2022-12-12 05:19:46.151982: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 5, total sm is 80
[2022-12-12 05:19:46.154887: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 6, total sm is 80
[2022-12-12 05:19:46.157803: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:516] dst 7, total sm is 80
[2022-12-12 05:23:07.385971: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:569] Coll Cache init block placement array
[2022-12-12 05:23:07.398230: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:645] Coll Cache init block placement array done
[2022-12-12 05:23:07.490197: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:647] Coll Cache model reset done
[2022-12-12 05:23:07.537944: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-12 05:23:07.538049: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-12 05:23:07.538083: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-12 05:23:07.538114: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-12 05:23:07.538680: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 05:23:07.538735: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:23:07.539707: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:23:07.540399: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:23:07.553551: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 1 solved
[2022-12-12 05:23:07.553625: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 1 initing device 1
[[2022-12-12 05:23:072022-12-12 05:23:07..553873553873: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::202202] ] 2 solved6 solved

[[2022-12-12 05:23:072022-12-12 05:23:07..553966553968: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::205205] ] worker 0 thread 2 initing device 2worker 0 thread 6 initing device 6

[2022-12-12 05:23:07.554076: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 05:23:07.554125: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:23:07[.2022-12-12 05:23:07554447.: 554451E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1815:] 1815Building Coll Cache with ... num gpu device is 8] 
Building Coll Cache with ... num gpu device is 8
[2022-12-12 05:23:07[.2022-12-12 05:23:07554513.: 554517E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980:] 1980eager alloc mem 381.47 MB] 
eager alloc mem 381.47 MB
[[2022-12-12 05:23:072022-12-12 05:23:07..554717554712: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::202202] ] 5 solved7 solved

[2022-12-12 05:23:07.[5548522022-12-12 05:23:07: .E554855 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.ccE: 205/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc] :worker 0 thread 5 initing device 5205
] worker 0 thread 7 initing device 7
[2022-12-12 05:23:07.555353: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8[
2022-12-12 05:23:07.555377: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] [Building Coll Cache with ... num gpu device is 82022-12-12 05:23:07
.555405: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:23:07.555442: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:23:07.557371: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:23:07.557422: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:23:07.557596: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 3 solved
[2022-12-12 05:23:07.557651: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc[:2022-12-12 05:23:07205.] 557656worker 0 thread 3 initing device 3: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:23:07.557690: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 4 solved
[2022-12-12 05:23:07.557749: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 4 initing device 4
[2022-12-12 05:23:07.558088: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 05:23:07.558130: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:23:07.558151: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:23:07.558186: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 05:23:07.558228: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:23:07.558656: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:23:07.561546: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:23:07.561822: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:23:07.561944: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:23:07.562461: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] [eager alloc mem 381.47 MB2022-12-12 05:23:07
.562488: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:23:07.562568: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:23:07.563038: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:23:07.566260: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:23:07.566376: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 05:23:07.621419: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-12 05:23:07.627204: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 05:23:07.627376: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 05:23:07.628269: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 76.68 MB
[2022-12-12 05:23:07.629033: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:23:07.630152: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:23:07.630200: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 76.27 MB
[2022-12-12 05:23:07.640996: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980[[] 2022-12-12 05:23:07[[2022-12-12 05:23:07eager alloc mem 1024.00 Bytes.2022-12-12 05:23:072022-12-12 05:23:07.
641052..641052: 641061641062: E: : E EE /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980::1980] 19801980] ] eager alloc mem 1024.00 Bytes
] eager alloc mem 1024.00 Byteseager alloc mem 1024.00 Byteseager alloc mem 1024.00 Bytes


[2022-12-12 05:23:07.647476: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 05:23:07.647593: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 05:23:07.647675: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[[2022-12-12 05:23:072022-12-12 05:23:07..647754647775: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 1024eager release cuda mem 400000000

[2022-12-12 05:23:07.647861: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-12 05:23:07:.638647859] : eager release cuda mem 400000000E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 05:23:07.647947[: 2022-12-12 05:23:07E. 647974/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :E638 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 1024:
638] eager release cuda mem 400000000
[2022-12-12 05:23:07.648045: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 05:23:07.649579: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-12 05:23:07.650641: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-12 05:23:07.653537: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 76.68 MB
[2022-12-12 05:23:07.654047: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 76.68 MB
[2022-12-12 05:23:07.654558: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 76.68 MB
[2022-12-12 05:23:07.655097: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 76.68 MB
[2022-12-12 05:23:07.655634: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 76.68 MB
[2022-12-12 05:23:07.668789: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:23:07.668826: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:23:07.668883: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:23:07.668946: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:23:07.668996: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:23:07.669283: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 05:23:07.669366: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 05:23:07.669813: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:23:07.669841: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-12 05:23:07:.638669856] : [eager release cuda mem 625663W[2022-12-12 05:23:07
 2022-12-12 05:23:07./hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc.669858:669877: 43: [E] E2022-12-12 05:23:07 WORKER[0] alloc host memory 76.09 MB ./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc669909::: 638638W] ]  eager release cuda mem 1024eager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc

:43] WORKER[0] alloc host memory 76.25 MB
[2022-12-12 05:23:07.669983: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:[43[2022-12-12 05:23:07] 2022-12-12 05:23:07.WORKER[0] alloc host memory 76.27 MB.669996
669992: [: E2022-12-12 05:23:07E . /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc670034/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:: :638E638]  ] [eager release cuda mem 400000000/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 6256632022-12-12 05:23:07
:
.638670110] : eager release cuda mem 625663E
[ 2022-12-12 05:23:07/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.:670175[1980: 2022-12-12 05:23:07] W.eager alloc mem 76.68 MB 670200
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc: :W43 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.ccWORKER[0] alloc host memory 76.27 MB:
43] WORKER[0] alloc host memory 76.29 MB
[2022-12-12 05:23:07.671073: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 76.68 MB
[2022-12-12 05:23:07.671499: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:23:07.671706: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:23:07.672627: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:23:07.672677: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 75.96 MB
[2022-12-12 05:23:07.672747: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:23:07.672804: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 76.28 MB
[2022-12-12 05:23:07.682883: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 05:23:07.683522: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 05:23:07.683568: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 9.54 GB
[2022-12-12 05:23:07.718815: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 05:23:07.719299: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 05:23:07.719428: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 05:23:07.719473: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 9.54 GB
[2022-12-12 05:23:07.719745: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 05:23:07.719903: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 05:23:07.719943: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 9.54 GB
[2022-12-12 05:23:07.720353: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 05:23:07.720395: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 9.52 GB
[2022-12-12 05:23:07.721495: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 05:23:07.722105: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 05:23:07.722146: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] [eager alloc mem 9.54 GB2022-12-12 05:23:07
.722148: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 05:23:07.722545: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 05:23:07.722757: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 05:23:07.722799: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 9.54 GB
[2022-12-12 05:23:07.722996: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 05:23:07.723155: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 05:23:07.723199: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 9.50 GB
[2022-12-12 05:23:07.723598: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 05:23:07.723641: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 9.54 GB
[[[[[[[[2022-12-12 05:23:102022-12-12 05:23:102022-12-12 05:23:102022-12-12 05:23:102022-12-12 05:23:102022-12-12 05:23:102022-12-12 05:23:102022-12-12 05:23:10........908128908128908123908124908123908122908124908122: : : : : : : : EEEEEEEE        /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::::::19261926192619261926192619261926] ] ] ] ] ] ] ] Device 1 init p2p of link 7Device 0 init p2p of link 3Device 7 init p2p of link 4Device 3 init p2p of link 2Device 2 init p2p of link 1Device 6 init p2p of link 0Device 5 init p2p of link 6Device 4 init p2p of link 5







[[[[[2022-12-12 05:23:10[[2022-12-12 05:23:10[2022-12-12 05:23:102022-12-12 05:23:102022-12-12 05:23:10.2022-12-12 05:23:102022-12-12 05:23:10.2022-12-12 05:23:10...908744..908744.908745908745908747: 908748908753: 908758: : : E: : E: EEE EE E   /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:::1980::1980:198019801980] 19801980] 1980] ] ] eager alloc mem 611.00 KB] ] eager alloc mem 611.00 KB] eager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KB
eager alloc mem 611.00 KBeager alloc mem 611.00 KB
eager alloc mem 611.00 KB





[2022-12-12 05:23:10[.[2022-12-12 05:23:10909930[2022-12-12 05:23:10.[: 2022-12-12 05:23:10[.[9099362022-12-12 05:23:10E.2022-12-12 05:23:109099392022-12-12 05:23:10: . 909945[.: .E909954/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: 2022-12-12 05:23:10909971E909962 : :E.:  : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE638 909997E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE: ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:  : 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 625663:E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] :
638 :] :eager release cuda mem 625663638] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638eager release cuda mem 625663638
] eager release cuda mem 625663:] 
] eager release cuda mem 625663
638eager release cuda mem 625663eager release cuda mem 625663
] 

eager release cuda mem 625663
[2022-12-12 05:23:10.928050: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 5
[2022-12-12 05:23:10.928215: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:23:10.928682: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 2
[2022-12-12 05:23:10.928844: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:23:10.929135: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:23:10.929603: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 1
[2022-12-12 05:23:10.929747: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[:2022-12-12 05:23:10638.] 929764eager release cuda mem 625663: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:23:10.929995: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 3
[2022-12-12 05:23:10.930157: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[[2022-12-12 05:23:102022-12-12 05:23:10..930191930194: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19261926] ] Device 4 init p2p of link 7Device 3 init p2p of link 0

[2022-12-12 05:23:10.930377: [E2022-12-12 05:23:10 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu930385:: 1980E]  eager alloc mem 611.00 KB/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
:1980] eager alloc mem 611.00 KB
[2022-12-12 05:23:10.930672: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:23:10.930756: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 4
[2022-12-12 05:23:10.930913: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:23:10.930935: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 6
[2022-12-12 05:23:10.931045: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:23:10.931111: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[[2022-12-12 05:23:102022-12-12 05:23:10..931315931320: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 625663eager release cuda mem 625663

[2022-12-12 05:23:10.931803: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:23:10.931961: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:23:10.945670: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 3
[[2022-12-12 05:23:102022-12-12 05:23:10..945775945789: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19261980] ] Device 6 init p2p of link 4eager alloc mem 611.00 KB

[2022-12-12 05:23:10.945948: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:23:10.946683: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 1
[2022-12-12 05:23:10.946749: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:23:10.946824: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:23:10.946852: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:23:10.947737: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:23:10.948043: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 6
[2022-12-12 05:23:10.948177: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:23:10.948330: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 0
[2022-12-12 05:23:10.948453: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:23:10.948516: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 5
[2022-12-12 05:23:10.948636: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:23:10.949019: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 2
[[2022-12-12 05:23:102022-12-12 05:23:10..949052949067: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::1926638] ] Device 5 init p2p of link 7eager release cuda mem 625663[

2022-12-12 05:23:10.949143: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:23:10.949238: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:23:10.949334: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:23:10.949466: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:23:10.949992: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:23:10.950124: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:23:10.969596: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 0
[2022-12-12 05:23:10.969710: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:23:10.969726: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 1
[2022-12-12 05:23:10.969844: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:23:10.969913: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 6
[2022-12-12 05:23:10.970030: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:23:10.970268: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 7
[2022-12-12 05:23:10.970381: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:23:10.970599: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:23:10.970729: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:23:10.970915: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:23:10.971263: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:23:10.971866: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 2
[2022-12-12 05:23:10.971990: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:23:10.972150: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 5
[2022-12-12 05:23:10.972275: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:23:10.972891: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:23:10.973161: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:23:10.974089: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 4
[2022-12-12 05:23:10.974215: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:23:10.975019: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[:2022-12-12 05:23:101926.] 975047Device 5 init p2p of link 3: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:23:10.975177: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 05:23:10.976006: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 05:23:10.993541: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 80400000
[2022-12-12 05:23:10.993849: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 80400000
[2022-12-12 05:23:10.993989: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 19912917 / 100000000 nodes ( 19.91 %~20.00 %) | remote 55616735 / 100000000 nodes ( 55.62 %) | cpu 24470348 / 100000000 nodes ( 24.47 %) | 9.50 GB | 3.43948 secs 
[2022-12-12 05:23:10.994338: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 19993944 / 100000000 nodes ( 19.99 %~20.00 %) | remote 55535708 / 100000000 nodes ( 55.54 %) | cpu 24470348 / 100000000 nodes ( 24.47 %) | 9.54 GB | 3.43894 secs 
[2022-12-12 05:23:10.995414: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 80400000
[2022-12-12 05:23:10.996322: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 80400000
[2022-12-12 05:23:10.996382: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 80400000
[2022-12-12 05:23:10.996523: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 80400000
[2022-12-12 05:23:10.997001: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 19945738 / 100000000 nodes ( 19.95 %~20.00 %) | remote 55583914 / 100000000 nodes ( 55.58 %) | cpu 24470348 / 100000000 nodes ( 24.47 %) | 9.52 GB | 3.44289 secs 
[2022-12-12 05:23:10.997346: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 80400000
[2022-12-12 05:23:10.997452: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 80400000
[2022-12-12 05:23:10.998369: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 19989703 / 100000000 nodes ( 19.99 %~20.00 %) | remote 55539949 / 100000000 nodes ( 55.54 %) | cpu 24470348 / 100000000 nodes ( 24.47 %) | 9.54 GB | 3.44386 secs 
[2022-12-12 05:23:10.998533: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 19994164 / 100000000 nodes ( 19.99 %~20.00 %) | remote 55535488 / 100000000 nodes ( 55.54 %) | cpu 24470348 / 100000000 nodes ( 24.47 %) | 9.54 GB | 3.44031 secs 
[2022-12-12 05:23:10.999550: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 19996166 / 100000000 nodes ( 20.00 %~20.00 %) | remote 55533486 / 100000000 nodes ( 55.53 %) | cpu 24470348 / 100000000 nodes ( 24.47 %) | 9.54 GB | 3.44143 secs 
[2022-12-12 05:23:11.   744: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 19999761 / 100000000 nodes ( 20.00 %~20.00 %) | remote 55529891 / 100000000 nodes ( 55.53 %) | cpu 24470348 / 100000000 nodes ( 24.47 %) | 9.54 GB | 3.44531 secs 
[2022-12-12 05:23:11.   931: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 19993010 / 100000000 nodes ( 19.99 %~20.00 %) | remote 55536642 / 100000000 nodes ( 55.54 %) | cpu 24470348 / 100000000 nodes ( 24.47 %) | 9.54 GB | 3.46221 secs 
[2022-12-12 05:23:11.  2915: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2213] before create ctx, mem is 16.18 GB
[2022-12-12 05:23:12.369470: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2219] after create ctx, mem is 16.44 GB
[2022-12-12 05:23:12.369866: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2226] after create stream, mem is 16.44 GB
[2022-12-12 05:23:12.370944: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2213] before create ctx, mem is 16.44 GB
[2022-12-12 05:23:13.738296: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2219] after create ctx, mem is 16.70 GB
[2022-12-12 05:23:13.738713: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2226] after create stream, mem is 16.70 GB
[2022-12-12 05:23:13.739172: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2213] before create ctx, mem is 16.70 GB
[2022-12-12 05:23:15.106738: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2219] after create ctx, mem is 16.92 GB
[2022-12-12 05:23:15.106892: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2226] after create stream, mem is 16.92 GB
[2022-12-12 05:23:15.107423: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2213] before create ctx, mem is 16.92 GB
[2022-12-12 05:23:16.351429: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2219] after create ctx, mem is 17.13 GB
[2022-12-12 05:23:16.352008: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2226] after create stream, mem is 17.13 GB
[2022-12-12 05:23:16.352756: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2243] before create stream, mem is 17.13 GB
[2022-12-12 05:23:16.353321: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2249] after create stream, mem is 17.13 GB
[2022-12-12 05:23:16.354839: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2213] before create ctx, mem is 17.13 GB
[2022-12-12 05:23:17.423824: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2219] after create ctx, mem is 17.33 GB
[2022-12-12 05:23:17.425765: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2226] after create stream, mem is 17.33 GB
[HCTR][05:23:18.595][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier
[HCTR][05:23:18.595][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier
[HCTR][05:23:18.595][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier
[HCTR][05:23:18.595][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier
[HCTR][05:23:18.595][ERROR][RK0][main]: replica 2 calling init per replica done, doing barrier
[HCTR][05:23:18.595][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier
[HCTR][05:23:18.595][ERROR][RK0][tid #140591375570688]: replica 0 calling init per replica done, doing barrier
[HCTR][05:23:18.595][ERROR][RK0][main]: replica 1 calling init per replica done, doing barrier
[HCTR][05:23:18.596][ERROR][RK0][main]: replica 1 calling init per replica done, doing barrier done
[HCTR][05:23:18.596][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier done
[HCTR][05:23:18.596][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier done
[HCTR][05:23:18.596][ERROR][RK0][main]: replica 2 calling init per replica done, doing barrier done
[HCTR][05:23:18.596][ERROR][RK0][tid #140591375570688]: replica 0 calling init per replica done, doing barrier done
[HCTR][05:23:18.596][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier done
[HCTR][05:23:18.596][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier done
[HCTR][05:23:18.596][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier done
[HCTR][05:23:18.596][ERROR][RK0][main]: init per replica done
[HCTR][05:23:18.596][ERROR][RK0][main]: init per replica done
[HCTR][05:23:18.596][ERROR][RK0][main]: init per replica done
[HCTR][05:23:18.596][ERROR][RK0][main]: init per replica done
[HCTR][05:23:18.596][ERROR][RK0][main]: init per replica done
[HCTR][05:23:18.596][ERROR][RK0][main]: init per replica done
[HCTR][05:23:18.596][ERROR][RK0][main]: init per replica done
[HCTR][05:23:18.598][ERROR][RK0][tid #140591375570688]: init per replica done
[HCTR][05:23:18.602][ERROR][RK0][main]: 5 allocated 3276800 at 0x7fcac7320000
[HCTR][05:23:18.602][ERROR][RK0][main]: 5 allocated 6553600 at 0x7fdf9f000000
[HCTR][05:23:18.602][ERROR][RK0][main]: 5 allocated 3276800 at 0x7fdf9f640000
[HCTR][05:23:18.602][ERROR][RK0][main]: 5 allocated 6553600 at 0x7fdf9f960000
[HCTR][05:23:18.602][ERROR][RK0][tid #140590360545024]: 3 allocated 3276800 at 0x7fcac7320000
[HCTR][05:23:18.602][ERROR][RK0][tid #140590360545024]: 3 allocated 6553600 at 0x7fdf9f000000
[HCTR][05:23:18.602][ERROR][RK0][tid #140590360545024]: 3 allocated 3276800 at 0x7fdf9f640000
[HCTR][05:23:18.602][ERROR][RK0][tid #140590360545024]: 3 allocated 6553600 at 0x7fdf9f960000
[HCTR][05:23:18.602][ERROR][RK0][tid #140590503155456]: 2 allocated 3276800 at 0x7fcacb320000
[HCTR][05:23:18.602][ERROR][RK0][tid #140590503155456]: 2 allocated 6553600 at 0x7fdf9f000000
[HCTR][05:23:18.602][ERROR][RK0][tid #140590503155456]: 2 allocated 3276800 at 0x7fdf9f640000
[HCTR][05:23:18.602][ERROR][RK0][tid #140590503155456]: 2 allocated 6553600 at 0x7fdf9f960000
[HCTR][05:23:18.602][ERROR][RK0][main]: 6 allocated 3276800 at 0x7fcacb320000
[HCTR][05:23:18.602][ERROR][RK0][main]: 6 allocated 6553600 at 0x7fdf9d000000
[HCTR][05:23:18.602][ERROR][RK0][tid #140590368937728]: 7 allocated 3276800 at 0x7fcac3320000
[HCTR][05:23:18.602][ERROR][RK0][main]: 6 allocated 3276800 at 0x7fdf9d640000
[HCTR][05:23:18.602][ERROR][RK0][main]: 6 allocated 6553600 at 0x7fdf9d960000
[HCTR][05:23:18.602][ERROR][RK0][tid #140590368937728]: 7 allocated 6553600 at 0x7fdf9f000000
[HCTR][05:23:18.602][ERROR][RK0][tid #140590368937728]: 7 allocated 3276800 at 0x7fdf9f640000
[HCTR][05:23:18.602][ERROR][RK0][main]: 1 allocated 3276800 at 0x7fcabf320000
[HCTR][05:23:18.602][ERROR][RK0][tid #140590368937728]: 7 allocated 6553600 at 0x7fdf9f960000
[HCTR][05:23:18.602][ERROR][RK0][main]: 1 allocated 6553600 at 0x7fdf9f000000
[HCTR][05:23:18.602][ERROR][RK0][main]: 1 allocated 3276800 at 0x7fdf9f640000
[HCTR][05:23:18.602][ERROR][RK0][main]: 1 allocated 6553600 at 0x7fdf9f960000
[HCTR][05:23:18.603][ERROR][RK0][tid #140590368937728]: 4 allocated 3276800 at 0x7fcab7320000
[HCTR][05:23:18.603][ERROR][RK0][tid #140590368937728]: 4 allocated 6553600 at 0x7fdf9f000000
[HCTR][05:23:18.603][ERROR][RK0][tid #140590368937728]: 4 allocated 3276800 at 0x7fdf9f640000
[HCTR][05:23:18.603][ERROR][RK0][tid #140590368937728]: 4 allocated 6553600 at 0x7fdf9f960000
[HCTR][05:23:18.605][ERROR][RK0][tid #140591375570688]: 0 allocated 3276800 at 0x7fc38e320000
[HCTR][05:23:18.605][ERROR][RK0][tid #140591375570688]: 0 allocated 6553600 at 0x7fdf9ec00000
[HCTR][05:23:18.605][ERROR][RK0][tid #140591375570688]: 0 allocated 3276800 at 0x7fc38ed0e800
[HCTR][05:23:18.605][ERROR][RK0][tid #140591375570688]: 0 allocated 6553600 at 0x7fc38f02e800








