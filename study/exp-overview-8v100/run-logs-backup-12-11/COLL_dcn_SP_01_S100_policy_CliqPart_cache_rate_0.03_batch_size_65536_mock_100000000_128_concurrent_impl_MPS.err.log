2022-12-11 23:15:54.012178: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.018524: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.024302: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.027834: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.035172: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.040219: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.053149: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.058645: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.110297: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.115906: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.121216: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.121714: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.123069: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.123714: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.131262: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.132829: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.135202: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.136482: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.136913: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.138211: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.138546: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.139804: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.140184: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.141436: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.141834: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.143065: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.143304: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.144650: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.145518: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.146377: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.147326: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.148377: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.150319: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.151332: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.152240: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.153146: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.154047: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.155199: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.156241: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.157283: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.162601: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 23:15:54.165429: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.167110: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.168334: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.168581: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.169949: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.170257: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.171803: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.171988: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.172252: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.174484: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.174521: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.174518: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.174658: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.176902: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.177114: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.177251: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.177251: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.179641: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.179850: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.179922: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.182259: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.182853: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.184860: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.185485: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.186608: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.187843: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.189095: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.189163: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.190745: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.192175: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.192477: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.193322: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.193963: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.195285: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.195872: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.196797: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.197477: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.198269: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.198895: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.199781: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.200266: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.201014: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.201621: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.202412: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.203071: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.203522: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.204182: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.207611: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.209390: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.209551: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.210554: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.211498: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.212261: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.212313: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.214322: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.214829: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.227249: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.230687: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.240936: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.248875: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.251634: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.251937: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.252000: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.252307: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.252337: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.253136: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.256596: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.256682: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.256699: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.256859: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.256904: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.257635: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.261102: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.262133: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.262234: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.262353: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.262403: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.265788: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.265843: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.265984: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.266006: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.266237: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.269945: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.270015: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.270100: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.270179: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.270276: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.273794: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.273894: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.273985: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.274027: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.274119: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.277340: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.277481: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.277612: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.277715: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.277746: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.281009: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.281129: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.281174: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.281211: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.281649: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.284263: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.284379: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.284417: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.284458: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.285451: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.288980: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.289059: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.289207: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.289255: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.290481: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.292750: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.292781: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.292819: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.292862: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.294181: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.296574: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.296614: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.296705: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.296763: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.297895: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.300164: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.300234: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.300257: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.300299: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.300397: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.301580: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.303995: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.304180: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.304195: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.304264: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.304426: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.305918: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.306840: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.307983: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.308053: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.308215: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.308261: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.308344: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.310418: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.311918: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.313718: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.313814: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.314724: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.314850: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.314853: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.316868: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.317909: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.317998: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.318578: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.318704: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.318789: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.320331: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 23:15:54.320713: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.321632: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.322214: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.322383: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.322469: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.322565: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.324299: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.325512: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.326118: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.326271: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.326409: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.326520: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.328207: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.329283: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.330271: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.330382: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.330428: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.330619: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.330730: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.333467: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.334564: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.335199: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.335476: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.335500: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.335614: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.335739: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.337307: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.338802: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.339550: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.339730: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.339778: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.339925: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.340726: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.341526: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.342861: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.343641: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.343764: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.343944: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.345145: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.345655: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.346895: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.347615: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.349424: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.350945: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.351756: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 23:15:54.351863: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.351945: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 23:15:54.352127: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 23:15:54.353361: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.354415: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.356250: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.356843: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.357954: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 23:15:54.358542: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.359361: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.360820: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.360923: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.360939: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.361028: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.363631: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.365793: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.365814: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.365867: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.365928: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.366396: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.368525: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.369995: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.370048: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.370237: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.370396: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.371469: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.372818: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.374561: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.375543: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.376620: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.410583: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.412582: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.416180: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.417753: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.422298: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.424025: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.457368: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.458375: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.462661: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.468893: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.469069: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 23:15:54.477587: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 23:15:54.478656: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.481991: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.486199: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.497612: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.499559: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:54.504086: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:55.490466: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:55.491320: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:55.492046: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:55.492519: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 23:15:55.492580: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-11 23:15:55.510397: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:55.511042: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:55.511726: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:55.512575: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:55.513337: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:55.513809: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-11 23:15:55.560440: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:15:55.560647: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:15:55.596535: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-11 23:15:55.734796: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:55.735428: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:55.735959: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:55.736647: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 23:15:55.736706: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-11 23:15:55.753322: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:55.754157: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:55.754856: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:55.755455: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:55.755993: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:55.756462: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-11 23:15:55.799120: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:55.799571: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:55.799872: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:55.800835: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:55.801355: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:55.801786: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:55.802323: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 23:15:55.802386: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-11 23:15:55.802612: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 23:15:55.802670: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-11 23:15:55.806559: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:55.807179: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:55.807709: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:55.808175: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 23:15:55.808226: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-11 23:15:55.813220: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:55.813836: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:55.814373: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:55.814832: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 23:15:55.814887: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-11 23:15:55.819487: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:55.820119: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:55.820622: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:55.820670: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:55.821734: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:55.821803: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:55.822744: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:55.822896: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:55.823634: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:55.823728: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-11 23:15:55.824212: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:55.825019: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-11 23:15:55.826583: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:55.827228: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:55.827751: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:55.828327: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:55.828830: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:55.829297: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-11 23:15:55.832709: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:55.833336: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:55.833841: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:55.834422: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:55.834932: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:55.835420: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-11 23:15:55.837497: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:15:55.837667: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:15:55.839510: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-11 23:15:55.858797: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:55.858802: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:55.860154: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:55.860251: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:55.861193: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:55.861418: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:55.862050: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 23:15:55.862111: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-11 23:15:55.862247: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 23:15:55.862304: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-11 23:15:55.870196: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:15:55.870375: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:15:55.872161: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
2022-12-11 23:15:55.873355: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:15:55.873533: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:15:55.875365: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
2022-12-11 23:15:55.878574: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:55.879232: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:55.879745: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:55.880324: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:55.880500: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:55.881311: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:55.881513: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:55.882191: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:15:55.882266: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-11 23:15:55.882333: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:15:55.882474: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:55.883067: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:55.883607: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:15:55.884083: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-11 23:15:55.884214: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
2022-12-11 23:15:55.915396: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:15:55.915593: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:15:55.917470: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
2022-12-11 23:15:55.927696: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:15:55.927869: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:15:55.929607: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
2022-12-11 23:15:55.929593: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:15:55.929770: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:15:55.931515: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
[HCTR][23:15:57.185][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][23:15:57.186][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][23:15:57.186][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][23:15:57.187][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][23:15:57.187][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][23:15:57.187][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][23:15:57.211][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][23:15:57.223][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:01,  1.58s/it]warmup run: 1it [00:01,  1.52s/it]warmup run: 97it [00:01, 80.16it/s]warmup run: 97it [00:01, 83.08it/s]warmup run: 1it [00:01,  1.51s/it]warmup run: 194it [00:01, 174.34it/s]warmup run: 1it [00:01,  1.49s/it]warmup run: 1it [00:01,  1.51s/it]warmup run: 195it [00:01, 181.07it/s]warmup run: 1it [00:01,  1.49s/it]warmup run: 101it [00:01, 87.14it/s]warmup run: 289it [00:01, 276.17it/s]warmup run: 99it [00:01, 86.28it/s]warmup run: 1it [00:01,  1.60s/it]warmup run: 1it [00:01,  1.48s/it]warmup run: 89it [00:01, 76.49it/s]warmup run: 294it [00:01, 290.11it/s]warmup run: 98it [00:01, 85.62it/s]warmup run: 202it [00:01, 188.52it/s]warmup run: 385it [00:01, 383.65it/s]warmup run: 197it [00:01, 185.30it/s]warmup run: 96it [00:01, 78.54it/s]warmup run: 100it [00:01, 88.00it/s]warmup run: 177it [00:01, 164.51it/s]warmup run: 392it [00:01, 401.29it/s]warmup run: 198it [00:01, 187.24it/s]warmup run: 302it [00:01, 298.38it/s]warmup run: 482it [00:02, 491.25it/s]warmup run: 297it [00:01, 296.70it/s]warmup run: 193it [00:01, 171.96it/s]warmup run: 200it [00:01, 190.12it/s]warmup run: 268it [00:01, 265.14it/s]warmup run: 491it [00:02, 510.69it/s]warmup run: 298it [00:01, 298.25it/s]warmup run: 400it [00:01, 408.27it/s]warmup run: 580it [00:02, 592.65it/s]warmup run: 397it [00:01, 411.32it/s]warmup run: 289it [00:01, 274.57it/s]warmup run: 300it [00:01, 301.46it/s]warmup run: 364it [00:01, 377.38it/s]warmup run: 591it [00:02, 613.02it/s]warmup run: 398it [00:01, 412.64it/s]warmup run: 494it [00:02, 507.16it/s]warmup run: 678it [00:02, 679.82it/s]warmup run: 497it [00:01, 522.62it/s]warmup run: 385it [00:01, 381.70it/s]warmup run: 400it [00:01, 415.94it/s]warmup run: 462it [00:02, 490.07it/s]warmup run: 691it [00:02, 700.81it/s]warmup run: 498it [00:01, 523.09it/s]warmup run: 594it [00:02, 610.39it/s]warmup run: 775it [00:02, 750.38it/s]warmup run: 599it [00:02, 627.31it/s]warmup run: 482it [00:02, 488.89it/s]warmup run: 500it [00:01, 526.67it/s]warmup run: 560it [00:02, 593.28it/s]warmup run: 790it [00:02, 770.74it/s]warmup run: 598it [00:02, 624.33it/s]warmup run: 694it [00:02, 700.15it/s]warmup run: 872it [00:02, 806.06it/s]warmup run: 700it [00:02, 714.89it/s]warmup run: 601it [00:02, 629.51it/s]warmup run: 659it [00:02, 684.55it/s]warmup run: 573it [00:02, 563.08it/s]warmup run: 890it [00:02, 829.13it/s]warmup run: 701it [00:02, 717.31it/s]warmup run: 794it [00:02, 773.84it/s]warmup run: 968it [00:02, 846.91it/s]warmup run: 800it [00:02, 785.16it/s]warmup run: 701it [00:02, 715.69it/s]warmup run: 757it [00:02, 757.61it/s]warmup run: 990it [00:02, 873.71it/s]warmup run: 661it [00:02, 625.35it/s]warmup run: 802it [00:02, 788.35it/s]warmup run: 895it [00:02, 833.79it/s]warmup run: 1065it [00:02, 878.89it/s]warmup run: 899it [00:02, 831.75it/s]warmup run: 800it [00:02, 783.29it/s]warmup run: 855it [00:02, 814.37it/s]warmup run: 1090it [00:02, 908.27it/s]warmup run: 760it [00:02, 712.37it/s]warmup run: 903it [00:02, 845.38it/s]warmup run: 994it [00:02, 875.91it/s]warmup run: 1161it [00:02, 900.25it/s]warmup run: 899it [00:02, 835.58it/s]warmup run: 997it [00:02, 865.39it/s]warmup run: 951it [00:02, 851.43it/s]warmup run: 1193it [00:02, 941.06it/s]warmup run: 851it [00:02, 749.77it/s]warmup run: 1004it [00:02, 888.67it/s]warmup run: 1094it [00:02, 909.14it/s]warmup run: 1257it [00:02, 914.99it/s]warmup run: 999it [00:02, 880.10it/s]warmup run: 1094it [00:02, 885.90it/s]warmup run: 1053it [00:02, 896.03it/s]warmup run: 1296it [00:02, 966.34it/s]warmup run: 949it [00:02, 810.09it/s]warmup run: 1105it [00:02, 921.40it/s]warmup run: 1193it [00:02, 931.79it/s]warmup run: 1354it [00:02, 930.83it/s]warmup run: 1100it [00:02, 916.15it/s]warmup run: 1190it [00:02, 900.06it/s]warmup run: 1155it [00:02, 929.66it/s]warmup run: 1397it [00:02, 973.71it/s]warmup run: 1041it [00:02, 831.14it/s]warmup run: 1207it [00:02, 947.87it/s]warmup run: 1292it [00:02, 945.69it/s]warmup run: 1450it [00:03, 933.89it/s]warmup run: 1203it [00:02, 946.29it/s]warmup run: 1256it [00:02, 952.01it/s]warmup run: 1286it [00:02, 900.84it/s]warmup run: 1500it [00:03, 987.73it/s]warmup run: 1132it [00:02, 843.03it/s]warmup run: 1308it [00:02, 963.18it/s]warmup run: 1391it [00:02, 952.07it/s]warmup run: 1546it [00:03, 928.39it/s]warmup run: 1304it [00:02, 964.07it/s]warmup run: 1357it [00:02, 968.56it/s]warmup run: 1602it [00:03, 995.68it/s]warmup run: 1380it [00:02, 901.51it/s]warmup run: 1222it [00:02, 855.54it/s]warmup run: 1409it [00:02, 962.39it/s]warmup run: 1490it [00:03, 962.15it/s]warmup run: 1641it [00:03, 929.76it/s]warmup run: 1406it [00:02, 979.00it/s]warmup run: 1703it [00:03, 998.39it/s]warmup run: 1473it [00:03, 909.25it/s]warmup run: 1457it [00:03, 933.96it/s]warmup run: 1321it [00:03, 892.74it/s]warmup run: 1508it [00:03, 961.05it/s]warmup run: 1590it [00:03, 970.91it/s]warmup run: 1736it [00:03, 934.66it/s]warmup run: 1507it [00:02, 983.38it/s]warmup run: 1804it [00:03, 1001.72it/s]warmup run: 1567it [00:03, 915.66it/s]warmup run: 1561it [00:03, 962.96it/s]warmup run: 1421it [00:03, 922.85it/s]warmup run: 1606it [00:03, 964.03it/s]warmup run: 1690it [00:03, 977.97it/s]warmup run: 1831it [00:03, 939.10it/s]warmup run: 1608it [00:03, 990.09it/s]warmup run: 1905it [00:03, 999.96it/s] warmup run: 1660it [00:03, 913.04it/s]warmup run: 1665it [00:03, 983.27it/s]warmup run: 1520it [00:03, 940.31it/s]warmup run: 1705it [00:03, 970.39it/s]warmup run: 1789it [00:03, 979.86it/s]warmup run: 1926it [00:03, 936.81it/s]warmup run: 1709it [00:03, 995.83it/s]warmup run: 2006it [00:03, 1000.90it/s]warmup run: 1753it [00:03, 912.08it/s]warmup run: 1769it [00:03, 997.14it/s]warmup run: 1621it [00:03, 959.96it/s]warmup run: 1807it [00:03, 984.82it/s]warmup run: 1888it [00:03, 979.46it/s]warmup run: 2025it [00:03, 951.18it/s]warmup run: 1810it [00:03, 997.61it/s]warmup run: 2127it [00:03, 1062.40it/s]warmup run: 1848it [00:03, 920.95it/s]warmup run: 1873it [00:03, 1008.49it/s]warmup run: 1725it [00:03, 981.98it/s]warmup run: 1910it [00:03, 995.68it/s]warmup run: 1987it [00:03, 977.52it/s]warmup run: 2145it [00:03, 1023.42it/s]warmup run: 1912it [00:03, 1003.11it/s]warmup run: 2248it [00:03, 1105.65it/s]warmup run: 1946it [00:03, 937.17it/s]warmup run: 1976it [00:03, 1013.95it/s]warmup run: 1829it [00:03, 998.44it/s]warmup run: 2014it [00:03, 1007.29it/s]warmup run: 2103it [00:03, 1029.30it/s]warmup run: 2267it [00:03, 1080.75it/s]warmup run: 2017it [00:03, 1016.29it/s]warmup run: 2370it [00:03, 1138.37it/s]warmup run: 2053it [00:03, 974.74it/s]warmup run: 2092it [00:03, 1055.91it/s]warmup run: 1933it [00:03, 1008.30it/s]warmup run: 2133it [00:03, 1060.54it/s]warmup run: 2223it [00:03, 1077.54it/s]warmup run: 2391it [00:03, 1127.09it/s]warmup run: 2136it [00:03, 1065.75it/s]warmup run: 2493it [00:03, 1163.38it/s]warmup run: 2172it [00:03, 1037.77it/s]warmup run: 2213it [00:03, 1100.88it/s]warmup run: 2041it [00:03, 1029.01it/s]warmup run: 2252it [00:03, 1096.75it/s]warmup run: 2343it [00:03, 1111.55it/s]warmup run: 2515it [00:04, 1158.17it/s]warmup run: 2255it [00:03, 1100.90it/s]warmup run: 2616it [00:04, 1180.61it/s]warmup run: 2291it [00:03, 1082.66it/s]warmup run: 2335it [00:03, 1135.03it/s]warmup run: 2163it [00:03, 1083.29it/s]warmup run: 2371it [00:03, 1122.82it/s]warmup run: 2462it [00:03, 1134.19it/s]warmup run: 2639it [00:04, 1181.37it/s]warmup run: 2374it [00:03, 1125.57it/s]warmup run: 2738it [00:04, 1191.90it/s]warmup run: 2410it [00:03, 1113.73it/s]warmup run: 2454it [00:03, 1148.71it/s]warmup run: 2284it [00:03, 1121.04it/s]warmup run: 2490it [00:03, 1140.37it/s]warmup run: 2581it [00:04, 1150.00it/s]warmup run: 2762it [00:04, 1194.20it/s]warmup run: 2492it [00:03, 1140.14it/s]warmup run: 2859it [00:04, 1194.79it/s]warmup run: 2529it [00:04, 1134.68it/s]warmup run: 2577it [00:04, 1170.87it/s]warmup run: 2405it [00:04, 1146.96it/s]warmup run: 2609it [00:04, 1153.33it/s]warmup run: 2701it [00:04, 1162.75it/s]warmup run: 2886it [00:04, 1205.52it/s]warmup run: 2610it [00:03, 1150.83it/s]warmup run: 2982it [00:04, 1203.84it/s]warmup run: 2648it [00:04, 1150.14it/s]warmup run: 2699it [00:04, 1184.99it/s]warmup run: 2526it [00:04, 1165.33it/s]warmup run: 3000it [00:04, 688.22it/s] warmup run: 2728it [00:04, 1161.84it/s]warmup run: 2819it [00:04, 1166.53it/s]warmup run: 3000it [00:04, 666.27it/s] warmup run: 2727it [00:04, 1156.16it/s]warmup run: 2765it [00:04, 1155.95it/s]warmup run: 2819it [00:04, 1187.84it/s]warmup run: 2646it [00:04, 1175.38it/s]warmup run: 2846it [00:04, 1165.37it/s]warmup run: 2939it [00:04, 1174.00it/s]warmup run: 2845it [00:04, 1162.29it/s]warmup run: 2884it [00:04, 1165.08it/s]warmup run: 2941it [00:04, 1197.42it/s]warmup run: 3000it [00:04, 684.14it/s] warmup run: 2764it [00:04, 1175.67it/s]warmup run: 2965it [00:04, 1172.06it/s]warmup run: 3000it [00:04, 682.17it/s] warmup run: 2963it [00:04, 1166.55it/s]warmup run: 3000it [00:04, 689.94it/s] warmup run: 3000it [00:04, 677.03it/s] warmup run: 2886it [00:04, 1187.17it/s]warmup run: 3000it [00:04, 693.45it/s] warmup run: 3000it [00:04, 660.55it/s] 

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]



warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   5%|▌         | 160/3000 [00:00<00:01, 1598.47it/s]warmup should be done:   5%|▌         | 163/3000 [00:00<00:01, 1626.12it/s]warmup should be done:   5%|▌         | 164/3000 [00:00<00:01, 1630.62it/s]warmup should be done:   5%|▌         | 152/3000 [00:00<00:01, 1516.15it/s]warmup should be done:   5%|▌         | 162/3000 [00:00<00:01, 1612.01it/s]warmup should be done:   5%|▌         | 162/3000 [00:00<00:01, 1612.37it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1650.61it/s]warmup should be done:   5%|▌         | 162/3000 [00:00<00:01, 1609.10it/s]warmup should be done:  11%|█         | 324/3000 [00:00<00:01, 1621.21it/s]warmup should be done:  11%|█         | 329/3000 [00:00<00:01, 1643.53it/s]warmup should be done:  11%|█         | 325/3000 [00:00<00:01, 1622.24it/s]warmup should be done:  11%|█         | 318/3000 [00:00<00:01, 1598.51it/s]warmup should be done:  11%|█         | 330/3000 [00:00<00:01, 1643.24it/s]warmup should be done:  11%|█         | 333/3000 [00:00<00:01, 1659.15it/s]warmup should be done:  11%|█         | 323/3000 [00:00<00:01, 1597.68it/s]warmup should be done:  11%|█         | 324/3000 [00:00<00:01, 1598.79it/s]warmup should be done:  16%|█▌        | 483/3000 [00:00<00:01, 1620.41it/s]warmup should be done:  16%|█▌        | 487/3000 [00:00<00:01, 1619.69it/s]warmup should be done:  16%|█▋        | 495/3000 [00:00<00:01, 1642.65it/s]warmup should be done:  17%|█▋        | 500/3000 [00:00<00:01, 1660.37it/s]warmup should be done:  16%|█▋        | 488/3000 [00:00<00:01, 1619.11it/s]warmup should be done:  16%|█▋        | 490/3000 [00:00<00:01, 1627.40it/s]warmup should be done:  16%|█▋        | 488/3000 [00:00<00:01, 1617.40it/s]warmup should be done:  16%|█▋        | 494/3000 [00:00<00:01, 1627.68it/s]warmup should be done:  22%|██▏       | 649/3000 [00:00<00:01, 1619.26it/s]warmup should be done:  22%|██▏       | 648/3000 [00:00<00:01, 1630.28it/s]warmup should be done:  22%|██▏       | 660/3000 [00:00<00:01, 1644.74it/s]warmup should be done:  22%|██▏       | 667/3000 [00:00<00:01, 1662.44it/s]warmup should be done:  22%|██▏       | 650/3000 [00:00<00:01, 1616.78it/s]warmup should be done:  22%|██▏       | 657/3000 [00:00<00:01, 1640.54it/s]warmup should be done:  22%|██▏       | 653/3000 [00:00<00:01, 1626.47it/s]warmup should be done:  22%|██▏       | 658/3000 [00:00<00:01, 1629.73it/s]warmup should be done:  27%|██▋       | 813/3000 [00:00<00:01, 1625.03it/s]warmup should be done:  28%|██▊       | 825/3000 [00:00<00:01, 1645.71it/s]warmup should be done:  27%|██▋       | 813/3000 [00:00<00:01, 1634.14it/s]warmup should be done:  28%|██▊       | 834/3000 [00:00<00:01, 1661.23it/s]warmup should be done:  27%|██▋       | 823/3000 [00:00<00:01, 1645.92it/s]warmup should be done:  27%|██▋       | 821/3000 [00:00<00:01, 1628.73it/s]warmup should be done:  27%|██▋       | 816/3000 [00:00<00:01, 1618.86it/s]warmup should be done:  27%|██▋       | 812/3000 [00:00<00:01, 1602.18it/s]warmup should be done:  33%|███▎      | 977/3000 [00:00<00:01, 1635.20it/s]warmup should be done:  33%|███▎      | 990/3000 [00:00<00:01, 1645.38it/s]warmup should be done:  33%|███▎      | 978/3000 [00:00<00:01, 1630.31it/s]warmup should be done:  33%|███▎      | 1001/3000 [00:00<00:01, 1659.03it/s]warmup should be done:  33%|███▎      | 989/3000 [00:00<00:01, 1648.80it/s]warmup should be done:  33%|███▎      | 984/3000 [00:00<00:01, 1627.24it/s]warmup should be done:  33%|███▎      | 980/3000 [00:00<00:01, 1624.65it/s]warmup should be done:  32%|███▏      | 973/3000 [00:00<00:01, 1602.84it/s]warmup should be done:  38%|███▊      | 1143/3000 [00:00<00:01, 1641.80it/s]warmup should be done:  38%|███▊      | 1142/3000 [00:00<00:01, 1629.49it/s]warmup should be done:  38%|███▊      | 1155/3000 [00:00<00:01, 1641.06it/s]warmup should be done:  38%|███▊      | 1154/3000 [00:00<00:01, 1648.23it/s]warmup should be done:  39%|███▉      | 1167/3000 [00:00<00:01, 1655.29it/s]warmup should be done:  38%|███▊      | 1147/3000 [00:00<00:01, 1624.08it/s]warmup should be done:  38%|███▊      | 1143/3000 [00:00<00:01, 1624.34it/s]warmup should be done:  38%|███▊      | 1134/3000 [00:00<00:01, 1599.75it/s]warmup should be done:  44%|████▎     | 1309/3000 [00:00<00:01, 1646.14it/s]warmup should be done:  44%|████▎     | 1306/3000 [00:00<00:01, 1630.54it/s]warmup should be done:  44%|████▍     | 1320/3000 [00:00<00:01, 1640.57it/s]warmup should be done:  44%|████▍     | 1320/3000 [00:00<00:01, 1650.71it/s]warmup should be done:  44%|████▍     | 1333/3000 [00:00<00:01, 1654.93it/s]warmup should be done:  44%|████▎     | 1310/3000 [00:00<00:01, 1623.95it/s]warmup should be done:  44%|████▎     | 1307/3000 [00:00<00:01, 1628.75it/s]warmup should be done:  43%|████▎     | 1295/3000 [00:00<00:01, 1600.29it/s]warmup should be done:  49%|████▉     | 1475/3000 [00:00<00:00, 1648.25it/s]warmup should be done:  49%|████▉     | 1471/3000 [00:00<00:00, 1633.90it/s]warmup should be done:  50%|████▉     | 1485/3000 [00:00<00:00, 1641.73it/s]warmup should be done:  50%|████▉     | 1486/3000 [00:00<00:00, 1650.90it/s]warmup should be done:  50%|████▉     | 1499/3000 [00:00<00:00, 1654.62it/s]warmup should be done:  49%|████▉     | 1473/3000 [00:00<00:00, 1623.59it/s]warmup should be done:  49%|████▉     | 1473/3000 [00:00<00:00, 1636.60it/s]warmup should be done:  49%|████▊     | 1456/3000 [00:00<00:00, 1601.88it/s]warmup should be done:  55%|█████▍    | 1641/3000 [00:01<00:00, 1649.62it/s]warmup should be done:  55%|█████▌    | 1652/3000 [00:01<00:00, 1648.81it/s]warmup should be done:  55%|█████▍    | 1635/3000 [00:01<00:00, 1630.17it/s]warmup should be done:  55%|█████▌    | 1652/3000 [00:01<00:00, 1651.21it/s]warmup should be done:  56%|█████▌    | 1665/3000 [00:01<00:00, 1653.55it/s]warmup should be done:  55%|█████▍    | 1637/3000 [00:01<00:00, 1636.34it/s]warmup should be done:  55%|█████▍    | 1636/3000 [00:01<00:00, 1622.78it/s]warmup should be done:  54%|█████▍    | 1617/3000 [00:01<00:00, 1603.66it/s]warmup should be done:  60%|██████    | 1807/3000 [00:01<00:00, 1650.09it/s]warmup should be done:  61%|██████    | 1819/3000 [00:01<00:00, 1653.57it/s]warmup should be done:  61%|██████    | 1831/3000 [00:01<00:00, 1654.99it/s]warmup should be done:  61%|██████    | 1818/3000 [00:01<00:00, 1652.01it/s]warmup should be done:  60%|█████▉    | 1799/3000 [00:01<00:00, 1627.29it/s]warmup should be done:  60%|██████    | 1801/3000 [00:01<00:00, 1635.65it/s]warmup should be done:  60%|█████▉    | 1799/3000 [00:01<00:00, 1622.86it/s]warmup should be done:  59%|█████▉    | 1778/3000 [00:01<00:00, 1603.57it/s]warmup should be done:  66%|██████▌   | 1973/3000 [00:01<00:00, 1648.59it/s]warmup should be done:  66%|██████▌   | 1986/3000 [00:01<00:00, 1656.59it/s]warmup should be done:  66%|██████▌   | 1984/3000 [00:01<00:00, 1651.33it/s]warmup should be done:  65%|██████▌   | 1962/3000 [00:01<00:00, 1624.72it/s]warmup should be done:  65%|██████▌   | 1962/3000 [00:01<00:00, 1623.54it/s]warmup should be done:  66%|██████▌   | 1965/3000 [00:01<00:00, 1633.14it/s]warmup should be done:  65%|██████▍   | 1939/3000 [00:01<00:00, 1601.55it/s]warmup should be done:  67%|██████▋   | 1997/3000 [00:01<00:00, 1635.64it/s]warmup should be done:  72%|███████▏  | 2153/3000 [00:01<00:00, 1658.66it/s]warmup should be done:  71%|███████▏  | 2138/3000 [00:01<00:00, 1643.78it/s]warmup should be done:  72%|███████▏  | 2150/3000 [00:01<00:00, 1651.25it/s]warmup should be done:  71%|███████   | 2125/3000 [00:01<00:00, 1623.35it/s]warmup should be done:  71%|███████   | 2125/3000 [00:01<00:00, 1623.37it/s]warmup should be done:  71%|███████   | 2129/3000 [00:01<00:00, 1632.89it/s]warmup should be done:  70%|███████   | 2100/3000 [00:01<00:00, 1601.93it/s]warmup should be done:  72%|███████▏  | 2161/3000 [00:01<00:00, 1630.53it/s]warmup should be done:  77%|███████▋  | 2320/3000 [00:01<00:00, 1660.13it/s]warmup should be done:  77%|███████▋  | 2303/3000 [00:01<00:00, 1641.25it/s]warmup should be done:  77%|███████▋  | 2316/3000 [00:01<00:00, 1648.95it/s]warmup should be done:  76%|███████▋  | 2293/3000 [00:01<00:00, 1632.13it/s]warmup should be done:  76%|███████▋  | 2288/3000 [00:01<00:00, 1620.88it/s]warmup should be done:  75%|███████▌  | 2261/3000 [00:01<00:00, 1601.65it/s]warmup should be done:  76%|███████▋  | 2288/3000 [00:01<00:00, 1604.47it/s]warmup should be done:  78%|███████▊  | 2325/3000 [00:01<00:00, 1628.28it/s]warmup should be done:  83%|████████▎ | 2487/3000 [00:01<00:00, 1658.90it/s]warmup should be done:  83%|████████▎ | 2481/3000 [00:01<00:00, 1646.70it/s]warmup should be done:  82%|████████▏ | 2468/3000 [00:01<00:00, 1635.17it/s]warmup should be done:  82%|████████▏ | 2457/3000 [00:01<00:00, 1628.93it/s]warmup should be done:  82%|████████▏ | 2451/3000 [00:01<00:00, 1617.71it/s]warmup should be done:  81%|████████  | 2422/3000 [00:01<00:00, 1597.65it/s]warmup should be done:  82%|████████▏ | 2450/3000 [00:01<00:00, 1608.30it/s]warmup should be done:  83%|████████▎ | 2488/3000 [00:01<00:00, 1623.43it/s]warmup should be done:  88%|████████▊ | 2654/3000 [00:01<00:00, 1659.68it/s]warmup should be done:  88%|████████▊ | 2647/3000 [00:01<00:00, 1648.07it/s]warmup should be done:  88%|████████▊ | 2632/3000 [00:01<00:00, 1634.37it/s]warmup should be done:  87%|████████▋ | 2621/3000 [00:01<00:00, 1629.34it/s]warmup should be done:  87%|████████▋ | 2614/3000 [00:01<00:00, 1619.00it/s]warmup should be done:  86%|████████▌ | 2582/3000 [00:01<00:00, 1598.19it/s]warmup should be done:  87%|████████▋ | 2613/3000 [00:01<00:00, 1614.71it/s]warmup should be done:  88%|████████▊ | 2651/3000 [00:01<00:00, 1621.88it/s]warmup should be done:  94%|█████████▍| 2821/3000 [00:01<00:00, 1660.04it/s]warmup should be done:  93%|█████████▎| 2796/3000 [00:01<00:00, 1635.09it/s]warmup should be done:  94%|█████████▍| 2813/3000 [00:01<00:00, 1648.94it/s]warmup should be done:  93%|█████████▎| 2785/3000 [00:01<00:00, 1630.46it/s]warmup should be done:  93%|█████████▎| 2777/3000 [00:01<00:00, 1619.35it/s]warmup should be done:  91%|█████████▏| 2743/3000 [00:01<00:00, 1598.93it/s]warmup should be done:  93%|█████████▎| 2777/3000 [00:01<00:00, 1619.51it/s]warmup should be done:  94%|█████████▍| 2815/3000 [00:01<00:00, 1627.16it/s]warmup should be done: 100%|█████████▉| 2989/3000 [00:01<00:00, 1665.60it/s]warmup should be done:  99%|█████████▉| 2963/3000 [00:01<00:00, 1644.13it/s]warmup should be done:  99%|█████████▉| 2980/3000 [00:01<00:00, 1653.62it/s]warmup should be done:  98%|█████████▊| 2952/3000 [00:01<00:00, 1641.16it/s]warmup should be done:  98%|█████████▊| 2942/3000 [00:01<00:00, 1626.62it/s]warmup should be done:  97%|█████████▋| 2906/3000 [00:01<00:00, 1605.67it/s]warmup should be done:  98%|█████████▊| 2943/3000 [00:01<00:00, 1629.43it/s]warmup should be done:  99%|█████████▉| 2981/3000 [00:01<00:00, 1634.12it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1652.94it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1646.69it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1642.94it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1637.31it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1630.25it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1624.20it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1623.36it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1604.71it/s]







warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|▌         | 171/3000 [00:00<00:01, 1708.86it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1679.54it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1676.44it/s]warmup should be done:   6%|▌         | 168/3000 [00:00<00:01, 1673.65it/s]warmup should be done:   6%|▌         | 169/3000 [00:00<00:01, 1683.00it/s]warmup should be done:   6%|▌         | 169/3000 [00:00<00:01, 1681.67it/s]warmup should be done:   6%|▌         | 169/3000 [00:00<00:01, 1681.93it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1660.63it/s]warmup should be done:  11%|█▏        | 342/3000 [00:00<00:01, 1709.30it/s]warmup should be done:  11%|█▏        | 338/3000 [00:00<00:01, 1689.47it/s]warmup should be done:  11%|█         | 336/3000 [00:00<00:01, 1677.34it/s]warmup should be done:  11%|█▏        | 338/3000 [00:00<00:01, 1683.20it/s]warmup should be done:  11%|█▏        | 338/3000 [00:00<00:01, 1680.19it/s]warmup should be done:  11%|█▏        | 338/3000 [00:00<00:01, 1677.58it/s]warmup should be done:  11%|█         | 334/3000 [00:00<00:01, 1652.70it/s]warmup should be done:  11%|█         | 336/3000 [00:00<00:01, 1643.69it/s]warmup should be done:  17%|█▋        | 514/3000 [00:00<00:01, 1712.00it/s]warmup should be done:  17%|█▋        | 505/3000 [00:00<00:01, 1679.73it/s]warmup should be done:  17%|█▋        | 509/3000 [00:00<00:01, 1694.96it/s]warmup should be done:  17%|█▋        | 508/3000 [00:00<00:01, 1688.39it/s]warmup should be done:  17%|█▋        | 507/3000 [00:00<00:01, 1680.22it/s]warmup should be done:  17%|█▋        | 507/3000 [00:00<00:01, 1679.32it/s]warmup should be done:  17%|█▋        | 500/3000 [00:00<00:01, 1645.64it/s]warmup should be done:  17%|█▋        | 501/3000 [00:00<00:01, 1620.61it/s]warmup should be done:  23%|██▎       | 686/3000 [00:00<00:01, 1713.28it/s]warmup should be done:  23%|██▎       | 679/3000 [00:00<00:01, 1695.88it/s]warmup should be done:  22%|██▎       | 675/3000 [00:00<00:01, 1684.79it/s]warmup should be done:  23%|██▎       | 679/3000 [00:00<00:01, 1694.01it/s]warmup should be done:  23%|██▎       | 678/3000 [00:00<00:01, 1689.01it/s]warmup should be done:  23%|██▎       | 677/3000 [00:00<00:01, 1683.62it/s]warmup should be done:  22%|██▏       | 666/3000 [00:00<00:01, 1649.86it/s]warmup should be done:  22%|██▏       | 666/3000 [00:00<00:01, 1628.75it/s]warmup should be done:  29%|██▊       | 859/3000 [00:00<00:01, 1716.23it/s]warmup should be done:  28%|██▊       | 849/3000 [00:00<00:01, 1694.36it/s]warmup should be done:  28%|██▊       | 844/3000 [00:00<00:01, 1683.17it/s]warmup should be done:  28%|██▊       | 850/3000 [00:00<00:01, 1697.72it/s]warmup should be done:  28%|██▊       | 847/3000 [00:00<00:01, 1686.39it/s]warmup should be done:  28%|██▊       | 833/3000 [00:00<00:01, 1656.71it/s]warmup should be done:  28%|██▊       | 847/3000 [00:00<00:01, 1678.97it/s]warmup should be done:  28%|██▊       | 835/3000 [00:00<00:01, 1647.89it/s]warmup should be done:  34%|███▍      | 1031/3000 [00:00<00:01, 1716.52it/s]warmup should be done:  34%|███▍      | 1020/3000 [00:00<00:01, 1697.53it/s]warmup should be done:  34%|███▍      | 1013/3000 [00:00<00:01, 1682.87it/s]warmup should be done:  34%|███▍      | 1019/3000 [00:00<00:01, 1686.13it/s]warmup should be done:  34%|███▍      | 1016/3000 [00:00<00:01, 1685.13it/s]warmup should be done:  33%|███▎      | 1000/3000 [00:00<00:01, 1660.20it/s]warmup should be done:  34%|███▍      | 1016/3000 [00:00<00:01, 1680.70it/s]warmup should be done:  33%|███▎      | 1003/3000 [00:00<00:01, 1657.30it/s]warmup should be done:  40%|████      | 1203/3000 [00:00<00:01, 1713.87it/s]warmup should be done:  40%|███▉      | 1190/3000 [00:00<00:01, 1695.79it/s]warmup should be done:  39%|███▉      | 1182/3000 [00:00<00:01, 1682.62it/s]warmup should be done:  40%|███▉      | 1185/3000 [00:00<00:01, 1682.53it/s]warmup should be done:  40%|███▉      | 1187/3000 [00:00<00:01, 1688.63it/s]warmup should be done:  39%|███▉      | 1167/3000 [00:00<00:01, 1659.41it/s]warmup should be done:  40%|███▉      | 1188/3000 [00:00<00:01, 1679.52it/s]warmup should be done:  39%|███▉      | 1172/3000 [00:00<00:01, 1666.17it/s]warmup should be done:  46%|████▌     | 1376/3000 [00:00<00:00, 1716.71it/s]warmup should be done:  45%|████▌     | 1361/3000 [00:00<00:00, 1699.49it/s]warmup should be done:  45%|████▌     | 1352/3000 [00:00<00:00, 1685.31it/s]warmup should be done:  44%|████▍     | 1333/3000 [00:00<00:01, 1659.37it/s]warmup should be done:  45%|████▌     | 1358/3000 [00:00<00:00, 1694.69it/s]warmup should be done:  45%|████▌     | 1355/3000 [00:00<00:00, 1685.49it/s]warmup should be done:  45%|████▌     | 1356/3000 [00:00<00:00, 1675.90it/s]warmup should be done:  45%|████▍     | 1342/3000 [00:00<00:00, 1675.45it/s]warmup should be done:  52%|█████▏    | 1548/3000 [00:00<00:00, 1716.93it/s]warmup should be done:  51%|█████     | 1531/3000 [00:00<00:00, 1699.17it/s]warmup should be done:  51%|█████     | 1521/3000 [00:00<00:00, 1684.06it/s]warmup should be done:  51%|█████     | 1528/3000 [00:00<00:00, 1696.23it/s]warmup should be done:  50%|████▉     | 1499/3000 [00:00<00:00, 1657.35it/s]warmup should be done:  51%|█████     | 1524/3000 [00:00<00:00, 1671.56it/s]warmup should be done:  51%|█████     | 1524/3000 [00:00<00:00, 1671.35it/s]warmup should be done:  50%|█████     | 1512/3000 [00:00<00:00, 1680.27it/s]warmup should be done:  57%|█████▋    | 1720/3000 [00:01<00:00, 1716.63it/s]warmup should be done:  57%|█████▋    | 1702/3000 [00:01<00:00, 1699.82it/s]warmup should be done:  56%|█████▋    | 1691/3000 [00:01<00:00, 1686.21it/s]warmup should be done:  57%|█████▋    | 1698/3000 [00:01<00:00, 1696.68it/s]warmup should be done:  56%|█████▌    | 1665/3000 [00:01<00:00, 1657.69it/s]warmup should be done:  56%|█████▋    | 1692/3000 [00:01<00:00, 1669.45it/s]warmup should be done:  56%|█████▋    | 1695/3000 [00:01<00:00, 1681.46it/s]warmup should be done:  56%|█████▌    | 1682/3000 [00:01<00:00, 1685.94it/s]warmup should be done:  63%|██████▎   | 1893/3000 [00:01<00:00, 1718.59it/s]warmup should be done:  62%|██████▏   | 1873/3000 [00:01<00:00, 1700.25it/s]warmup should be done:  62%|██████▏   | 1861/3000 [00:01<00:00, 1688.27it/s]warmup should be done:  62%|██████▏   | 1869/3000 [00:01<00:00, 1698.84it/s]warmup should be done:  61%|██████    | 1831/3000 [00:01<00:00, 1656.89it/s]warmup should be done:  62%|██████▏   | 1859/3000 [00:01<00:00, 1669.01it/s]warmup should be done:  62%|██████▏   | 1866/3000 [00:01<00:00, 1689.93it/s]warmup should be done:  62%|██████▏   | 1852/3000 [00:01<00:00, 1689.54it/s]warmup should be done:  69%|██████▉   | 2065/3000 [00:01<00:00, 1717.77it/s]warmup should be done:  68%|██████▊   | 2044/3000 [00:01<00:00, 1700.13it/s]warmup should be done:  68%|██████▊   | 2031/3000 [00:01<00:00, 1691.38it/s]warmup should be done:  68%|██████▊   | 2040/3000 [00:01<00:00, 1699.64it/s]warmup should be done:  67%|██████▋   | 1997/3000 [00:01<00:00, 1652.60it/s]warmup should be done:  68%|██████▊   | 2026/3000 [00:01<00:00, 1665.60it/s]warmup should be done:  68%|██████▊   | 2036/3000 [00:01<00:00, 1685.28it/s]warmup should be done:  67%|██████▋   | 2022/3000 [00:01<00:00, 1689.89it/s]warmup should be done:  75%|███████▍  | 2237/3000 [00:01<00:00, 1716.45it/s]warmup should be done:  73%|███████▎  | 2201/3000 [00:01<00:00, 1688.06it/s]warmup should be done:  74%|███████▍  | 2215/3000 [00:01<00:00, 1693.97it/s]warmup should be done:  74%|███████▎  | 2210/3000 [00:01<00:00, 1695.13it/s]warmup should be done:  72%|███████▏  | 2163/3000 [00:01<00:00, 1651.95it/s]warmup should be done:  73%|███████▎  | 2193/3000 [00:01<00:00, 1663.15it/s]warmup should be done:  73%|███████▎  | 2191/3000 [00:01<00:00, 1689.62it/s]warmup should be done:  74%|███████▎  | 2205/3000 [00:01<00:00, 1680.85it/s]warmup should be done:  80%|████████  | 2409/3000 [00:01<00:00, 1714.72it/s]warmup should be done:  79%|███████▉  | 2371/3000 [00:01<00:00, 1690.79it/s]warmup should be done:  78%|███████▊  | 2329/3000 [00:01<00:00, 1653.77it/s]warmup should be done:  79%|███████▉  | 2380/3000 [00:01<00:00, 1692.66it/s]warmup should be done:  80%|███████▉  | 2385/3000 [00:01<00:00, 1687.66it/s]warmup should be done:  79%|███████▊  | 2360/3000 [00:01<00:00, 1663.46it/s]warmup should be done:  79%|███████▊  | 2361/3000 [00:01<00:00, 1691.30it/s]warmup should be done:  79%|███████▉  | 2374/3000 [00:01<00:00, 1680.12it/s]warmup should be done:  86%|████████▌ | 2581/3000 [00:01<00:00, 1715.33it/s]warmup should be done:  85%|████████▍ | 2544/3000 [00:01<00:00, 1701.40it/s]warmup should be done:  83%|████████▎ | 2495/3000 [00:01<00:00, 1655.26it/s]warmup should be done:  85%|████████▌ | 2550/3000 [00:01<00:00, 1693.48it/s]warmup should be done:  85%|████████▌ | 2554/3000 [00:01<00:00, 1687.01it/s]warmup should be done:  84%|████████▍ | 2527/3000 [00:01<00:00, 1665.24it/s]warmup should be done:  84%|████████▍ | 2531/3000 [00:01<00:00, 1692.29it/s]warmup should be done:  85%|████████▍ | 2543/3000 [00:01<00:00, 1680.92it/s]warmup should be done:  92%|█████████▏| 2753/3000 [00:01<00:00, 1716.45it/s]warmup should be done:  91%|█████████ | 2717/3000 [00:01<00:00, 1709.48it/s]warmup should be done:  89%|████████▊ | 2661/3000 [00:01<00:00, 1655.38it/s]warmup should be done:  91%|█████████ | 2720/3000 [00:01<00:00, 1694.10it/s]warmup should be done:  91%|█████████ | 2724/3000 [00:01<00:00, 1688.22it/s]warmup should be done:  90%|████████▉ | 2695/3000 [00:01<00:00, 1667.50it/s]warmup should be done:  90%|█████████ | 2701/3000 [00:01<00:00, 1693.40it/s]warmup should be done:  90%|█████████ | 2712/3000 [00:01<00:00, 1680.09it/s]warmup should be done:  98%|█████████▊| 2925/3000 [00:01<00:00, 1717.37it/s]warmup should be done:  96%|█████████▋| 2889/3000 [00:01<00:00, 1712.33it/s]warmup should be done:  94%|█████████▍| 2830/3000 [00:01<00:00, 1665.28it/s]warmup should be done:  96%|█████████▋| 2890/3000 [00:01<00:00, 1693.32it/s]warmup should be done:  96%|█████████▋| 2893/3000 [00:01<00:00, 1687.09it/s]warmup should be done:  95%|█████████▌| 2864/3000 [00:01<00:00, 1671.58it/s]warmup should be done:  96%|█████████▌| 2871/3000 [00:01<00:00, 1694.72it/s]warmup should be done:  96%|█████████▌| 2881/3000 [00:01<00:00, 1677.88it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1715.84it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1693.88it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1692.15it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1691.73it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1681.19it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1678.95it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1675.20it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1675.62it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1659.49it/s]
WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fec39ad6190>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fec39ad82b0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fec39ae51c0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fec39ae60d0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fec39e0cd30>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fec39e09e80>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fec39ad71f0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fec39e0a730>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
2022-12-11 23:17:26.851362: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fe767029f30 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 23:17:26.851426: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 23:17:26.861006: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 23:17:27.528431: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fe76b02dbd0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 23:17:27.528495: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 23:17:27.534858: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fe772831c60 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 23:17:27.534913: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 23:17:27.536684: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 23:17:27.545050: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 23:17:27.699554: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fe7768bde00 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 23:17:27.699621: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 23:17:27.707632: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 23:17:27.769771: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fe772835110 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 23:17:27.769840: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 23:17:27.770544: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fe77282d3e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 23:17:27.770604: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 23:17:27.777831: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 23:17:27.780598: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 23:17:27.790084: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fe76b032290 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 23:17:27.790145: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 23:17:27.798481: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 23:17:27.811586: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fe77215f630 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 23:17:27.811652: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 23:17:27.821365: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 23:17:34.146443: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 23:17:34.418209: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 23:17:34.424314: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 23:17:34.455876: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 23:17:34.597932: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 23:17:34.620887: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 23:17:34.687120: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 23:17:34.758491: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][23:18:34.340][ERROR][RK0][main]: replica 4 reaches 1000, calling init pre replica
[HCTR][23:18:34.341][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][23:18:34.346][ERROR][RK0][main]: coll ps creation done
[HCTR][23:18:34.346][ERROR][RK0][main]: replica 4 waits for coll ps creation barrier
[HCTR][23:18:34.398][ERROR][RK0][main]: replica 2 reaches 1000, calling init pre replica
[HCTR][23:18:34.398][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][23:18:34.406][ERROR][RK0][main]: coll ps creation done
[HCTR][23:18:34.406][ERROR][RK0][main]: replica 2 waits for coll ps creation barrier
[HCTR][23:18:34.454][ERROR][RK0][tid #140632186152704]: replica 3 reaches 1000, calling init pre replica
[HCTR][23:18:34.454][ERROR][RK0][tid #140632186152704]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][23:18:34.457][ERROR][RK0][tid #140632723023616]: replica 6 reaches 1000, calling init pre replica
[HCTR][23:18:34.458][ERROR][RK0][tid #140632723023616]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][23:18:34.459][ERROR][RK0][tid #140632186152704]: coll ps creation done
[HCTR][23:18:34.459][ERROR][RK0][tid #140632186152704]: replica 3 waits for coll ps creation barrier
[HCTR][23:18:34.459][ERROR][RK0][tid #140632194545408]: replica 5 reaches 1000, calling init pre replica
[HCTR][23:18:34.460][ERROR][RK0][tid #140632194545408]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][23:18:34.464][ERROR][RK0][tid #140632723023616]: coll ps creation done
[HCTR][23:18:34.464][ERROR][RK0][tid #140632723023616]: replica 6 waits for coll ps creation barrier
[HCTR][23:18:34.464][ERROR][RK0][tid #140632194545408]: coll ps creation done
[HCTR][23:18:34.464][ERROR][RK0][tid #140632194545408]: replica 5 waits for coll ps creation barrier
[HCTR][23:18:34.546][ERROR][RK0][main]: replica 0 reaches 1000, calling init pre replica
[HCTR][23:18:34.546][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][23:18:34.548][ERROR][RK0][main]: replica 1 reaches 1000, calling init pre replica
[HCTR][23:18:34.548][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][23:18:34.551][ERROR][RK0][main]: coll ps creation done
[HCTR][23:18:34.551][ERROR][RK0][main]: replica 0 waits for coll ps creation barrier
[HCTR][23:18:34.552][ERROR][RK0][main]: coll ps creation done
[HCTR][23:18:34.552][ERROR][RK0][main]: replica 1 waits for coll ps creation barrier
[HCTR][23:18:34.617][ERROR][RK0][main]: replica 7 reaches 1000, calling init pre replica
[HCTR][23:18:34.617][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][23:18:34.624][ERROR][RK0][main]: coll ps creation done
[HCTR][23:18:34.624][ERROR][RK0][main]: replica 7 waits for coll ps creation barrier
[HCTR][23:18:34.624][ERROR][RK0][main]: replica 0 preparing frequency
[HCTR][23:18:35.477][ERROR][RK0][main]: replica 0 preparing frequency done
[HCTR][23:18:35.516][ERROR][RK0][main]: replica 0 calling init per replica
[HCTR][23:18:35.516][ERROR][RK0][main]: replica 7 calling init per replica
[HCTR][23:18:35.516][ERROR][RK0][tid #140632186152704]: replica 3 calling init per replica
[HCTR][23:18:35.516][ERROR][RK0][tid #140632194545408]: replica 5 calling init per replica
[HCTR][23:18:35.516][ERROR][RK0][main]: replica 2 calling init per replica
[HCTR][23:18:35.516][ERROR][RK0][main]: replica 1 calling init per replica
[HCTR][23:18:35.516][ERROR][RK0][main]: replica 4 calling init per replica
[HCTR][23:18:35.516][ERROR][RK0][tid #140632723023616]: replica 6 calling init per replica
[HCTR][23:18:35.516][ERROR][RK0][main]: Calling build_v2
[HCTR][23:18:35.516][ERROR][RK0][main]: Calling build_v2
[HCTR][23:18:35.516][ERROR][RK0][tid #140632186152704]: Calling build_v2
[HCTR][23:18:35.516][ERROR][RK0][tid #140632194545408]: Calling build_v2
[HCTR][23:18:35.516][ERROR][RK0][main]: Calling build_v2
[HCTR][23:18:35.516][ERROR][RK0][main]: Calling build_v2
[HCTR][23:18:35.516][ERROR][RK0][main]: Calling build_v2
[HCTR][23:18:35.516][ERROR][RK0][tid #140632723023616]: Calling build_v2
[HCTR][23:18:35.516][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][23:18:35.516][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][23:18:35.516][ERROR][RK0][tid #140632186152704]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][23:18:35.516][ERROR][RK0][tid #140632194545408]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][23:18:35.516][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][23:18:35.516][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][23:18:35.516][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][23:18:35.516][ERROR][RK0][tid #140632723023616]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[[[[[[[2022-12-11 23:18:352022-12-11 23:18:352022-12-11 23:18:352022-12-11 23:18:352022-12-11 23:18:35.2022-12-11 23:18:35.2022-12-11 23:18:35...516646.516646.516652516652516652: 516652: 516652: : : E[: E: EEE E E   /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc 2022-12-11 23:18:35/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc.:::136:136:516758136136136] 136] 136: ] ] ] using concurrent impl MPS] using concurrent impl MPS] Eusing concurrent impl MPS using concurrent impl MPSusing concurrent impl MPS
using concurrent impl MPS
using concurrent impl MPS
/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc



:136] using concurrent impl MPS
[2022-12-11 23:18:35.521042: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-11 23:18:35.521079: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[:2022-12-11 23:18:35196.] 521086assigning 8 to cpu: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-11 23:18:35.521135[: 2022-12-11 23:18:35E. 521136/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[: :2022-12-11 23:18:35E196. [] 521164/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 23:18:35assigning 8 to cpu: :.
E178521182 [] : /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 23:18:35v100x8, slow pcieE:.
 212[521227/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] [[2022-12-11 23:18:35: :build asymm link desc with 8X Tesla V100-SXM2-32GB out of 82022-12-11 23:18:352022-12-11 23:18:35.E178
..521265 [] 521277521273[: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 23:18:35v100x8, slow pcie: : 2022-12-11 23:18:35E:.
EE[. 178521320  [2022-12-11 23:18:35521350/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] : /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 23:18:35.: :v100x8, slow pcieE::.521376E212
 196178521404:  ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] [] : E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccbuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8:assigning 8 to cpu2022-12-11 23:18:35v100x8, slow pcieE :/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
178
.
 213:] 521489/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] [[178v100x8, slow pcie: :remote time is 8.684212022-12-11 23:18:352022-12-11 23:18:35] 
E196[
..v100x8, slow pcie ] [2022-12-11 23:18:35521565521564[
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccassigning 8 to cpu2022-12-11 23:18:35.: : 2022-12-11 23:18:35:
.521603E[E.196521629:  2022-12-11 23:18:35 521642] : E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: assigning 8 to cpuE [:521678:E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 23:18:35196: 213 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:.] E] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:212521739assigning 8 to cpu remote time is 8.68421:196] : 
[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
:214] build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8E2022-12-11 23:18:35196] assigning 8 to cpu[
 .] cpu time is 97.0588[
2022-12-11 23:18:35/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc521830assigning 8 to cpu[
2022-12-11 23:18:35.:: 
2022-12-11 23:18:35.521866212E.[521883: ]  5219102022-12-11 23:18:35: Ebuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: .E 
:E521968 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc212[ : /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:[] 2022-12-11 23:18:35/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE:2142022-12-11 23:18:35build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8.: 212] .
522024213/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] cpu time is 97.0588522037: ] [:build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8
: Eremote time is 8.684212022-12-11 23:18:35212
E 
.]  /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[522127build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:2022-12-11 23:18:35: 
2022-12-11 23:18:35:212.E.213[2022-12-11 23:18:35] 522192 522198] .: build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: remote time is 8.68421522242E
:E
:  213 [E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 23:18:35 [:2022-12-11 23:18:35remote time is 8.68421:./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc213.
214522316:] 522318] : 213[remote time is 8.68421: cpu time is 97.0588EE] 2022-12-11 23:18:35

  remote time is 8.68421.[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
5223782022-12-11 23:18:35::: .[213214E5224282022-12-11 23:18:35] ]  : .remote time is 8.68421cpu time is 97.0588/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE522470

: : 214/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE] : [cpu time is 97.0588214/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 23:18:35
] :.cpu time is 97.0588214522552
] : cpu time is 97.0588E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:214] cpu time is 97.0588
[2022-12-11 23:19:54.900452: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-11 23:19:54.940562: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
block 0 storage is 00010001
	access is	0	0	0	0	4	4	4	4	
block 1 storage is 00100010
	access is	1	1	1	1	5	5	5	5	
block 2 storage is 01000100
	access is	2	2	2	2	6	6	6	6	
block 3 storage is 10001000
	access is	3	3	3	3	7	7	7	7	
block 4 storage is 00000000
	access is	8	8	8	8	8	8	8	8	
[2022-12-11 23:19:55. 50350: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-11 23:19:55. 50415: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-11 23:19:55.203214: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-11 23:19:55.203252: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-11 23:19:55.203813: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 23:19:55.203862: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:19:55.204787: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:19:55.205554: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:19:55.218447: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 3 solved
[2022-12-11 23:19:55.218522: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 3 initing device 3
[2022-12-11 23:19:55.218814: [E 2022-12-11 23:19:55/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc.:218837202: ] E2 solved 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] [7 solved2022-12-11 23:19:55
.218894: E[ 2022-12-11 23:19:55/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc.:218911205: ] Eworker 0 thread 2 initing device 2 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 7 initing device 7
[2022-12-11 23:19:55.218972: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 23:19:55.219023: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:19:55.219109: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202[] 2022-12-11 23:19:556 solved.
219144: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc2022-12-11 23:19:55:.202219184] : 5 solvedE
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205[] 2022-12-11 23:19:55worker 0 thread 6 initing device 6.
219218: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 5 initing device 5
[2022-12-11 23:19:55.219340: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[:2022-12-11 23:19:551815.] 219357Building Coll Cache with ... num gpu device is 8: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 23:19:55.219398[: 2022-12-11 23:19:55E. 219414/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: :E1980 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 381.47 MB:
1980] eager alloc mem 381.47 MB
[2022-12-11 23:19:55.219621: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8[
2022-12-11 23:19:55.219651: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] [Building Coll Cache with ... num gpu device is 82022-12-11 23:19:55
.219676: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] [eager alloc mem 381.47 MB2022-12-11 23:19:55
.219700: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:19:55.220916: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 1 solved
[2022-12-11 23:19:55.220969[: 2022-12-11 23:19:55E. 220962/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc: :E205 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.ccworker 0 thread 1 initing device 1:
202] 4 solved
[2022-12-11 23:19:55.221065: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 4 initing device 4
[2022-12-11 23:19:55.221434: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 23:19:55.221477: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:[19802022-12-11 23:19:55] .eager alloc mem 381.47 MB221490
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 23:19:55.221551: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:19:55.223216: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:19:55.223606: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:19:55.223665: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:19:55.224203: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:19:55.224432: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:19:55.225910: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:19:55.225970: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:19:55.227857: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:19:55.228136: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:19:55.228242: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:19:55.228294: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:19:55.228938: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:19:55.230418: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:19:55.230456: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:19:55.280830: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.00 Bytes
[2022-12-11 23:19:55.285823: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-11 23:19:55.285906: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 23:19:55.286674: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.83 MB
[2022-12-11 23:19:55.287210: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:19:55.288201: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:19:55.288246: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 11.44 MB
[2022-12-11 23:19:55.291198: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 23:19:55.291929: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 23:19:55.291974: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.44 GB
[2022-12-11 23:19:55.311855: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.00 Bytes
[2022-12-11 23:19:55.316530: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-11 23:19:55.316611: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 23:19:55.317452: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.83 MB
[2022-12-11 23:19:55.318075: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:19:55.319058: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:19:55.319101: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 11.44 MB
[2022-12-11 23:19:55.320681: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.00 Bytes
[[[[2022-12-11 23:19:552022-12-11 23:19:552022-12-11 23:19:552022-12-11 23:19:55....320754320754320754[320754: : : 2022-12-11 23:19:55: EEE.E   320805 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:::E:198019801980 1980] ] ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] eager alloc mem 5.00 Byteseager alloc mem 5.00 Byteseager alloc mem 5.00 Bytes:eager alloc mem 5.00 Bytes


1980
] eager alloc mem 5.00 Bytes
[2022-12-11 23:19:55.327180: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-11 23:19:55[.2022-12-11 23:19:55327272.: 327265E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :[/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu6382022-12-11 23:19:55:] .1980eager release cuda mem 400000000327309] 
: eager alloc mem 25.25 KBE
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-11 23:19:55.327393: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-11 23:19:55.327434: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 23:19:55.327468: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 23:19:55.327496: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[[2022-12-11 23:19:552022-12-11 23:19:55..327574327563: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 400000000eager release cuda mem 5

[2022-12-11 23:19:55[.2022-12-11 23:19:55327647.: 327667E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638:] 638eager release cuda mem 5] 
eager release cuda mem 400000000
[2022-12-11 23:19:55.327756: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 23:19:55.328062: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 23:19:55.328108: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.44 GB
[2022-12-11 23:19:55.328462: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.83 MB
[2022-12-11 23:19:55.329225: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.83 MB
[2022-12-11 23:19:55.331924: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.83 MB
[2022-12-11 23:19:55.332660: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.83 MB
[2022-12-11 23:19:55.333278: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.83 MB
[2022-12-11 23:19:55.333901: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.83 MB
[2022-12-11 23:19:55.334801: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:19:55.334999: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:19:55.335265: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:19:55.335372: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:19:55.335471: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:19:55.335525: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:19:55.335790: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:19:55.335835: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 11.44 MB
[2022-12-11 23:19:55.335963: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:19:55.336014: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 11.44 MB
[2022-12-11 23:19:55.336233: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:19:55.336281: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 11.44 MB
[2022-12-11 23:19:55.336351: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:19:55.336396: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 11.44 MB
[2022-12-11 23:19:55.336448: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:19:55.[3364952022-12-11 23:19:55: .W336497 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.ccE: 43/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] :WORKER[0] alloc host memory 11.44 MB638
] eager release cuda mem 625663
[2022-12-11 23:19:55.336560: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 11.44 MB
[2022-12-11 23:19:55.339531: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 23:19:55.340264: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 23:19:55.340308: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.44 GB
[[2022-12-11 23:19:552022-12-11 23:19:55..344408344416: : EE  [/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-11 23:19:55::.19801980344443] ] : eager alloc mem 25.25 KBeager alloc mem 25.25 KBE

 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 23:19:55.344960: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 23:19:55.345055: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638[] 2022-12-11 23:19:55eager release cuda mem 25855.
345073: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 23:19:55.345106: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980[] 2022-12-11 23:19:55eager alloc mem 1.44 GB.
345124: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.44 GB
[2022-12-11 23:19:55.345167: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 23:19:55.345212: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.44 GB
[2022-12-11 23:19:55.345564: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 23:19:55.345606: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.44 GB
[2022-12-11 23:19:55.345679: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 23:19:55.346287: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 23:19:55.346328: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.44 GB
[[[[[[[[2022-12-11 23:19:552022-12-11 23:19:552022-12-11 23:19:552022-12-11 23:19:552022-12-11 23:19:552022-12-11 23:19:552022-12-11 23:19:552022-12-11 23:19:55........881016881013881013881013881013881013881017881014: : : : : : : : EEEEEEEE        /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::::::19261926192619261926192619261926] ] ] ] ] ] ] ] Device 0 init p2p of link 3Device 6 init p2p of link 0Device 3 init p2p of link 2Device 2 init p2p of link 1Device 5 init p2p of link 6Device 7 init p2p of link 4Device 4 init p2p of link 5Device 1 init p2p of link 7







[[[[2022-12-11 23:19:55[2022-12-11 23:19:55[2022-12-11 23:19:55[2022-12-11 23:19:55.2022-12-11 23:19:55.[2022-12-11 23:19:55.2022-12-11 23:19:55.881573.8815732022-12-11 23:19:55.881573.881573: 881579: .881580: 881601: E: E881592: E: E E : E E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980:1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980:1980] 1980] :1980] 1980] eager alloc mem 611.00 KB] eager alloc mem 611.00 KB1980] eager alloc mem 611.00 KB] eager alloc mem 611.00 KB
eager alloc mem 611.00 KB
] eager alloc mem 611.00 KB
eager alloc mem 611.00 KB

eager alloc mem 611.00 KB


[[2022-12-11 23:19:552022-12-11 23:19:55[[..2022-12-11 23:19:552022-12-11 23:19:55882615882620.[.: : [8826232022-12-11 23:19:55882624EE2022-12-11 23:19:55: .:   [.E882639E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-11 23:19:55882646 :  :2022-12-11 23:19:55:.: /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638.638882666E: :] 882677] :  638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638eager release cuda mem 625663: eager release cuda mem 625663E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] :] 
E
 :eager release cuda mem 625663638eager release cuda mem 625663 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638
] 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:] eager release cuda mem 625663:638eager release cuda mem 625663
638] 
] eager release cuda mem 625663eager release cuda mem 625663

[2022-12-11 23:19:55.896006: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 5
[2022-12-11 23:19:55.896162: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:19:55.896961: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:[19262022-12-11 23:19:55] .Device 1 init p2p of link 2896989
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:19:55.897046: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 1
[2022-12-11 23:19:55.897105: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:19:55.897205: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:19:55.897402: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 4
[2022-12-11 23:19:55.897560: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:19:55.897635: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 0
[2022-12-11 23:19:55.897780: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:19:55.897928: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] [eager release cuda mem 6256632022-12-11 23:19:55
.897939: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926[] 2022-12-11 23:19:55Device 2 init p2p of link 3.
897975: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:19:55.898084: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[[2022-12-11 23:19:552022-12-11 23:19:55..898370898383: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::1926638] ] Device 4 init p2p of link 7eager release cuda mem 625663

[[2022-12-11 23:19:552022-12-11 23:19:55..898559898575: : E[E 2022-12-11 23:19:55 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:898593:1926: 1980] E] Device 0 init p2p of link 6 eager alloc mem 611.00 KB
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
:638] eager release cuda mem 625663
[2022-12-11 23:19:55.898796: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:19:55.898879: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:19:55.899465: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:19:55.899571: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:19:55.910336: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 6
[2022-12-11 23:19:55.910457: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:19:55.910655: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 4
[2022-12-11 23:19:55.910773: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[[2022-12-11 23:19:552022-12-11 23:19:55..911259911253: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::6381926] ] eager release cuda mem 625663Device 1 init p2p of link 3

[2022-12-11 23:19:55.911392: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:19:55.911566: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:19:55.911599: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 1
[2022-12-11 23:19:55.911721: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:19:55.911918: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 7
[2022-12-11 23:19:55.912044: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:19:55.912109: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 5
[2022-12-11 23:19:55.912186: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:19:55.912225: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:19:55.912288: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 0
[2022-12-11 23:19:55.912403: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:19:55.912531: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:19:55.912583: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 2
[2022-12-11 23:19:55.912701: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:19:55.912839: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:19:55.912989: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:19:55.913199: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:19:55.913466: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:19:55.927930: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 1
[2022-12-11 23:19:55.928045: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:19:55.928095: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 5
[2022-12-11 23:19:55.928210: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:19:55.928609: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 6
[2022-12-11 23:19:55.928727: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:19:55.928857: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:19:55.928938: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 7
[2022-12-11 23:19:55.929016: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:19:55.929050: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:19:55.929405: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 0
[2022-12-11 23:19:55.929517: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:19:55.929538: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:19:55.929750: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 2
[2022-12-11 23:19:55.929842: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638[] 2022-12-11 23:19:55eager release cuda mem 625663.
929863: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:19:55.930033: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 3
[2022-12-11 23:19:55.930157: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:19:55.930313: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:19:55.930666: [E2022-12-11 23:19:55 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu930685:: 1926E]  Device 2 init p2p of link 4/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
:638] eager release cuda mem 625663
[2022-12-11 23:19:55.930816: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:19:55.930924: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:19:55.931582: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:19:55.946771: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 12399996
[2022-12-11 23:19:55[.2022-12-11 23:19:55947052.: 947059E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638:] 638eager release cuda mem 12399996] 
eager release cuda mem 12399996
[2022-12-11 23:19:55.947459: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 12399996
[2022-12-11 23:19:55.947817: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 12399996
[2022-12-11 23:19:55.947983: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 2999999 / 100000000 nodes ( 3.00 %~3.00 %) | remote 8999997 / 100000000 nodes ( 9.00 %) | cpu 88000004 / 100000000 nodes ( 88.00 %) | 1.44 GB | 0.728288 secs 
[2022-12-11 23:19:55.948106: E[ 2022-12-11 23:19:55/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.:948125638: ] Eeager release cuda mem 12399996 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 2999999 / 100000000 nodes ( 3.00 %~3.00 %) | remote 8999997 / 100000000 nodes ( 9.00 %) | cpu 88000004 / 100000000 nodes ( 88.00 %) | 1.44 GB | 0.728735 secs 
[2022-12-11 23:19:55.948214: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 2999999 / 100000000 nodes ( 3.00 %~3.00 %) | remote 8999997 / 100000000 nodes ( 9.00 %) | cpu 88000004 / 100000000 nodes ( 88.00 %) | 1.44 GB | 0.729204 secs 
[2022-12-11 23:19:55.948500: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 12399996
[2022-12-11 23:19:55.948665: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 2999999 / 100000000 nodes ( 3.00 %~3.00 %) | remote 8999997 / 100000000 nodes ( 9.00 %) | cpu 88000004 / 100000000 nodes ( 88.00 %) | 1.44 GB | 0.72926 secs 
[2022-12-11 23:19:55.948832: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 12399996
[2022-12-11 23:19:55.949014: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 2999999 / 100000000 nodes ( 3.00 %~3.00 %) | remote 8999997 / 100000000 nodes ( 9.00 %) | cpu 88000004 / 100000000 nodes ( 88.00 %) | 1.44 GB | 0.72747 secs 
[2022-12-11 23:19:55.949157: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 2999999 / 100000000 nodes ( 3.00 %~3.00 %) | remote 8999997 / 100000000 nodes ( 9.00 %) | cpu 88000004 / 100000000 nodes ( 88.00 %) | 1.44 GB | 0.72949 secs 
[2022-12-11 23:19:55.950465: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 2999999 / 100000000 nodes ( 3.00 %~3.00 %) | remote 8999997 / 100000000 nodes ( 9.00 %) | cpu 88000004 / 100000000 nodes ( 88.00 %) | 1.44 GB | 0.728995 secs 
[2022-12-11 23:19:55.952374: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 2999999 / 100000000 nodes ( 3.00 %~3.00 %) | remote 8999997 / 100000000 nodes ( 9.00 %) | cpu 88000004 / 100000000 nodes ( 88.00 %) | 1.44 GB | 0.748521 secs 
[2022-12-11 23:19:55.953706: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 9.75 GB
[2022-12-11 23:19:57.405206: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 10.02 GB
[2022-12-11 23:19:57.405658: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 10.02 GB
[2022-12-11 23:19:57.406587: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 10.02 GB
[2022-12-11 23:19:58.852957: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 10.28 GB
[2022-12-11 23:19:58.853843: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 10.28 GB
[2022-12-11 23:19:58.856109: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 10.28 GB
[2022-12-11 23:20:00.358424: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 10.50 GB
[2022-12-11 23:20:00.358592: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 10.50 GB
[2022-12-11 23:20:00.358891: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 10.50 GB
[2022-12-11 23:20:01.587842: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 10.71 GB
[2022-12-11 23:20:01.588015: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 10.71 GB
[2022-12-11 23:20:01.588582: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 10.71 GB
[2022-12-11 23:20:02.662203: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 11.17 GB
[2022-12-11 23:20:02.662646: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 11.17 GB
[2022-12-11 23:20:02.663526: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 11.17 GB
[2022-12-11 23:20:04.102800: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 11.37 GB
[2022-12-11 23:20:04.103467: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 11.37 GB
[HCTR][23:20:05.331][ERROR][RK0][tid #140632723023616]: replica 6 calling init per replica done, doing barrier
[HCTR][23:20:05.331][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier
[HCTR][23:20:05.331][ERROR][RK0][tid #140632194545408]: replica 5 calling init per replica done, doing barrier
[HCTR][23:20:05.331][ERROR][RK0][main]: replica 2 calling init per replica done, doing barrier
[HCTR][23:20:05.331][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier
[HCTR][23:20:05.331][ERROR][RK0][tid #140632186152704]: replica 3 calling init per replica done, doing barrier
[HCTR][23:20:05.331][ERROR][RK0][main]: replica 1 calling init per replica done, doing barrier
[HCTR][23:20:05.331][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier
[HCTR][23:20:05.331][ERROR][RK0][tid #140632194545408]: replica 5 calling init per replica done, doing barrier done
[HCTR][23:20:05.331][ERROR][RK0][tid #140632723023616]: replica 6 calling init per replica done, doing barrier done
[HCTR][23:20:05.331][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier done
[HCTR][23:20:05.331][ERROR][RK0][main]: replica 1 calling init per replica done, doing barrier done
[HCTR][23:20:05.331][ERROR][RK0][main]: replica 2 calling init per replica done, doing barrier done
[HCTR][23:20:05.331][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier done
[HCTR][23:20:05.331][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier done
[HCTR][23:20:05.331][ERROR][RK0][tid #140632186152704]: replica 3 calling init per replica done, doing barrier done
[HCTR][23:20:05.331][ERROR][RK0][tid #140632194545408]: init per replica done
[HCTR][23:20:05.331][ERROR][RK0][tid #140632723023616]: init per replica done
[HCTR][23:20:05.331][ERROR][RK0][main]: init per replica done
[HCTR][23:20:05.331][ERROR][RK0][main]: init per replica done
[HCTR][23:20:05.331][ERROR][RK0][main]: init per replica done
[HCTR][23:20:05.331][ERROR][RK0][main]: init per replica done
[HCTR][23:20:05.331][ERROR][RK0][tid #140632186152704]: init per replica done
[HCTR][23:20:05.334][ERROR][RK0][main]: init per replica done
[HCTR][23:20:05.370][ERROR][RK0][tid #140632748168960]: 2 allocated 3276800 at 0x7fcad8238400
[HCTR][23:20:05.370][ERROR][RK0][tid #140632748168960]: 2 allocated 6553600 at 0x7fcad8558400
[HCTR][23:20:05.370][ERROR][RK0][tid #140632748168960]: 2 allocated 3276800 at 0x7fcad8b98400
[HCTR][23:20:05.370][ERROR][RK0][tid #140632748168960]: 2 allocated 6553600 at 0x7fcad8eb8400
[HCTR][23:20:05.370][ERROR][RK0][main]: 6 allocated 3276800 at 0x7fcbd4238400
[HCTR][23:20:05.370][ERROR][RK0][main]: 6 allocated 6553600 at 0x7fcbd4558400
[HCTR][23:20:05.370][ERROR][RK0][tid #140632186152704]: 3 allocated 3276800 at 0x7fcbe8238400
[HCTR][23:20:05.370][ERROR][RK0][main]: 6 allocated 3276800 at 0x7fcbd4b98400
[HCTR][23:20:05.370][ERROR][RK0][tid #140632186152704]: 3 allocated 6553600 at 0x7fcbe8558400
[HCTR][23:20:05.370][ERROR][RK0][main]: 6 allocated 6553600 at 0x7fcbd4eb8400
[HCTR][23:20:05.370][ERROR][RK0][tid #140632186152704]: 3 allocated 3276800 at 0x7fcbe8b98400
[HCTR][23:20:05.370][ERROR][RK0][tid #140632186152704]: 3 allocated 6553600 at 0x7fcbe8eb8400
[HCTR][23:20:05.370][ERROR][RK0][main]: 4 allocated 3276800 at 0x7fcbe4238400
[HCTR][23:20:05.370][ERROR][RK0][main]: 4 allocated 6553600 at 0x7fcbe4558400
[HCTR][23:20:05.370][ERROR][RK0][main]: 4 allocated 3276800 at 0x7fcbe4b98400
[HCTR][23:20:05.370][ERROR][RK0][main]: 4 allocated 6553600 at 0x7fcbe4eb8400
[HCTR][23:20:05.370][ERROR][RK0][tid #140632194545408]: 7 allocated 3276800 at 0x7fcac0238400
[HCTR][23:20:05.370][ERROR][RK0][tid #140632194545408]: 7 allocated 6553600 at 0x7fcac0558400
[HCTR][23:20:05.370][ERROR][RK0][tid #140632194545408]: 7 allocated 3276800 at 0x7fcac0b98400
[HCTR][23:20:05.370][ERROR][RK0][tid #140632194545408]: 7 allocated 6553600 at 0x7fcac0eb8400
[HCTR][23:20:05.370][ERROR][RK0][main]: 5 allocated 3276800 at 0x7fcb6c238400
[HCTR][23:20:05.371][ERROR][RK0][main]: 5 allocated 6553600 at 0x7fcb6c558400
[HCTR][23:20:05.371][ERROR][RK0][main]: 5 allocated 3276800 at 0x7fcb6cb98400
[HCTR][23:20:05.371][ERROR][RK0][main]: 5 allocated 6553600 at 0x7fcb6ceb8400
[HCTR][23:20:05.371][ERROR][RK0][tid #140632882386688]: 1 allocated 3276800 at 0x7fcbea238400
[HCTR][23:20:05.371][ERROR][RK0][tid #140632882386688]: 1 allocated 6553600 at 0x7fcbea558400
[HCTR][23:20:05.371][ERROR][RK0][tid #140632882386688]: 1 allocated 3276800 at 0x7fcbeab98400
[HCTR][23:20:05.371][ERROR][RK0][tid #140632882386688]: 1 allocated 6553600 at 0x7fcbeaeb8400
[HCTR][23:20:05.374][ERROR][RK0][tid #140632530089728]: 0 allocated 3276800 at 0x7fcbb8320000
[HCTR][23:20:05.374][ERROR][RK0][tid #140632530089728]: 0 allocated 6553600 at 0x7fcbb8640000
[HCTR][23:20:05.374][ERROR][RK0][tid #140632530089728]: 0 allocated 3276800 at 0x7fcbb8c80000
[HCTR][23:20:05.374][ERROR][RK0][tid #140632530089728]: 0 allocated 6553600 at 0x7fcbb8fa0000
