2022-12-12 04:53:16.403401: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.408728: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.415281: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.421144: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.432738: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.441108: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.445342: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.457890: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.516133: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.516804: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.517464: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.518337: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.519083: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.519920: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.520731: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.521692: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.522448: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.523278: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.524047: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.524880: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.525872: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.526289: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.527700: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.527800: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.529547: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.530560: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.531517: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.532546: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.533675: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.534645: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.535735: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.536710: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.538488: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.539616: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.540603: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.541613: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.542578: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.543654: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.544717: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.545825: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.550447: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.551276: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 04:53:16.552071: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.553502: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.553489: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.555225: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.555253: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.556885: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.556893: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.558395: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.558450: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.559929: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.560029: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.561127: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.561606: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.561758: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.563469: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.563949: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.565367: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.565924: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.569296: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.571920: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.572014: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.573653: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.574997: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.575231: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.575384: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.576835: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.578657: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.578904: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.579190: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.579513: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.579880: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.582262: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.582553: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.582866: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.583377: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.583475: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.586112: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.586299: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.586564: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.586702: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.587156: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.589646: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.589734: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.589869: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.589915: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.590656: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.592946: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.592990: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.593092: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.593252: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.593747: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.595824: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.605471: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.605578: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.605861: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.607838: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.608465: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.609748: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.609982: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.610632: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.611203: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.611943: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.613037: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.613578: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.640513: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.650449: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.651119: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.652424: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.652485: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.652954: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.653908: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.655737: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.655833: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.656219: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.656290: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.656622: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.656840: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.659923: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.660065: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.661597: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.661771: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.662134: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.662313: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.664640: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.664743: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.665379: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.665513: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.665949: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.666019: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.668771: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.668870: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.669486: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.669671: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.669992: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.670112: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.672909: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.673039: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.673739: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.674029: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.674147: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.674441: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.676790: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.676957: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.677658: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.678233: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.678267: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.678482: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.680725: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.680769: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.681864: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.682291: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.682386: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.682464: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.684716: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.684776: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.685475: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.686148: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.686228: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.686458: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.688599: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.688733: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.689825: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.690243: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.690414: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.690753: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.692628: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.692712: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.693568: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.693876: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.693942: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.694581: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.696238: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.696580: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.696819: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.697836: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.698021: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.698056: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.698630: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.700836: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.700864: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.701346: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.702513: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.702629: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.702818: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.703323: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.705582: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.705782: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.706860: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.706975: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.707101: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.707105: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.709950: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.710014: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.711149: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.711206: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.711367: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.711999: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.712102: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 04:53:16.713986: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.714800: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.715226: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.715264: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.715450: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.716137: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.717691: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.718813: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.719046: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.719090: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.719344: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.719699: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.721869: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.721930: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.722831: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.723763: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.723769: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.724122: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.724243: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.725800: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.726068: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.727223: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.728071: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.728241: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.728633: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.728851: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.730690: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.730931: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.731873: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.732746: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.733088: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.733538: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.733733: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.735433: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.736750: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.737647: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.737953: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.738042: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.738221: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.739644: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.741240: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.742503: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.742576: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.742778: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.744341: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.745381: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.746016: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 04:53:16.746712: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.746861: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.748020: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.749859: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 04:53:16.750291: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.750478: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.751458: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.752238: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 04:53:16.753483: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.753758: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.754416: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.755672: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.757588: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.758759: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.759395: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.759857: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 04:53:16.760275: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 04:53:16.760879: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.761626: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.761699: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.762221: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.764202: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.765533: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.765916: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.767859: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.768950: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.769434: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.769985: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.782470: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.783743: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.783870: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.787071: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.788779: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.788973: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.848906: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.854883: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.861511: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.873077: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 04:53:16.882627: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.897587: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:16.911737: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:17.891209: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:17.891846: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:17.892387: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:17.892851: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 04:53:17.892913: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 04:53:17.910865: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:17.911839: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:17.912388: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:17.912966: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:17.914006: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:17.914496: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 04:53:17.959777: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:53:17.959997: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:53:17.999860: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-12 04:53:18.086778: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:18.087669: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:18.088203: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:18.088665: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 04:53:18.088724: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 04:53:18.106331: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:18.107572: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:18.108273: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:18.108857: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:18.109394: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:18.109863: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 04:53:18.176497: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:18.177203: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:18.177734: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:18.178197: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 04:53:18.178258: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 04:53:18.189643: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:18.189694: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:18.190930: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:18.191023: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:18.191986: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:18.192162: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:18.192739: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 04:53:18.192806: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 04:53:18.192932: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 04:53:18.192990: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 04:53:18.193392: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:53:18.193567: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:53:18.194269: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:18.194586: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-12 04:53:18.195078: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:18.195677: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:18.196142: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 04:53:18.196205: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 04:53:18.197189: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:18.197787: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:18.198300: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:18.198370: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:18.199498: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:18.199553: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:18.200766: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:18.200844: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:18.201744: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 04:53:18.201803: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 04:53:18.201839: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 04:53:18.211267: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:18.211267: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:18.212424: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:18.212452: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:18.213416: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:18.213587: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:18.213635: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:18.214551: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:18.214836: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:18.215540: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:18.215627: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:18.216594: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:18.216750: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:18.217526: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:18.217666: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:18.218629: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:18.218707: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 04:53:18.218763: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 04:53:18.219609: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 04:53:18.219717: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 04:53:18.220131: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:18.220687: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:18.220888: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:18.221473: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 04:53:18.221793: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:18.222310: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:18.222949: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:18.223514: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:18.223980: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 04:53:18.237230: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:18.237916: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:18.238445: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:18.239027: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:18.239576: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 04:53:18.240047: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 04:53:18.248398: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:53:18.248606: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:53:18.250604: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
2022-12-12 04:53:18.265584: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:53:18.265784: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:53:18.265889: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:53:18.266075: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:53:18.266812: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
2022-12-12 04:53:18.267086: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
2022-12-12 04:53:18.267331: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:53:18.267469: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:53:18.268436: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
2022-12-12 04:53:18.270225: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:53:18.270360: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:53:18.272093: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
2022-12-12 04:53:18.319319: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:53:18.319508: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 04:53:18.320459: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
[HCTR][04:53:19.584][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][04:53:19.584][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][04:53:19.584][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][04:53:19.586][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][04:53:19.588][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][04:53:19.588][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][04:53:19.588][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][04:53:19.588][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:01,  1.55s/it]warmup run: 98it [00:01, 82.27it/s]warmup run: 1it [00:01,  1.54s/it]warmup run: 193it [00:01, 175.39it/s]warmup run: 100it [00:01, 84.72it/s]warmup run: 1it [00:01,  1.52s/it]warmup run: 1it [00:01,  1.52s/it]warmup run: 1it [00:01,  1.51s/it]warmup run: 288it [00:01, 278.29it/s]warmup run: 196it [00:01, 179.39it/s]warmup run: 1it [00:01,  1.59s/it]warmup run: 99it [00:01, 84.72it/s]warmup run: 101it [00:01, 86.60it/s]warmup run: 1it [00:01,  1.58s/it]warmup run: 101it [00:01, 86.97it/s]warmup run: 383it [00:01, 384.97it/s]warmup run: 1it [00:01,  1.48s/it]warmup run: 290it [00:01, 281.01it/s]warmup run: 101it [00:01, 82.93it/s]warmup run: 199it [00:01, 184.65it/s]warmup run: 202it [00:01, 187.60it/s]warmup run: 94it [00:01, 77.61it/s]warmup run: 203it [00:01, 189.38it/s]warmup run: 479it [00:02, 490.69it/s]warmup run: 98it [00:01, 85.76it/s]warmup run: 387it [00:01, 390.94it/s]warmup run: 201it [00:01, 179.35it/s]warmup run: 300it [00:01, 295.71it/s]warmup run: 303it [00:01, 298.39it/s]warmup run: 188it [00:01, 168.73it/s]warmup run: 306it [00:01, 302.95it/s]warmup run: 575it [00:02, 588.68it/s]warmup run: 197it [00:01, 186.30it/s]warmup run: 483it [00:02, 496.45it/s]warmup run: 300it [00:01, 285.22it/s]warmup run: 403it [00:01, 413.67it/s]warmup run: 403it [00:01, 411.81it/s]warmup run: 282it [00:01, 269.71it/s]warmup run: 409it [00:01, 420.61it/s]warmup run: 671it [00:02, 673.84it/s]warmup run: 296it [00:01, 296.34it/s]warmup run: 581it [00:02, 597.42it/s]warmup run: 398it [00:01, 393.89it/s]warmup run: 505it [00:02, 526.49it/s]warmup run: 503it [00:02, 521.85it/s]warmup run: 376it [00:01, 375.13it/s]warmup run: 512it [00:02, 534.57it/s]warmup run: 768it [00:02, 745.16it/s]warmup run: 394it [00:01, 408.24it/s]warmup run: 680it [00:02, 687.82it/s]warmup run: 492it [00:02, 493.41it/s]warmup run: 604it [00:02, 623.56it/s]warmup run: 605it [00:02, 626.50it/s]warmup run: 469it [00:02, 476.44it/s]warmup run: 617it [00:02, 642.76it/s]warmup run: 866it [00:02, 805.25it/s]warmup run: 493it [00:01, 517.76it/s]warmup run: 775it [00:02, 752.01it/s]warmup run: 593it [00:02, 599.66it/s]warmup run: 704it [00:02, 709.02it/s]warmup run: 707it [00:02, 717.28it/s]warmup run: 563it [00:02, 571.75it/s]warmup run: 721it [00:02, 734.07it/s]warmup run: 968it [00:02, 862.35it/s]warmup run: 593it [00:02, 619.89it/s]warmup run: 870it [00:02, 802.58it/s]warmup run: 695it [00:02, 694.21it/s]warmup run: 803it [00:02, 778.22it/s]warmup run: 809it [00:02, 791.52it/s]warmup run: 663it [00:02, 668.31it/s]warmup run: 824it [00:02, 806.52it/s]warmup run: 1069it [00:02, 901.49it/s]warmup run: 691it [00:02, 702.58it/s]warmup run: 965it [00:02, 834.88it/s]warmup run: 797it [00:02, 773.06it/s]warmup run: 902it [00:02, 832.07it/s]warmup run: 911it [00:02, 849.23it/s]warmup run: 763it [00:02, 749.16it/s]warmup run: 928it [00:02, 867.23it/s]warmup run: 1167it [00:02, 922.42it/s]warmup run: 787it [00:02, 764.31it/s]warmup run: 899it [00:02, 835.24it/s]warmup run: 1059it [00:02, 855.60it/s]warmup run: 1000it [00:02, 872.17it/s]warmup run: 1012it [00:02, 891.67it/s]warmup run: 866it [00:02, 820.56it/s]warmup run: 1031it [00:02, 910.19it/s]warmup run: 1266it [00:02, 941.66it/s]warmup run: 999it [00:02, 877.79it/s]warmup run: 1152it [00:02, 872.49it/s]warmup run: 1102it [00:02, 911.69it/s]warmup run: 883it [00:02, 759.95it/s]warmup run: 1113it [00:02, 924.39it/s]warmup run: 967it [00:02, 871.13it/s]warmup run: 1134it [00:02, 937.94it/s]warmup run: 1367it [00:02, 959.81it/s]warmup run: 1099it [00:02, 910.35it/s]warmup run: 1249it [00:02, 899.76it/s]warmup run: 1203it [00:02, 939.52it/s]warmup run: 985it [00:02, 826.55it/s]warmup run: 1215it [00:02, 950.37it/s]warmup run: 1070it [00:02, 914.49it/s]warmup run: 1237it [00:02, 961.79it/s]warmup run: 1471it [00:03, 980.65it/s]warmup run: 1200it [00:02, 938.48it/s]warmup run: 1349it [00:02, 926.30it/s]warmup run: 1306it [00:02, 964.57it/s]warmup run: 1089it [00:02, 882.34it/s]warmup run: 1318it [00:02, 973.05it/s]warmup run: 1172it [00:02, 944.31it/s]warmup run: 1341it [00:02, 982.93it/s]warmup run: 1574it [00:03, 993.88it/s]warmup run: 1303it [00:02, 962.07it/s]warmup run: 1446it [00:03, 938.45it/s]warmup run: 1409it [00:02, 982.66it/s]warmup run: 1193it [00:02, 924.71it/s]warmup run: 1421it [00:02, 988.27it/s]warmup run: 1274it [00:02, 965.67it/s]warmup run: 1446it [00:02, 1001.49it/s]warmup run: 1678it [00:03, 1005.82it/s]warmup run: 1404it [00:03, 973.62it/s]warmup run: 1542it [00:03, 943.12it/s]warmup run: 1512it [00:03, 995.48it/s]warmup run: 1296it [00:02, 953.44it/s]warmup run: 1525it [00:03, 1002.82it/s]warmup run: 1375it [00:02, 976.31it/s]warmup run: 1551it [00:03, 1015.37it/s]warmup run: 1781it [00:03, 1011.43it/s]warmup run: 1506it [00:03, 984.73it/s]warmup run: 1638it [00:03, 947.05it/s]warmup run: 1615it [00:03, 1003.15it/s]warmup run: 1401it [00:02, 979.07it/s]warmup run: 1628it [00:03, 1000.22it/s]warmup run: 1476it [00:03, 980.97it/s]warmup run: 1657it [00:03, 1028.22it/s]warmup run: 1885it [00:03, 1017.03it/s]warmup run: 1607it [00:03, 989.40it/s]warmup run: 1735it [00:03, 951.48it/s]warmup run: 1719it [00:03, 1011.37it/s]warmup run: 1506it [00:03, 997.58it/s]warmup run: 1730it [00:03, 1003.52it/s]warmup run: 1577it [00:03, 977.26it/s]warmup run: 1762it [00:03, 1032.26it/s]warmup run: 1988it [00:03, 1014.30it/s]warmup run: 1708it [00:03, 994.19it/s]warmup run: 1838it [00:03, 973.35it/s]warmup run: 1822it [00:03, 1013.87it/s]warmup run: 1608it [00:03, 995.52it/s]warmup run: 1832it [00:03, 1005.17it/s]warmup run: 1867it [00:03, 1034.53it/s]warmup run: 1677it [00:03, 974.16it/s]warmup run: 2101it [00:03, 1047.73it/s]warmup run: 1809it [00:03, 996.60it/s]warmup run: 1940it [00:03, 986.54it/s]warmup run: 1925it [00:03, 1016.67it/s]warmup run: 1710it [00:03, 994.22it/s]warmup run: 1935it [00:03, 1010.23it/s]warmup run: 1972it [00:03, 1037.41it/s]warmup run: 1776it [00:03, 975.61it/s]warmup run: 2219it [00:03, 1085.52it/s]warmup run: 1910it [00:03, 999.95it/s]warmup run: 2048it [00:03, 1014.22it/s]warmup run: 2032it [00:03, 1031.33it/s]warmup run: 1811it [00:03, 993.61it/s]warmup run: 2044it [00:03, 1033.77it/s]warmup run: 2084it [00:03, 1061.88it/s]warmup run: 1880it [00:03, 993.60it/s]warmup run: 2338it [00:03, 1115.67it/s]warmup run: 2013it [00:03, 1008.54it/s]warmup run: 2168it [00:03, 1069.33it/s]warmup run: 2148it [00:03, 1068.86it/s]warmup run: 1912it [00:03, 991.44it/s]warmup run: 2166it [00:03, 1087.42it/s]warmup run: 2207it [00:03, 1111.85it/s]warmup run: 1984it [00:03, 1005.55it/s]warmup run: 2457it [00:03, 1135.46it/s]warmup run: 2131it [00:03, 1058.16it/s]warmup run: 2288it [00:03, 1108.23it/s]warmup run: 2268it [00:03, 1105.33it/s]warmup run: 2012it [00:03, 993.78it/s]warmup run: 2288it [00:03, 1125.59it/s]warmup run: 2330it [00:03, 1145.89it/s]warmup run: 2104it [00:03, 1060.94it/s]warmup run: 2576it [00:04, 1150.37it/s]warmup run: 2249it [00:03, 1093.88it/s]warmup run: 2409it [00:03, 1136.65it/s]warmup run: 2388it [00:03, 1133.32it/s]warmup run: 2131it [00:03, 1049.78it/s]warmup run: 2410it [00:03, 1153.17it/s]warmup run: 2453it [00:03, 1170.51it/s]warmup run: 2227it [00:03, 1110.86it/s]warmup run: 2694it [00:04, 1157.32it/s]warmup run: 2367it [00:03, 1119.52it/s]warmup run: 2530it [00:04, 1157.05it/s]warmup run: 2509it [00:03, 1154.21it/s]warmup run: 2250it [00:03, 1089.54it/s]warmup run: 2532it [00:03, 1173.08it/s]warmup run: 2577it [00:03, 1188.65it/s]warmup run: 2350it [00:03, 1145.15it/s]warmup run: 2813it [00:04, 1166.49it/s]warmup run: 2486it [00:04, 1138.03it/s]warmup run: 2650it [00:04, 1169.44it/s]warmup run: 2629it [00:04, 1164.67it/s]warmup run: 2369it [00:03, 1118.00it/s]warmup run: 2653it [00:04, 1183.68it/s]warmup run: 2696it [00:04, 1177.95it/s]warmup run: 2473it [00:04, 1169.92it/s]warmup run: 2932it [00:04, 1171.70it/s]warmup run: 2604it [00:04, 1149.93it/s]warmup run: 2772it [00:04, 1181.72it/s]warmup run: 2749it [00:04, 1175.04it/s]warmup run: 2489it [00:03, 1139.79it/s]warmup run: 3000it [00:04, 676.65it/s] warmup run: 2773it [00:04, 1187.55it/s]warmup run: 2818it [00:04, 1187.61it/s]warmup run: 2595it [00:04, 1182.41it/s]warmup run: 2720it [00:04, 1152.67it/s]warmup run: 2891it [00:04, 1163.70it/s]warmup run: 2869it [00:04, 1180.73it/s]warmup run: 2609it [00:04, 1155.84it/s]warmup run: 2893it [00:04, 1190.79it/s]warmup run: 2940it [00:04, 1197.02it/s]warmup run: 2714it [00:04, 1182.46it/s]warmup run: 3000it [00:04, 700.40it/s] warmup run: 2838it [00:04, 1158.78it/s]warmup run: 2988it [00:04, 1170.94it/s]warmup run: 3000it [00:04, 671.20it/s] warmup run: 2729it [00:04, 1166.33it/s]warmup run: 3000it [00:04, 688.77it/s] warmup run: 3000it [00:04, 693.26it/s] warmup run: 2833it [00:04, 1172.71it/s]warmup run: 2956it [00:04, 1162.64it/s]warmup run: 2850it [00:04, 1177.25it/s]warmup run: 3000it [00:04, 674.11it/s] warmup run: 2954it [00:04, 1182.24it/s]warmup run: 3000it [00:04, 673.82it/s] warmup run: 2971it [00:04, 1185.06it/s]warmup run: 3000it [00:04, 686.96it/s] 


warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|         | 165/3000 [00:00<00:01, 1649.18it/s]warmup should be done:   6%|         | 166/3000 [00:00<00:01, 1658.18it/s]warmup should be done:   5%|         | 164/3000 [00:00<00:01, 1631.50it/s]warmup should be done:   5%|         | 160/3000 [00:00<00:01, 1591.82it/s]warmup should be done:   5%|         | 152/3000 [00:00<00:01, 1512.18it/s]warmup should be done:   5%|         | 154/3000 [00:00<00:01, 1532.18it/s]warmup should be done:   6%|         | 165/3000 [00:00<00:01, 1641.42it/s]warmup should be done:   6%|         | 165/3000 [00:00<00:01, 1640.58it/s]warmup should be done:  11%|         | 331/3000 [00:00<00:01, 1653.46it/s]warmup should be done:  11%|         | 334/3000 [00:00<00:01, 1668.97it/s]warmup should be done:  11%|         | 324/3000 [00:00<00:01, 1618.16it/s]warmup should be done:  11%|         | 331/3000 [00:00<00:01, 1651.79it/s]warmup should be done:  10%|         | 309/3000 [00:00<00:01, 1541.46it/s]warmup should be done:  10%|         | 309/3000 [00:00<00:01, 1544.19it/s]warmup should be done:  11%|         | 329/3000 [00:00<00:01, 1636.80it/s]warmup should be done:  11%|         | 330/3000 [00:00<00:01, 1636.25it/s]warmup should be done:  17%|        | 501/3000 [00:00<00:01, 1666.59it/s]warmup should be done:  16%|        | 472/3000 [00:00<00:01, 1581.04it/s]warmup should be done:  17%|        | 497/3000 [00:00<00:01, 1649.51it/s]warmup should be done:  15%|        | 464/3000 [00:00<00:01, 1540.96it/s]warmup should be done:  16%|        | 486/3000 [00:00<00:01, 1609.63it/s]warmup should be done:  16%|        | 493/3000 [00:00<00:01, 1627.74it/s]warmup should be done:  17%|        | 497/3000 [00:00<00:01, 1640.48it/s]warmup should be done:  16%|        | 494/3000 [00:00<00:01, 1627.21it/s]warmup should be done:  22%|       | 668/3000 [00:00<00:01, 1665.68it/s]warmup should be done:  21%|        | 635/3000 [00:00<00:01, 1599.32it/s]warmup should be done:  22%|       | 662/3000 [00:00<00:01, 1645.75it/s]warmup should be done:  22%|       | 648/3000 [00:00<00:01, 1610.97it/s]warmup should be done:  21%|        | 619/3000 [00:00<00:01, 1537.18it/s]warmup should be done:  22%|       | 662/3000 [00:00<00:01, 1642.44it/s]warmup should be done:  22%|       | 656/3000 [00:00<00:01, 1624.71it/s]warmup should be done:  22%|       | 657/3000 [00:00<00:01, 1621.68it/s]warmup should be done:  27%|       | 798/3000 [00:00<00:01, 1606.94it/s]warmup should be done:  28%|       | 835/3000 [00:00<00:01, 1661.97it/s]warmup should be done:  28%|       | 827/3000 [00:00<00:01, 1641.29it/s]warmup should be done:  26%|       | 773/3000 [00:00<00:01, 1536.46it/s]warmup should be done:  27%|       | 810/3000 [00:00<00:01, 1611.19it/s]warmup should be done:  27%|       | 819/3000 [00:00<00:01, 1622.81it/s]warmup should be done:  28%|       | 827/3000 [00:00<00:01, 1637.94it/s]warmup should be done:  27%|       | 820/3000 [00:00<00:01, 1617.17it/s]warmup should be done:  32%|      | 960/3000 [00:00<00:01, 1611.20it/s]warmup should be done:  33%|      | 1002/3000 [00:00<00:01, 1657.41it/s]warmup should be done:  32%|      | 972/3000 [00:00<00:01, 1611.74it/s]warmup should be done:  31%|       | 927/3000 [00:00<00:01, 1534.42it/s]warmup should be done:  33%|      | 992/3000 [00:00<00:01, 1635.29it/s]warmup should be done:  33%|      | 982/3000 [00:00<00:01, 1620.73it/s]warmup should be done:  33%|      | 991/3000 [00:00<00:01, 1635.58it/s]warmup should be done:  33%|      | 982/3000 [00:00<00:01, 1611.58it/s]warmup should be done:  37%|      | 1122/3000 [00:00<00:01, 1613.29it/s]warmup should be done:  39%|      | 1169/3000 [00:00<00:01, 1658.88it/s]warmup should be done:  38%|      | 1135/3000 [00:00<00:01, 1615.71it/s]warmup should be done:  39%|      | 1156/3000 [00:00<00:01, 1635.64it/s]warmup should be done:  36%|      | 1081/3000 [00:00<00:01, 1530.02it/s]warmup should be done:  39%|      | 1156/3000 [00:00<00:01, 1638.43it/s]warmup should be done:  38%|      | 1145/3000 [00:00<00:01, 1617.65it/s]warmup should be done:  38%|      | 1144/3000 [00:00<00:01, 1611.28it/s]warmup should be done:  43%|     | 1285/3000 [00:00<00:01, 1615.78it/s]warmup should be done:  44%|     | 1335/3000 [00:00<00:01, 1659.06it/s]warmup should be done:  43%|     | 1298/3000 [00:00<00:01, 1617.38it/s]warmup should be done:  44%|     | 1320/3000 [00:00<00:01, 1638.03it/s]warmup should be done:  41%|      | 1235/3000 [00:00<00:01, 1529.96it/s]warmup should be done:  44%|     | 1320/3000 [00:00<00:01, 1632.65it/s]warmup should be done:  44%|     | 1307/3000 [00:00<00:01, 1617.80it/s]warmup should be done:  44%|     | 1306/3000 [00:00<00:01, 1611.02it/s]warmup should be done:  48%|     | 1448/3000 [00:00<00:00, 1618.97it/s]warmup should be done:  50%|     | 1501/3000 [00:00<00:00, 1659.10it/s]warmup should be done:  49%|     | 1460/3000 [00:00<00:00, 1616.64it/s]warmup should be done:  49%|     | 1484/3000 [00:00<00:00, 1638.23it/s]warmup should be done:  46%|     | 1388/3000 [00:00<00:01, 1528.06it/s]warmup should be done:  49%|     | 1484/3000 [00:00<00:00, 1632.62it/s]warmup should be done:  49%|     | 1469/3000 [00:00<00:00, 1617.58it/s]warmup should be done:  49%|     | 1468/3000 [00:00<00:00, 1610.22it/s]warmup should be done:  54%|    | 1610/3000 [00:01<00:00, 1617.63it/s]warmup should be done:  56%|    | 1668/3000 [00:01<00:00, 1660.12it/s]warmup should be done:  54%|    | 1622/3000 [00:01<00:00, 1613.50it/s]warmup should be done:  52%|    | 1545/3000 [00:01<00:00, 1539.78it/s]warmup should be done:  55%|    | 1648/3000 [00:01<00:00, 1633.76it/s]warmup should be done:  54%|    | 1631/3000 [00:01<00:00, 1613.40it/s]warmup should be done:  55%|    | 1648/3000 [00:01<00:00, 1627.87it/s]warmup should be done:  54%|    | 1631/3000 [00:01<00:00, 1614.84it/s]warmup should be done:  59%|    | 1772/3000 [00:01<00:00, 1616.90it/s]warmup should be done:  61%|    | 1835/3000 [00:01<00:00, 1659.72it/s]warmup should be done:  59%|    | 1784/3000 [00:01<00:00, 1610.42it/s]warmup should be done:  60%|    | 1812/3000 [00:01<00:00, 1633.76it/s]warmup should be done:  57%|    | 1703/3000 [00:01<00:00, 1549.39it/s]warmup should be done:  60%|    | 1793/3000 [00:01<00:00, 1614.73it/s]warmup should be done:  60%|    | 1811/3000 [00:01<00:00, 1622.32it/s]warmup should be done:  60%|    | 1793/3000 [00:01<00:00, 1601.41it/s]warmup should be done:  64%|   | 1934/3000 [00:01<00:00, 1614.44it/s]warmup should be done:  67%|   | 2001/3000 [00:01<00:00, 1659.58it/s]warmup should be done:  62%|   | 1858/3000 [00:01<00:00, 1548.96it/s]warmup should be done:  66%|   | 1976/3000 [00:01<00:00, 1633.34it/s]warmup should be done:  65%|   | 1956/3000 [00:01<00:00, 1617.41it/s]warmup should be done:  65%|   | 1946/3000 [00:01<00:00, 1607.39it/s]warmup should be done:  66%|   | 1974/3000 [00:01<00:00, 1620.69it/s]warmup should be done:  65%|   | 1954/3000 [00:01<00:00, 1601.42it/s]warmup should be done:  70%|   | 2096/3000 [00:01<00:00, 1614.57it/s]warmup should be done:  72%|  | 2167/3000 [00:01<00:00, 1658.43it/s]warmup should be done:  71%|   | 2120/3000 [00:01<00:00, 1622.07it/s]warmup should be done:  70%|   | 2107/3000 [00:01<00:00, 1600.76it/s]warmup should be done:  71%|  | 2140/3000 [00:01<00:00, 1624.21it/s]warmup should be done:  67%|   | 2013/3000 [00:01<00:00, 1535.62it/s]warmup should be done:  71%|   | 2137/3000 [00:01<00:00, 1621.38it/s]warmup should be done:  71%|   | 2116/3000 [00:01<00:00, 1604.73it/s]warmup should be done:  75%|  | 2258/3000 [00:01<00:00, 1612.18it/s]warmup should be done:  78%|  | 2333/3000 [00:01<00:00, 1656.03it/s]warmup should be done:  76%|  | 2283/3000 [00:01<00:00, 1622.83it/s]warmup should be done:  76%|  | 2268/3000 [00:01<00:00, 1601.77it/s]warmup should be done:  72%|  | 2168/3000 [00:01<00:00, 1537.18it/s]warmup should be done:  77%|  | 2303/3000 [00:01<00:00, 1617.36it/s]warmup should be done:  77%|  | 2300/3000 [00:01<00:00, 1618.44it/s]warmup should be done:  76%|  | 2277/3000 [00:01<00:00, 1600.89it/s]warmup should be done:  81%|  | 2420/3000 [00:01<00:00, 1614.27it/s]warmup should be done:  83%| | 2500/3000 [00:01<00:00, 1657.73it/s]warmup should be done:  81%|  | 2431/3000 [00:01<00:00, 1609.13it/s]warmup should be done:  82%| | 2446/3000 [00:01<00:00, 1616.10it/s]warmup should be done:  78%|  | 2326/3000 [00:01<00:00, 1549.61it/s]warmup should be done:  82%| | 2465/3000 [00:01<00:00, 1615.32it/s]warmup should be done:  82%| | 2462/3000 [00:01<00:00, 1616.52it/s]warmup should be done:  81%| | 2439/3000 [00:01<00:00, 1604.03it/s]warmup should be done:  86%| | 2582/3000 [00:01<00:00, 1615.36it/s]warmup should be done:  89%| | 2666/3000 [00:01<00:00, 1656.44it/s]warmup should be done:  86%| | 2593/3000 [00:01<00:00, 1612.14it/s]warmup should be done:  83%| | 2485/3000 [00:01<00:00, 1559.49it/s]warmup should be done:  87%| | 2624/3000 [00:01<00:00, 1613.83it/s]warmup should be done:  87%| | 2600/3000 [00:01<00:00, 1605.33it/s]warmup should be done:  88%| | 2627/3000 [00:01<00:00, 1586.62it/s]warmup should be done:  87%| | 2608/3000 [00:01<00:00, 1554.94it/s]warmup should be done:  92%|| 2745/3000 [00:01<00:00, 1617.49it/s]warmup should be done:  94%|| 2832/3000 [00:01<00:00, 1653.76it/s]warmup should be done:  92%|| 2756/3000 [00:01<00:00, 1615.89it/s]warmup should be done:  88%| | 2646/3000 [00:01<00:00, 1573.99it/s]warmup should be done:  93%|| 2786/3000 [00:01<00:00, 1611.02it/s]warmup should be done:  93%|| 2786/3000 [00:01<00:00, 1573.77it/s]warmup should be done:  92%|| 2761/3000 [00:01<00:00, 1574.18it/s]warmup should be done:  92%|| 2765/3000 [00:01<00:00, 1557.77it/s]warmup should be done:  97%|| 2908/3000 [00:01<00:00, 1620.34it/s]warmup should be done: 100%|| 2998/3000 [00:01<00:00, 1653.28it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1657.97it/s]warmup should be done:  97%|| 2919/3000 [00:01<00:00, 1618.44it/s]warmup should be done:  94%|| 2808/3000 [00:01<00:00, 1585.24it/s]warmup should be done:  98%|| 2948/3000 [00:01<00:00, 1613.03it/s]warmup should be done:  98%|| 2945/3000 [00:01<00:00, 1578.39it/s]warmup should be done:  97%|| 2924/3000 [00:01<00:00, 1589.06it/s]warmup should be done:  97%|| 2922/3000 [00:01<00:00, 1551.39it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1625.47it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1617.01it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1612.52it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1609.83it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1604.78it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1600.92it/s]warmup should be done:  99%|| 2967/3000 [00:01<00:00, 1585.83it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1552.18it/s]

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]




warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|         | 165/3000 [00:00<00:01, 1648.71it/s]warmup should be done:   5%|         | 164/3000 [00:00<00:01, 1634.76it/s]warmup should be done:   5%|         | 164/3000 [00:00<00:01, 1635.75it/s]warmup should be done:   5%|         | 160/3000 [00:00<00:01, 1595.21it/s]warmup should be done:   6%|         | 166/3000 [00:00<00:01, 1651.16it/s]warmup should be done:   6%|         | 167/3000 [00:00<00:01, 1664.03it/s]warmup should be done:   6%|         | 167/3000 [00:00<00:01, 1663.40it/s]warmup should be done:   6%|         | 165/3000 [00:00<00:01, 1640.76it/s]warmup should be done:  11%|         | 330/3000 [00:00<00:01, 1649.36it/s]warmup should be done:  11%|         | 323/3000 [00:00<00:01, 1614.26it/s]warmup should be done:  11%|         | 336/3000 [00:00<00:01, 1677.05it/s]warmup should be done:  11%|         | 334/3000 [00:00<00:01, 1669.90it/s]warmup should be done:  11%|         | 331/3000 [00:00<00:01, 1650.44it/s]warmup should be done:  11%|         | 331/3000 [00:00<00:01, 1648.03it/s]warmup should be done:  11%|        | 338/3000 [00:00<00:01, 1685.30it/s]warmup should be done:  11%|         | 332/3000 [00:00<00:01, 1647.62it/s]warmup should be done:  17%|        | 496/3000 [00:00<00:01, 1651.14it/s]warmup should be done:  17%|        | 504/3000 [00:00<00:01, 1682.23it/s]warmup should be done:  17%|        | 505/3000 [00:00<00:01, 1681.21it/s]warmup should be done:  17%|        | 499/3000 [00:00<00:01, 1661.93it/s]warmup should be done:  16%|        | 485/3000 [00:00<00:01, 1611.75it/s]warmup should be done:  17%|        | 498/3000 [00:00<00:01, 1654.70it/s]warmup should be done:  17%|        | 509/3000 [00:00<00:01, 1693.16it/s]warmup should be done:  17%|        | 498/3000 [00:00<00:01, 1651.07it/s]warmup should be done:  22%|       | 663/3000 [00:00<00:01, 1656.63it/s]warmup should be done:  22%|       | 675/3000 [00:00<00:01, 1691.09it/s]warmup should be done:  22%|       | 649/3000 [00:00<00:01, 1622.58it/s]warmup should be done:  22%|       | 668/3000 [00:00<00:01, 1670.13it/s]warmup should be done:  22%|       | 665/3000 [00:00<00:01, 1657.94it/s]warmup should be done:  23%|       | 680/3000 [00:00<00:01, 1696.72it/s]warmup should be done:  22%|       | 666/3000 [00:00<00:01, 1660.27it/s]warmup should be done:  22%|       | 674/3000 [00:00<00:01, 1676.92it/s]warmup should be done:  28%|       | 830/3000 [00:00<00:01, 1658.62it/s]warmup should be done:  28%|       | 846/3000 [00:00<00:01, 1696.37it/s]warmup should be done:  28%|       | 837/3000 [00:00<00:01, 1675.20it/s]warmup should be done:  27%|       | 812/3000 [00:00<00:01, 1620.27it/s]warmup should be done:  28%|       | 851/3000 [00:00<00:01, 1699.53it/s]warmup should be done:  28%|       | 832/3000 [00:00<00:01, 1659.63it/s]warmup should be done:  28%|       | 842/3000 [00:00<00:01, 1669.24it/s]warmup should be done:  28%|       | 833/3000 [00:00<00:01, 1648.36it/s]warmup should be done:  34%|      | 1016/3000 [00:00<00:01, 1696.33it/s]warmup should be done:  33%|      | 996/3000 [00:00<00:01, 1656.38it/s]warmup should be done:  34%|      | 1006/3000 [00:00<00:01, 1677.84it/s]warmup should be done:  34%|      | 1023/3000 [00:00<00:01, 1704.56it/s]warmup should be done:  32%|      | 975/3000 [00:00<00:01, 1617.71it/s]warmup should be done:  34%|      | 1009/3000 [00:00<00:01, 1665.25it/s]warmup should be done:  33%|      | 998/3000 [00:00<00:01, 1631.18it/s]warmup should be done:  33%|      | 998/3000 [00:00<00:01, 1625.23it/s]warmup should be done:  39%|      | 1162/3000 [00:00<00:01, 1656.16it/s]warmup should be done:  40%|      | 1186/3000 [00:00<00:01, 1693.45it/s]warmup should be done:  39%|      | 1175/3000 [00:00<00:01, 1679.17it/s]warmup should be done:  40%|      | 1194/3000 [00:00<00:01, 1703.23it/s]warmup should be done:  38%|      | 1137/3000 [00:00<00:01, 1614.38it/s]warmup should be done:  39%|      | 1176/3000 [00:00<00:01, 1658.88it/s]warmup should be done:  39%|      | 1164/3000 [00:00<00:01, 1635.19it/s]warmup should be done:  39%|      | 1162/3000 [00:00<00:01, 1623.01it/s]warmup should be done:  44%|     | 1328/3000 [00:00<00:01, 1654.77it/s]warmup should be done:  45%|     | 1357/3000 [00:00<00:00, 1697.05it/s]warmup should be done:  45%|     | 1345/3000 [00:00<00:00, 1682.99it/s]warmup should be done:  46%|     | 1365/3000 [00:00<00:00, 1698.49it/s]warmup should be done:  43%|     | 1299/3000 [00:00<00:01, 1612.56it/s]warmup should be done:  45%|     | 1344/3000 [00:00<00:00, 1662.64it/s]warmup should be done:  44%|     | 1330/3000 [00:00<00:01, 1640.07it/s]warmup should be done:  44%|     | 1325/3000 [00:00<00:01, 1616.13it/s]warmup should be done:  50%|     | 1494/3000 [00:00<00:00, 1655.92it/s]warmup should be done:  51%|     | 1527/3000 [00:00<00:00, 1695.76it/s]warmup should be done:  50%|     | 1514/3000 [00:00<00:00, 1684.58it/s]warmup should be done:  51%|     | 1535/3000 [00:00<00:00, 1696.43it/s]warmup should be done:  49%|     | 1461/3000 [00:00<00:00, 1606.19it/s]warmup should be done:  50%|     | 1512/3000 [00:00<00:00, 1665.87it/s]warmup should be done:  50%|     | 1496/3000 [00:00<00:00, 1645.35it/s]warmup should be done:  50%|     | 1487/3000 [00:00<00:00, 1613.19it/s]warmup should be done:  55%|    | 1661/3000 [00:01<00:00, 1658.25it/s]warmup should be done:  56%|    | 1684/3000 [00:01<00:00, 1686.24it/s]warmup should be done:  57%|    | 1697/3000 [00:01<00:00, 1692.03it/s]warmup should be done:  57%|    | 1706/3000 [00:01<00:00, 1700.29it/s]warmup should be done:  54%|    | 1622/3000 [00:01<00:00, 1604.08it/s]warmup should be done:  56%|    | 1680/3000 [00:01<00:00, 1667.64it/s]warmup should be done:  55%|    | 1663/3000 [00:01<00:00, 1650.86it/s]warmup should be done:  55%|    | 1649/3000 [00:01<00:00, 1613.21it/s]warmup should be done:  61%|    | 1828/3000 [00:01<00:00, 1659.30it/s]warmup should be done:  62%|   | 1854/3000 [00:01<00:00, 1688.45it/s]warmup should be done:  62%|   | 1868/3000 [00:01<00:00, 1695.36it/s]warmup should be done:  63%|   | 1878/3000 [00:01<00:00, 1703.48it/s]warmup should be done:  62%|   | 1847/3000 [00:01<00:00, 1666.31it/s]warmup should be done:  59%|    | 1783/3000 [00:01<00:00, 1599.99it/s]warmup should be done:  61%|    | 1829/3000 [00:01<00:00, 1647.70it/s]warmup should be done:  60%|    | 1811/3000 [00:01<00:00, 1611.24it/s]warmup should be done:  66%|   | 1994/3000 [00:01<00:00, 1657.98it/s]warmup should be done:  68%|   | 2039/3000 [00:01<00:00, 1697.34it/s]warmup should be done:  67%|   | 2023/3000 [00:01<00:00, 1683.32it/s]warmup should be done:  68%|   | 2049/3000 [00:01<00:00, 1705.11it/s]warmup should be done:  65%|   | 1945/3000 [00:01<00:00, 1603.74it/s]warmup should be done:  67%|   | 2014/3000 [00:01<00:00, 1660.38it/s]warmup should be done:  66%|   | 1994/3000 [00:01<00:00, 1642.87it/s]warmup should be done:  66%|   | 1973/3000 [00:01<00:00, 1607.40it/s]warmup should be done:  72%|  | 2160/3000 [00:01<00:00, 1655.56it/s]warmup should be done:  74%|  | 2209/3000 [00:01<00:00, 1695.25it/s]warmup should be done:  73%|  | 2192/3000 [00:01<00:00, 1683.13it/s]warmup should be done:  74%|  | 2220/3000 [00:01<00:00, 1704.39it/s]warmup should be done:  70%|   | 2106/3000 [00:01<00:00, 1603.81it/s]warmup should be done:  73%|  | 2181/3000 [00:01<00:00, 1661.65it/s]warmup should be done:  72%|  | 2160/3000 [00:01<00:00, 1646.59it/s]warmup should be done:  71%|   | 2134/3000 [00:01<00:00, 1607.63it/s]warmup should be done:  78%|  | 2327/3000 [00:01<00:00, 1657.65it/s]warmup should be done:  80%|  | 2391/3000 [00:01<00:00, 1704.83it/s]warmup should be done:  79%|  | 2361/3000 [00:01<00:00, 1682.76it/s]warmup should be done:  79%|  | 2379/3000 [00:01<00:00, 1686.78it/s]warmup should be done:  78%|  | 2350/3000 [00:01<00:00, 1668.00it/s]warmup should be done:  76%|  | 2267/3000 [00:01<00:00, 1591.93it/s]warmup should be done:  78%|  | 2326/3000 [00:01<00:00, 1649.00it/s]warmup should be done:  77%|  | 2296/3000 [00:01<00:00, 1609.41it/s]warmup should be done:  83%| | 2494/3000 [00:01<00:00, 1659.22it/s]warmup should be done:  85%| | 2563/3000 [00:01<00:00, 1707.10it/s]warmup should be done:  84%| | 2530/3000 [00:01<00:00, 1682.76it/s]warmup should be done:  85%| | 2549/3000 [00:01<00:00, 1688.56it/s]warmup should be done:  84%| | 2519/3000 [00:01<00:00, 1672.27it/s]warmup should be done:  81%|  | 2429/3000 [00:01<00:00, 1597.78it/s]warmup should be done:  83%| | 2493/3000 [00:01<00:00, 1653.90it/s]warmup should be done:  82%| | 2458/3000 [00:01<00:00, 1610.26it/s]warmup should be done:  89%| | 2660/3000 [00:01<00:00, 1657.35it/s]warmup should be done:  90%| | 2699/3000 [00:01<00:00, 1684.71it/s]warmup should be done:  91%| | 2735/3000 [00:01<00:00, 1708.30it/s]warmup should be done:  91%| | 2720/3000 [00:01<00:00, 1693.14it/s]warmup should be done:  90%| | 2688/3000 [00:01<00:00, 1675.68it/s]warmup should be done:  86%| | 2589/3000 [00:01<00:00, 1598.34it/s]warmup should be done:  89%| | 2659/3000 [00:01<00:00, 1654.09it/s]warmup should be done:  87%| | 2624/3000 [00:01<00:00, 1623.02it/s]warmup should be done:  94%|| 2826/3000 [00:01<00:00, 1656.33it/s]warmup should be done:  96%|| 2868/3000 [00:01<00:00, 1685.13it/s]warmup should be done:  97%|| 2906/3000 [00:01<00:00, 1708.06it/s]warmup should be done:  96%|| 2890/3000 [00:01<00:00, 1694.82it/s]warmup should be done:  95%|| 2856/3000 [00:01<00:00, 1676.84it/s]warmup should be done:  92%|| 2750/3000 [00:01<00:00, 1599.04it/s]warmup should be done:  94%|| 2825/3000 [00:01<00:00, 1654.16it/s]warmup should be done:  93%|| 2791/3000 [00:01<00:00, 1634.35it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1702.13it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1691.80it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1680.43it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1669.28it/s]warmup should be done: 100%|| 2993/3000 [00:01<00:00, 1659.05it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1656.65it/s]warmup should be done:  97%|| 2910/3000 [00:01<00:00, 1595.97it/s]warmup should be done: 100%|| 2992/3000 [00:01<00:00, 1657.52it/s]warmup should be done:  99%|| 2958/3000 [00:01<00:00, 1642.02it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1649.02it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1627.29it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1605.38it/s]2022-12-12 04:54:55.816976: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f0f87834ba0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 04:54:55.817040: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 04:54:55.866570: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7ef0d0031dc0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 04:54:55.866634: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 04:54:55.887508: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f0f8782cd70 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 04:54:55.887573: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 04:54:55.901895: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7ef11002d930 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 04:54:55.901965: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 04:54:55.928820: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7ef17c0295a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 04:54:55.928888: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 04:54:56.279604: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7ef1c002d940 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 04:54:56.279666: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 04:54:56.320834: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f0f83796c90 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 04:54:56.320905: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 04:54:56.333787: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f0f8b830ca0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 04:54:56.333855: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 04:54:58.102181: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 04:54:58.108620: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 04:54:58.135059: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 04:54:58.190471: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 04:54:58.228531: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 04:54:58.577800: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 04:54:58.642715: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 04:54:58.655361: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 04:55:00.970279: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 04:55:01.032568: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 04:55:01.043805: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 04:55:01.065286: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 04:55:01.160303: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 04:55:01.454661: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 04:55:01.587483: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 04:55:01.605692: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][04:55:26.228][ERROR][RK0][tid #139705194628864]: replica 1 reaches 1000, calling init pre replica
[HCTR][04:55:26.228][ERROR][RK0][tid #139705194628864]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][04:55:26.234][ERROR][RK0][tid #139705194628864]: coll ps creation done
[HCTR][04:55:26.234][ERROR][RK0][tid #139705194628864]: replica 1 waits for coll ps creation barrier
[HCTR][04:55:26.260][ERROR][RK0][main]: replica 4 reaches 1000, calling init pre replica
[HCTR][04:55:26.261][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][04:55:26.269][ERROR][RK0][main]: coll ps creation done
[HCTR][04:55:26.269][ERROR][RK0][main]: replica 4 waits for coll ps creation barrier
[HCTR][04:55:26.308][ERROR][RK0][main]: replica 2 reaches 1000, calling init pre replica
[HCTR][04:55:26.308][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][04:55:26.311][ERROR][RK0][tid #139705320453888]: replica 0 reaches 1000, calling init pre replica
[HCTR][04:55:26.311][ERROR][RK0][tid #139705320453888]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][04:55:26.314][ERROR][RK0][main]: coll ps creation done
[HCTR][04:55:26.314][ERROR][RK0][main]: replica 2 waits for coll ps creation barrier
[HCTR][04:55:26.316][ERROR][RK0][tid #139705320453888]: coll ps creation done
[HCTR][04:55:26.316][ERROR][RK0][tid #139705320453888]: replica 0 waits for coll ps creation barrier
[HCTR][04:55:26.321][ERROR][RK0][main]: replica 7 reaches 1000, calling init pre replica
[HCTR][04:55:26.321][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][04:55:26.324][ERROR][RK0][main]: replica 6 reaches 1000, calling init pre replica
[HCTR][04:55:26.324][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][04:55:26.325][ERROR][RK0][main]: coll ps creation done
[HCTR][04:55:26.325][ERROR][RK0][main]: replica 7 waits for coll ps creation barrier
[HCTR][04:55:26.331][ERROR][RK0][main]: coll ps creation done
[HCTR][04:55:26.331][ERROR][RK0][main]: replica 6 waits for coll ps creation barrier
[HCTR][04:55:26.338][ERROR][RK0][main]: replica 5 reaches 1000, calling init pre replica
[HCTR][04:55:26.338][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][04:55:26.342][ERROR][RK0][main]: replica 3 reaches 1000, calling init pre replica
[HCTR][04:55:26.342][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][04:55:26.342][ERROR][RK0][main]: coll ps creation done
[HCTR][04:55:26.342][ERROR][RK0][main]: replica 5 waits for coll ps creation barrier
[HCTR][04:55:26.346][ERROR][RK0][main]: coll ps creation done
[HCTR][04:55:26.346][ERROR][RK0][main]: replica 3 waits for coll ps creation barrier
[HCTR][04:55:26.346][ERROR][RK0][tid #139705320453888]: replica 0 preparing frequency
[HCTR][04:55:27.202][ERROR][RK0][tid #139705320453888]: replica 0 preparing frequency done
[HCTR][04:55:27.249][ERROR][RK0][tid #139705320453888]: replica 0 calling init per replica
[HCTR][04:55:27.249][ERROR][RK0][main]: replica 7 calling init per replica
[HCTR][04:55:27.249][ERROR][RK0][tid #139705194628864]: replica 1 calling init per replica
[HCTR][04:55:27.249][ERROR][RK0][main]: replica 5 calling init per replica
[HCTR][04:55:27.249][ERROR][RK0][main]: replica 4 calling init per replica
[HCTR][04:55:27.249][ERROR][RK0][main]: replica 2 calling init per replica
[HCTR][04:55:27.249][ERROR][RK0][main]: replica 6 calling init per replica
[HCTR][04:55:27.249][ERROR][RK0][main]: replica 3 calling init per replica
[HCTR][04:55:27.249][ERROR][RK0][tid #139705320453888]: Calling build_v2
[HCTR][04:55:27.249][ERROR][RK0][main]: Calling build_v2
[HCTR][04:55:27.249][ERROR][RK0][tid #139705194628864]: Calling build_v2
[HCTR][04:55:27.249][ERROR][RK0][main]: Calling build_v2
[HCTR][04:55:27.249][ERROR][RK0][main]: Calling build_v2
[HCTR][04:55:27.249][ERROR][RK0][main]: Calling build_v2
[HCTR][04:55:27.249][ERROR][RK0][main]: Calling build_v2
[HCTR][04:55:27.249][ERROR][RK0][main]: Calling build_v2
[HCTR][04:55:27.249][ERROR][RK0][tid #139705320453888]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][04:55:27.249][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][04:55:27.249][ERROR][RK0][tid #139705194628864]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][04:55:27.249][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][04:55:27.249][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][04:55:27.249][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][04:55:27.249][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][04:55:27.249][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[[[[[[[2022-12-12 04:55:272022-12-12 04:55:272022-12-12 04:55:272022-12-12 04:55:27[2022-12-12 04:55:27..2022-12-12 04:55:27..2022-12-12 04:55:27.2495152022-12-12 04:55:27249512.249512249511.249522: .: 249525: : 249525: E249525E: EE: E :  E  E /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.ccE/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc: :/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc::/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:136/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc136:136136:136] :] 136] ] 136] using concurrent impl MPS136using concurrent impl MPS] using concurrent impl MPSusing concurrent impl MPS] using concurrent impl MPS
] 
using concurrent impl MPS

using concurrent impl MPS
using concurrent impl MPS


[2022-12-12 04:55:27.253674: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-12 04:55:27.253714: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[:2022-12-12 04:55:27196.] 253720assigning 8 to cpu: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[[2022-12-12 04:55:272022-12-12 04:55:27..253763253769: [: E2022-12-12 04:55:27E . /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc253783/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:: :178E[196]  2022-12-12 04:55:27] v100x8, slow pcie/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.assigning 8 to cpu
:253808
212: [] E2022-12-12 04:55:27build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8 .
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[253843:2022-12-12 04:55:27: 178.E] [253854 [v100x8, slow pcie2022-12-12 04:55:27: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 04:55:27
.E:.253876[ [196253879: 2022-12-12 04:55:27/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 04:55:27] : E.:.assigning 8 to cpuE 253898178253906
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: ] : /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:[Ev100x8, slow pcieE:2122022-12-12 04:55:27 
 213] ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] [[build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8253949[::remote time is 8.684212022-12-12 04:55:272022-12-12 04:55:27
: 2022-12-12 04:55:27178196
..[E.] ] [2539922539962022-12-12 04:55:27 254004v100x8, slow pcieassigning 8 to cpu2022-12-12 04:55:27: : ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 

.EE254066:E254059[  : 178 : 2022-12-12 04:55:27/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE[.:: v100x8, slow pcie: 2022-12-12 04:55:27254133178196/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
212/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.: ] ] :] :254175[Ev100x8, slow pcieassigning 8 to cpu214build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8213: 2022-12-12 04:55:27 

] 
] E./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cccpu time is 97.0588[remote time is 8.68421 254244[:
2022-12-12 04:55:27
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 2022-12-12 04:55:27196[.:[E.] 2022-12-12 04:55:272543082122022-12-12 04:55:27 254319assigning 8 to cpu.: ] ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 
254346Ebuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8254361:E:  
: 196 E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[[ : assigning 8 to cpu:2022-12-12 04:55:272022-12-12 04:55:27/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc196/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
213..:] :] 254473254473212assigning 8 to cpu214remote time is 8.68421: : ] 
] 
EEbuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8cpu time is 97.0588[  
[
2022-12-12 04:55:27/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 04:55:27[.[212:.2022-12-12 04:55:272545912022-12-12 04:55:27] 213254601.: .build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8] : 254620E254625
remote time is 8.68421E:  : 
 E[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[ 2022-12-12 04:55:27: :2022-12-12 04:55:27/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.212/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc214.:254712] :] 254725212: build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8213cpu time is 97.0588: ] E
] 
Ebuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8 remote time is 8.68421[ 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
2022-12-12 04:55:27/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:.:[213[2548562142022-12-12 04:55:27] 2022-12-12 04:55:27: ] .remote time is 8.68421.Ecpu time is 97.0588254884
254884 
: : [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccEE2022-12-12 04:55:27:  .213/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc254963] ::: remote time is 8.68421213214E
] ]  remote time is 8.68421cpu time is 97.0588[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc

2022-12-12 04:55:27:.[2142550522022-12-12 04:55:27] : .cpu time is 97.0588E255084
 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE: 214/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] :cpu time is 97.0588214
] cpu time is 97.0588
[2022-12-12 04:56:46.748154: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-12 04:56:46.803898: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
block 0 storage is 00010001
	access is	0	0	0	0	4	4	4	4	
block 1 storage is 00100010
	access is	1	1	1	1	5	5	5	5	
block 2 storage is 01000100
	access is	2	2	2	2	6	6	6	6	
block 3 storage is 10001000
	access is	3	3	3	3	7	7	7	7	
block 4 storage is 00000000
	access is	8	8	8	8	8	8	8	8	
[2022-12-12 04:56:46.928189: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-12 04:56:46.928249: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-12 04:56:46.928283: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-12 04:56:46.928314: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-12 04:56:46.928792: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 04:56:46.928844: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:56:46.929753: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:56:46.930398: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:56:46.943583: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 7 solved
[2022-12-12 04:56:46.943671: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 7 initing device 7
[2022-12-12 04:56:46.943848: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 1 solved
[2022-12-12 04:56:46.943911: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 1 initing device 1
[2022-12-12 04:56:46.943926: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 2 solved
[2022-12-12 04:56:46.943979: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 2 initing device 2
[2022-12-12 04:56:46.944129: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 04:56:46.944182: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] [eager alloc mem 381.47 MB2022-12-12 04:56:46
.944188: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 4 solved
[2022-12-12 04:56:46.944246: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 4 initing device 4[
[2022-12-12 04:56:462022-12-12 04:56:46..944250944253: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::[2022022022-12-12 04:56:46] ] 3 solved.5 solved
944336
[: [[2022-12-12 04:56:46E[2022-12-12 04:56:462022-12-12 04:56:46. 2022-12-12 04:56:46..944364/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.944384944389: :944394: : E1815: EE ] E  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.ccBuilding Coll Cache with ... num gpu device is 8 /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::202:205205] 1815] ] 6 solved[] worker 0 thread 3 initing device 3worker 0 thread 5 initing device 5
2022-12-12 04:56:46Building Coll Cache with ... num gpu device is 8

.
[9445302022-12-12 04:56:46: .E944572 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE:[ 19802022-12-12 04:56:46/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc] .:eager alloc mem 381.47 MB944602205
: ] Eworker 0 thread 6 initing device 6 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:56:46.944674: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-12 04:56:46.944720: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:56:46.944976: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8[
2022-12-12 04:56:46.944997: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[[2022-12-12 04:56:462022-12-12 04:56:46..945028945029: : EE[  2022-12-12 04:56:46/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.::94505419801815: ] ] Eeager alloc mem 381.47 MBBuilding Coll Cache with ... num gpu device is 8 

/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:56:46.945154: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:56:46.948395: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:56:46.948761: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:56:46.948814: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:56:46.948868: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:56:46.949365: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:56:46.949431: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:56:46.949485: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:56:46.952741: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:56:46.952990: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:56:46.953035: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:56:46.953085: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:56:46.953138: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:56:46.953638: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:56:46.953690: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 04:56:47.  5009: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.00 Bytes
[2022-12-12 04:56:47. 22692: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 04:56:47. 22777: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 04:56:47. 28680: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 76.68 MB
[2022-12-12 04:56:47. 29471: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:56:47. 30556: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:56:47. 30602: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 76.29 MB
[2022-12-12 04:56:47. 34172: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.00 Bytes
[2022-12-12 04:56:47. 39282: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 04:56:47. 39360: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 04:56:47. 40131: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 76.68 MB
[2022-12-12 04:56:47. 40699: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:56:47. 41738: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:56:47. 41784: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 76.29 MB
[[2022-12-12 04:56:472022-12-12 04:56:47.. 45802 45802: : EE[  [[[2022-12-12 04:56:47/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-12 04:56:472022-12-12 04:56:472022-12-12 04:56:47.::... 4586519801980 45866 45866 45868: ] ] : : : Eeager alloc mem 5.00 Byteseager alloc mem 5.00 BytesEEE 

   /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::1980198019801980] ] ] ] eager alloc mem 5.00 Byteseager alloc mem 5.00 Byteseager alloc mem 5.00 Byteseager alloc mem 5.00 Bytes



[2022-12-12 04:56:47. 52255: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[[2022-12-12 04:56:472022-12-12 04:56:47.. 52339 52360: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 5eager release cuda mem 400000000

[2022-12-12 04:56:47. 52410: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 04:56:47. 52455: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 04:56:47. 52489: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 04:56:47. 52503: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-12 04:56:47. 52587: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] [eager release cuda mem 4000000002022-12-12 04:56:47
. 52590: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[[2022-12-12 04:56:472022-12-12 04:56:47.. 52657 52677: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 5eager release cuda mem 400000000

[2022-12-12 04:56:47. 52762: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 04:56:47. 53423: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 76.68 MB
[2022-12-12 04:56:47. 54155: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 76.68 MB
[2022-12-12 04:56:47. 54977: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 76.68 MB
[2022-12-12 04:56:47. 55734: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 76.68 MB
[2022-12-12 04:56:47. 56295: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 76.68 MB
[2022-12-12 04:56:47. 56924: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 76.68 MB
[2022-12-12 04:56:47. 57818: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:56:47. 58011: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:56:47. 58181: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:56:47. 58282: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:56:47. 58328: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:56:47. 58374: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:56:47. 58881: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:56:47. 58927: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 76.29 MB
[2022-12-12 04:56:47. 59056: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:56:47. 59101: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 76.29 MB
[2022-12-12 04:56:47. 59235: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:56:47. 59280: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 76.29 MB
[2022-12-12 04:56:47. 59330: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:56:47. 59375: W[ 2022-12-12 04:56:47/hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc.: 5938143: ] EWORKER[0] alloc host memory 76.29 MB 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:56:47. 59436: [E2022-12-12 04:56:47 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc 59449:: 638W]  eager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc
:43] WORKER[0] alloc host memory 76.29 MB
[2022-12-12 04:56:47. 59497: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 76.29 MB
[2022-12-12 04:56:47. 80873: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 04:56:47. 81491: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 04:56:47. 81540: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 9.54 GB
[2022-12-12 04:56:47. 91242: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 04:56:47. 91847: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 04:56:47. 91890: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 9.54 GB
[2022-12-12 04:56:47.107992: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 04:56:47.108632: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 04:56:47.108676: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 9.54 GB
[2022-12-12 04:56:47.108697: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 04:56:47.108802: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 04:56:47.108850: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 04:56:47.109313: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 04:56:47.109355: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 9.54 GB
[2022-12-12 04:56:47.109402: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 04:56:47.109443: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-12 04:56:47:.1980109452] : eager alloc mem 9.54 GBE
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 04:56:47.109507: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 9.54 GB
[2022-12-12 04:56:47.109696: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 04:56:47.110295: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 04:56:47.110333: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 9.54 GB
[2022-12-12 04:56:47.110432: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 04:56:47.111051: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 04:56:47.111093: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 9.54 GB
[[2022-12-12 04:56:50[.2022-12-12 04:56:50[705445.[2022-12-12 04:56:50: [7054362022-12-12 04:56:50[.E[: 2022-12-12 04:56:502022-12-12 04:56:50.2022-12-12 04:56:50705448 E2022-12-12 04:56:50..705436.: /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu .705437705436: 705436E:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu705436: : E:  1926:: EE E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] 1926E  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :Device 0 init p2p of link 3]  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1926
Device 6 init p2p of link 0/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::1926:] 
Device 3 init p2p of link 2:19261926] 1926
1926] ] Device 1 init p2p of link 7Device 2 init p2p of link 1] ] Device 5 init p2p of link 6

Device 7 init p2p of link 4Device 4 init p2p of link 5


[2022-12-12 04:56:50.706326: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-12 04:56:50:.1980706338[] : 2022-12-12 04:56:50[eager alloc mem 611.00 KB[E.[2022-12-12 04:56:50[
2022-12-12 04:56:50 7063692022-12-12 04:56:50.2022-12-12 04:56:50[./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: .706376.2022-12-12 04:56:50706380:E706389: 706393.: 1980 : E: 706420E] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE E:  eager alloc mem 611.00 KB: /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :] :1980:1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980eager alloc mem 611.00 KB1980] ] :] 
] eager alloc mem 611.00 KBeager alloc mem 611.00 KB1980eager alloc mem 611.00 KBeager alloc mem 611.00 KB

] 

eager alloc mem 611.00 KB
[2022-12-12 04:56:50.707330: [E2022-12-12 04:56:50 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc707345:: 638E]  eager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[
:2022-12-12 04:56:50638.] 707397eager release cuda mem 625663: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[[2022-12-12 04:56:50[2022-12-12 04:56:50[.2022-12-12 04:56:50.2022-12-12 04:56:50707525.707530.: 707531: 707536[E: E: 2022-12-12 04:56:50 E E./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc 707571:/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: 638:638:E] 638] 638 eager release cuda mem 625663] eager release cuda mem 625663] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
eager release cuda mem 625663
eager release cuda mem 625663:

638] eager release cuda mem 625663
[2022-12-12 04:56:50.721320: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 5
[2022-12-12 04:56:50.721496: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:56:50.721640: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 2
[2022-12-12 04:56:50.721793: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:56:50.722413: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:56:50.722743: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:56:50.730134: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 6
[2022-12-12 04:56:50.730299: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:56:50.730593: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 1
[2022-12-12 04:56:50.730749: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:56:50.730876: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 3
[2022-12-12 04:56:50.[7310052022-12-12 04:56:50: .E731029 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: 1926[/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] 2022-12-12 04:56:50:Device 3 init p2p of link 0.1980
731074] : eager alloc mem 611.00 KBE
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:56:50.731244: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:56:50.731265: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 7
[2022-12-12 04:56:50.731371: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 4
[2022-12-12 04:56:50.731479: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:56:50.731523: E[ 2022-12-12 04:56:50/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.:7315341980: ] Eeager alloc mem 611.00 KB 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:56:50.732054: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:56:50.732158: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:56:50.732456[: 2022-12-12 04:56:50E. 732477/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :E638 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 625663:
638] eager release cuda mem 625663
[2022-12-12 04:56:50.735204: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 4
[2022-12-12 04:56:50.735327: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:56:50.735372: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 3
[2022-12-12 04:56:50.735491: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:56:50.736236: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:56:50.736397: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:56:50.743958: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 1
[2022-12-12 04:56:50.744089: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:56:50.744260: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 6
[2022-12-12 04:56:50.744404: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:56:50.745000: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:56:50.745316: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:56:50.752808: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 0
[2022-12-12 04:56:50.752931: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:56:50.753702: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 5
[2022-12-12 04:56:50.753830[: 2022-12-12 04:56:50E. 753835/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: :E1980 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager alloc mem 611.00 KB:
638] eager release cuda mem 625663
[2022-12-12 04:56:50.753931: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 7
[2022-12-12 04:56:50.754071: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:56:50.754249: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 2
[2022-12-12 04:56:50.754430: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:56:50.754661: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:56:50.754975: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:56:50.755249: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:56:50.762314: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 1
[2022-12-12 04:56:50.762445: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:56:50.763046: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 6
[2022-12-12 04:56:50.763218: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:56:50.763375: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:56:50.764180: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:56:50.764561: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 7
[2022-12-12 04:56:50.764679: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:56:50.764929: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 0
[2022-12-12 04:56:50.765050: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:56:50.765593: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:56:50.765972: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:56:50.767179: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 2
[2022-12-12 04:56:50.767301: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:56:50.767575: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 5
[2022-12-12 04:56:50.767695: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:56:50.768227: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:56:50.768611: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:56:50.777245: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 4
[2022-12-12 04:56:50.777368: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:56:50.778144: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:56:50.779516: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 3
[2022-12-12 04:56:50.779643: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 04:56:50.780415: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 04:56:50.780465: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 80400000
[2022-12-12 04:56:50.781604: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 20000000 / 100000000 nodes ( 20.00 %~20.00 %) | remote 60000000 / 100000000 nodes ( 60.00 %) | cpu 20000000 / 100000000 nodes ( 20.00 %) | 9.54 GB | 3.83701 secs 
[2022-12-12 04:56:50.783659: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 80400000
[2022-12-12 04:56:50.784814: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 20000000 / 100000000 nodes ( 20.00 %~20.00 %) | remote 60000000 / 100000000 nodes ( 60.00 %) | cpu 20000000 / 100000000 nodes ( 20.00 %) | 9.54 GB | 3.8398 secs 
[2022-12-12 04:56:50.788539: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 80400000
[2022-12-12 04:56:50.790755: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 80400000
[2022-12-12 04:56:50.791401: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 80400000
[2022-12-12 04:56:50.792159: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 80400000
[2022-12-12 04:56:50.793116: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 80400000
[2022-12-12 04:56:50.793668: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 80400000
[2022-12-12 04:56:50.812699: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 20000000 / 100000000 nodes ( 20.00 %~20.00 %) | remote 60000000 / 100000000 nodes ( 60.00 %) | cpu 20000000 / 100000000 nodes ( 20.00 %) | 9.54 GB | 3.86766 secs 
[2022-12-12 04:56:50.812933: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 20000000 / 100000000 nodes ( 20.00 %~20.00 %) | remote 60000000 / 100000000 nodes ( 60.00 %) | cpu 20000000 / 100000000 nodes ( 20.00 %) | 9.54 GB | 3.86822 secs 
[2022-12-12 04:56:50.813084: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 20000000 / 100000000 nodes ( 20.00 %~20.00 %) | remote 60000000 / 100000000 nodes ( 60.00 %) | cpu 20000000 / 100000000 nodes ( 20.00 %) | 9.54 GB | 3.86794 secs 
[2022-12-12 04:56:50.813259: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 20000000 / 100000000 nodes ( 20.00 %~20.00 %) | remote 60000000 / 100000000 nodes ( 60.00 %) | cpu 20000000 / 100000000 nodes ( 20.00 %) | 9.54 GB | 3.86874 secs 
[2022-12-12 04:56:50.813425: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 20000000 / 100000000 nodes ( 20.00 %~20.00 %) | remote 60000000 / 100000000 nodes ( 60.00 %) | cpu 20000000 / 100000000 nodes ( 20.00 %) | 9.54 GB | 3.88459 secs 
[2022-12-12 04:56:50.813642: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 20000000 / 100000000 nodes ( 20.00 %~20.00 %) | remote 60000000 / 100000000 nodes ( 60.00 %) | cpu 20000000 / 100000000 nodes ( 20.00 %) | 9.54 GB | 3.86948 secs 
[2022-12-12 04:56:50.813921: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 16.18 GB
[2022-12-12 04:56:52.168899: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 16.44 GB
[2022-12-12 04:56:52.169167: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 16.44 GB
[2022-12-12 04:56:52.170293: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 16.44 GB
[2022-12-12 04:56:53.560433: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 16.71 GB
[2022-12-12 04:56:53.561241: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 16.71 GB
[2022-12-12 04:56:53.562116: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 16.71 GB
[2022-12-12 04:56:54.904575: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 16.92 GB
[2022-12-12 04:56:54.904727: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 16.92 GB
[2022-12-12 04:56:54.905052: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 16.92 GB
[2022-12-12 04:56:56.487784: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 17.14 GB
[2022-12-12 04:56:56.488141: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 17.14 GB
[2022-12-12 04:56:56.488640: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 17.14 GB
[2022-12-12 04:56:58. 78918: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 17.59 GB
[2022-12-12 04:56:58. 79107: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 17.59 GB
[2022-12-12 04:56:58. 79930: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 17.59 GB
[2022-12-12 04:56:59.665066: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 17.79 GB
[2022-12-12 04:56:59.665265: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 17.79 GB
[HCTR][04:56:59.716][ERROR][RK0][tid #139705194628864]: replica 1 calling init per replica done, doing barrier
[HCTR][04:56:59.716][ERROR][RK0][main]: replica 2 calling init per replica done, doing barrier
[HCTR][04:56:59.716][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier
[HCTR][04:56:59.716][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier
[HCTR][04:56:59.716][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier
[HCTR][04:56:59.716][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier
[HCTR][04:56:59.716][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier
[HCTR][04:56:59.716][ERROR][RK0][tid #139705320453888]: replica 0 calling init per replica done, doing barrier
[HCTR][04:56:59.716][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier done
[HCTR][04:56:59.716][ERROR][RK0][tid #139705320453888]: replica 0 calling init per replica done, doing barrier done
[HCTR][04:56:59.716][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier done
[HCTR][04:56:59.716][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier done
[HCTR][04:56:59.716][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier done
[HCTR][04:56:59.716][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier done
[HCTR][04:56:59.716][ERROR][RK0][main]: replica 2 calling init per replica done, doing barrier done
[HCTR][04:56:59.716][ERROR][RK0][tid #139705194628864]: replica 1 calling init per replica done, doing barrier done
[HCTR][04:56:59.716][ERROR][RK0][main]: init per replica done
[HCTR][04:56:59.716][ERROR][RK0][main]: init per replica done
[HCTR][04:56:59.716][ERROR][RK0][main]: init per replica done
[HCTR][04:56:59.716][ERROR][RK0][main]: init per replica done
[HCTR][04:56:59.716][ERROR][RK0][main]: init per replica done
[HCTR][04:56:59.716][ERROR][RK0][main]: init per replica done
[HCTR][04:56:59.716][ERROR][RK0][tid #139705194628864]: init per replica done
[HCTR][04:56:59.719][ERROR][RK0][tid #139705320453888]: init per replica done
[HCTR][04:56:59.722][ERROR][RK0][main]: 4 allocated 3276800 at 0x7efcab320000
[HCTR][04:56:59.722][ERROR][RK0][main]: 4 allocated 6553600 at 0x7f117aa00000
[HCTR][04:56:59.722][ERROR][RK0][main]: 4 allocated 3276800 at 0x7f117b040000
[HCTR][04:56:59.722][ERROR][RK0][main]: 4 allocated 6553600 at 0x7f117b360000
[HCTR][04:56:59.722][ERROR][RK0][main]: 1 allocated 3276800 at 0x7efca7320000
[HCTR][04:56:59.722][ERROR][RK0][main]: 1 allocated 6553600 at 0x7f117ca00000
[HCTR][04:56:59.722][ERROR][RK0][main]: 1 allocated 3276800 at 0x7f117d040000
[HCTR][04:56:59.722][ERROR][RK0][main]: 1 allocated 6553600 at 0x7f117d360000
[HCTR][04:56:59.723][ERROR][RK0][main]: 3 allocated 3276800 at 0x7efca7320000
[HCTR][04:56:59.723][ERROR][RK0][main]: 5 allocated 3276800 at 0x7efcab320000
[HCTR][04:56:59.723][ERROR][RK0][main]: 3 allocated 6553600 at 0x7f117aa00000
[HCTR][04:56:59.722][ERROR][RK0][tid #139705588889344]: 2 allocated 3276800 at 0x7efca7320000
[HCTR][04:56:59.723][ERROR][RK0][main]: 5 allocated 6553600 at 0x7f117ca00000
[HCTR][04:56:59.723][ERROR][RK0][main]: 3 allocated 3276800 at 0x7f117b040000
[HCTR][04:56:59.723][ERROR][RK0][tid #139705588889344]: 2 allocated 6553600 at 0x7f117ca00000
[HCTR][04:56:59.723][ERROR][RK0][main]: 5 allocated 3276800 at 0x7f117d040000
[HCTR][04:56:59.723][ERROR][RK0][main]: 3 allocated 6553600 at 0x7f117b360000
[HCTR][04:56:59.723][ERROR][RK0][tid #139705588889344]: 2 allocated 3276800 at 0x7f117d040000
[HCTR][04:56:59.723][ERROR][RK0][main]: 5 allocated 6553600 at 0x7f117d360000
[HCTR][04:56:59.723][ERROR][RK0][tid #139705588889344]: 2 allocated 6553600 at 0x7f117d360000
[HCTR][04:56:59.723][ERROR][RK0][tid #139705186236160]: 6 allocated 3276800 at 0x7efc9f320000
[HCTR][04:56:59.723][ERROR][RK0][tid #139705186236160]: 6 allocated 6553600 at 0x7f117ca00000
[HCTR][04:56:59.723][ERROR][RK0][main]: 7 allocated 3276800 at 0x7efca7320000
[HCTR][04:56:59.723][ERROR][RK0][tid #139705186236160]: 6 allocated 3276800 at 0x7f117d040000
[HCTR][04:56:59.723][ERROR][RK0][main]: 7 allocated 6553600 at 0x7f1172a00000
[HCTR][04:56:59.723][ERROR][RK0][tid #139705186236160]: 6 allocated 6553600 at 0x7f117d360000
[HCTR][04:56:59.723][ERROR][RK0][main]: 7 allocated 3276800 at 0x7f1173040000
[HCTR][04:56:59.723][ERROR][RK0][main]: 7 allocated 6553600 at 0x7f1173360000
[HCTR][04:56:59.725][ERROR][RK0][tid #139705320453888]: 0 allocated 3276800 at 0x7f117e920000
[HCTR][04:56:59.725][ERROR][RK0][tid #139705320453888]: 0 allocated 6553600 at 0x7f117ee00000
[HCTR][04:56:59.725][ERROR][RK0][tid #139705320453888]: 0 allocated 3276800 at 0x7f117fb0e800
[HCTR][04:56:59.725][ERROR][RK0][tid #139705320453888]: 0 allocated 6553600 at 0x7f117fe2e800








