2022-12-11 23:35:17.219662: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.224044: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.231119: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.235323: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.242843: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.254618: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.262223: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.266623: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.316585: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.327157: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.332437: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.333129: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.341136: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.343563: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.345960: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.347050: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.347477: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.348718: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.348782: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.350638: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.350672: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.352474: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.352544: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.354226: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.354335: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.355718: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.356062: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.357094: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.358256: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.359201: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.360072: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.361045: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.362862: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.364073: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.365126: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.366131: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.367483: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.369237: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.369926: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.370523: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.371548: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.372058: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.373104: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.374262: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.375272: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.376290: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.377316: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.377802: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 23:35:17.378332: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.386906: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.387795: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.387971: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.388427: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.390067: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.390523: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.391069: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.392684: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.393355: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.393371: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.393496: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.396413: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.396490: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.396555: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.399655: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.399707: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.399784: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.400615: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.401911: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.403166: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.403241: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.403301: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.404516: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.405330: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.405617: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.407069: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.407171: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.407201: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.408451: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.409411: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.409487: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.411094: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.411361: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.412476: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.412793: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.413342: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.414850: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.415741: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.415835: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.416435: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.417626: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.418466: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.418514: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.419162: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.421141: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.421460: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.421745: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.422715: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.423273: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.423415: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.423651: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.425356: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.427267: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.427759: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.428603: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.429337: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.430443: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.431217: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.455491: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.458317: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.464090: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.466439: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.467584: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.468287: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.468333: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.468364: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.469589: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.470625: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.470941: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.472306: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.472490: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.472584: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.473522: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.475565: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.475780: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.476832: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.476887: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.477886: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.477978: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.479143: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.480721: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.481002: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.482462: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.482693: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.482735: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.482887: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.484238: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.485613: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.486479: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.487723: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.487807: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.487939: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.487983: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.490014: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.491001: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.491661: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.493073: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.493207: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.493304: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.493343: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.495952: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.496281: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.496894: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.498780: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.498821: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.498938: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.499138: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.501624: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.501849: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.502459: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.503834: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.503994: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.504047: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.504191: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.506657: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.506793: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.507338: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.508626: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.508685: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.508790: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.508927: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.511227: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.511412: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.511827: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.513974: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.514021: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.514178: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.514264: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.517350: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.517391: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.517793: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.519276: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.519325: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.519371: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.519626: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.522128: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.522140: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.522434: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.524070: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.524277: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.524446: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.524723: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.527234: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.527450: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.528350: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.528860: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.528949: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.529207: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.529520: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.531560: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.533030: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.533213: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.533298: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.533492: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.533769: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.535768: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.536110: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 23:35:17.537035: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.537129: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.537288: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.537509: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.537671: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.539589: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.541329: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.541348: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.542797: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.543052: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.543696: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.544623: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.545887: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.545919: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.545932: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.547152: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.547493: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.547873: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.549803: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.550528: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.550740: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.550757: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.551721: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.552313: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.552744: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.554830: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.555571: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.555709: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.556612: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.556873: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.557009: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.557444: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.559285: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.560525: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.561837: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.562132: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.562201: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.563079: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.565790: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.568337: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.568425: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.568471: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.568718: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.570255: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.571641: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 23:35:17.572570: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.572707: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.572859: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.573059: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.574115: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.577017: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.577085: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.577233: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.577329: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.578616: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.581043: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.581130: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.582359: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.584748: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 23:35:17.584921: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 23:35:17.584920: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 23:35:17.585030: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.585050: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.587808: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.587903: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.589970: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 23:35:17.593693: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 23:35:17.595058: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.595264: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.595456: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.599976: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.603444: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.606338: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.606782: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.606983: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.609347: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.609425: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.611158: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.612047: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.612098: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.614189: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:17.614222: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:18.762849: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:18.763488: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:18.764289: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:18.764761: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 23:35:18.764819: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-11 23:35:18.783236: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:18.783868: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:18.784382: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:18.784949: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:18.785474: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:18.785948: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-11 23:35:18.832388: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:35:18.832593: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:35:18.859842: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-11 23:35:19.004704: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:19.005452: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:19.005982: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:19.006459: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 23:35:19.006512: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-11 23:35:19.024716: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:19.025364: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:19.026289: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:19.027031: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:19.027955: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:19.028472: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-11 23:35:19.048128: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:19.048752: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:19.049298: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:19.049763: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 23:35:19.049817: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-11 23:35:19.067532: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:19.068376: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:19.068888: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:19.069468: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:19.069988: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:19.070452: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-11 23:35:19.103981: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:19.104568: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:19.105325: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:19.105788: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 23:35:19.105833: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-11 23:35:19.106410: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:19.107013: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:19.107550: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:19.108022: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 23:35:19.108073: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-11 23:35:19.109744: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:19.110336: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:19.110849: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:19.111295: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:19.111369: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 23:35:19.111420: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-11 23:35:19.112191: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:19.112747: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:19.113247: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 23:35:19.113295: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-11 23:35:19.123455: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:19.124083: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:19.124582: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:19.125173: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:19.125678: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:19.125954: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:19.126314: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-11 23:35:19.126708: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:19.127631: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:19.128208: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:19.128717: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:19.129199: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-11 23:35:19.129642: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:19.130311: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:19.130815: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:19.131290: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:19.131421: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:19.132506: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:19.132638: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:19.132701: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:19.133725: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:19.133911: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:35:19.134073: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:35:19.134195: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:19.134220: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-11 23:35:19.134913: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:19.135064: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:19.135974: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:19.136041: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-11 23:35:19.136058: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 23:35:19.136128: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-11 23:35:19.136685: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-11 23:35:19.153609: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:19.154288: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:19.154789: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:19.155396: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:19.155971: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 23:35:19.156449: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-11 23:35:19.167180: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:35:19.167399: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:35:19.169238: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
2022-12-11 23:35:19.170720: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:35:19.170903: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:35:19.172696: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
2022-12-11 23:35:19.173450: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:35:19.173584: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:35:19.175924: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
2022-12-11 23:35:19.179160: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:35:19.179301: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:35:19.181223: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
2022-12-11 23:35:19.182452: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:35:19.182607: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:35:19.184470: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
2022-12-11 23:35:19.201131: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:35:19.201318: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 23:35:19.203169: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
[HCTR][23:35:20.438][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][23:35:20.455][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][23:35:20.455][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][23:35:20.455][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][23:35:20.462][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][23:35:20.462][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][23:35:20.462][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][23:35:20.462][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:01,  1.55s/it]warmup run: 1it [00:01,  1.54s/it]warmup run: 101it [00:01, 84.97it/s]warmup run: 1it [00:01,  1.53s/it]warmup run: 1it [00:01,  1.51s/it]warmup run: 1it [00:01,  1.51s/it]warmup run: 98it [00:01, 83.07it/s]warmup run: 204it [00:01, 186.63it/s]warmup run: 1it [00:01,  1.49s/it]warmup run: 1it [00:01,  1.51s/it]warmup run: 1it [00:01,  1.51s/it]warmup run: 93it [00:01, 79.92it/s]warmup run: 99it [00:01, 84.38it/s]warmup run: 96it [00:01, 82.72it/s]warmup run: 197it [00:01, 181.10it/s]warmup run: 307it [00:01, 298.87it/s]warmup run: 100it [00:01, 87.42it/s]warmup run: 97it [00:01, 83.67it/s]warmup run: 98it [00:01, 84.46it/s]warmup run: 189it [00:01, 176.33it/s]warmup run: 194it [00:01, 178.57it/s]warmup run: 192it [00:01, 179.10it/s]warmup run: 296it [00:01, 289.25it/s]warmup run: 410it [00:01, 415.11it/s]warmup run: 200it [00:01, 188.85it/s]warmup run: 194it [00:01, 181.15it/s]warmup run: 196it [00:01, 182.73it/s]warmup run: 285it [00:01, 282.38it/s]warmup run: 288it [00:01, 280.80it/s]warmup run: 289it [00:01, 286.07it/s]warmup run: 395it [00:01, 401.28it/s]warmup run: 512it [00:02, 526.51it/s]warmup run: 300it [00:01, 299.88it/s]warmup run: 293it [00:01, 290.81it/s]warmup run: 295it [00:01, 291.92it/s]warmup run: 380it [00:01, 390.48it/s]warmup run: 380it [00:01, 383.28it/s]warmup run: 386it [00:01, 396.60it/s]warmup run: 494it [00:02, 510.59it/s]warmup run: 615it [00:02, 631.24it/s]warmup run: 401it [00:01, 415.72it/s]warmup run: 392it [00:01, 403.85it/s]warmup run: 389it [00:01, 396.87it/s]warmup run: 478it [00:02, 501.12it/s]warmup run: 475it [00:02, 488.60it/s]warmup run: 483it [00:02, 503.82it/s]warmup run: 595it [00:02, 614.71it/s]warmup run: 717it [00:02, 719.12it/s]warmup run: 501it [00:01, 525.82it/s]warmup run: 491it [00:02, 513.73it/s]warmup run: 487it [00:02, 505.72it/s]warmup run: 577it [00:02, 604.31it/s]warmup run: 571it [00:02, 586.40it/s]warmup run: 582it [00:02, 605.70it/s]warmup run: 696it [00:02, 704.88it/s]warmup run: 821it [00:02, 796.55it/s]warmup run: 591it [00:02, 616.30it/s]warmup run: 602it [00:02, 627.78it/s]warmup run: 586it [00:02, 607.46it/s]warmup run: 676it [00:02, 693.54it/s]warmup run: 664it [00:02, 665.34it/s]warmup run: 680it [00:02, 692.38it/s]warmup run: 795it [00:02, 774.58it/s]warmup run: 924it [00:02, 857.29it/s]warmup run: 691it [00:02, 704.32it/s]warmup run: 704it [00:02, 718.54it/s]warmup run: 684it [00:02, 693.65it/s]warmup run: 771it [00:02, 756.63it/s]warmup run: 756it [00:02, 727.61it/s]warmup run: 779it [00:02, 765.69it/s]warmup run: 893it [00:02, 827.83it/s]warmup run: 1027it [00:02, 903.87it/s]warmup run: 803it [00:02, 785.56it/s]warmup run: 791it [00:02, 776.30it/s]warmup run: 782it [00:02, 763.87it/s]warmup run: 870it [00:02, 817.22it/s]warmup run: 848it [00:02, 775.90it/s]warmup run: 876it [00:02, 818.46it/s]warmup run: 992it [00:02, 870.84it/s]warmup run: 1129it [00:02, 867.77it/s]warmup run: 905it [00:02, 844.76it/s]warmup run: 891it [00:02, 832.92it/s]warmup run: 878it [00:02, 812.86it/s]warmup run: 967it [00:02, 848.79it/s]warmup run: 940it [00:02, 810.01it/s]warmup run: 974it [00:02, 860.81it/s]warmup run: 1092it [00:02, 906.49it/s]warmup run: 1224it [00:02, 889.18it/s]warmup run: 990it [00:02, 875.39it/s]warmup run: 1006it [00:02, 887.90it/s]warmup run: 974it [00:02, 849.96it/s]warmup run: 1032it [00:02, 839.71it/s]warmup run: 1063it [00:02, 873.63it/s]warmup run: 1073it [00:02, 896.91it/s]warmup run: 1194it [00:02, 936.09it/s]warmup run: 1324it [00:02, 919.89it/s]warmup run: 1107it [00:02, 921.68it/s]warmup run: 1091it [00:02, 910.58it/s]warmup run: 1074it [00:02, 889.93it/s]warmup run: 1130it [00:02, 878.30it/s]warmup run: 1158it [00:02, 893.14it/s]warmup run: 1175it [00:02, 929.77it/s]warmup run: 1295it [00:02, 956.79it/s]warmup run: 1425it [00:02, 945.40it/s]warmup run: 1209it [00:02, 947.44it/s]warmup run: 1192it [00:02, 936.41it/s]warmup run: 1174it [00:02, 920.40it/s]warmup run: 1231it [00:02, 914.55it/s]warmup run: 1253it [00:02, 909.13it/s]warmup run: 1276it [00:02, 950.98it/s]warmup run: 1395it [00:02, 966.89it/s]warmup run: 1528it [00:03, 967.40it/s]warmup run: 1292it [00:02, 953.73it/s]warmup run: 1310it [00:02, 961.33it/s]warmup run: 1272it [00:02, 935.40it/s]warmup run: 1348it [00:02, 920.46it/s]warmup run: 1327it [00:02, 922.58it/s]warmup run: 1378it [00:02, 969.41it/s]warmup run: 1495it [00:03, 976.54it/s]warmup run: 1630it [00:03, 980.06it/s]warmup run: 1393it [00:02, 967.89it/s]warmup run: 1370it [00:02, 947.42it/s]warmup run: 1410it [00:02, 960.59it/s]warmup run: 1444it [00:03, 930.89it/s]warmup run: 1423it [00:03, 931.00it/s]warmup run: 1480it [00:03, 982.12it/s]warmup run: 1596it [00:03, 983.60it/s]warmup run: 1732it [00:03, 991.59it/s]warmup run: 1494it [00:03, 978.14it/s]warmup run: 1469it [00:03, 959.52it/s]warmup run: 1509it [00:03, 962.85it/s]warmup run: 1541it [00:03, 940.93it/s]warmup run: 1518it [00:03, 935.44it/s]warmup run: 1581it [00:03, 982.62it/s]warmup run: 1697it [00:03, 989.59it/s]warmup run: 1833it [00:03, 996.03it/s]warmup run: 1594it [00:03, 982.67it/s]warmup run: 1567it [00:03, 965.48it/s]warmup run: 1608it [00:03, 963.68it/s]warmup run: 1640it [00:03, 953.82it/s]warmup run: 1613it [00:03, 933.98it/s]warmup run: 1681it [00:03, 963.35it/s]warmup run: 1799it [00:03, 997.52it/s]warmup run: 1936it [00:03, 1003.60it/s]warmup run: 1668it [00:03, 978.56it/s]warmup run: 1695it [00:03, 988.29it/s]warmup run: 1706it [00:03, 957.43it/s]warmup run: 1740it [00:03, 965.28it/s]warmup run: 1708it [00:03, 930.15it/s]warmup run: 1901it [00:03, 1003.89it/s]warmup run: 1779it [00:03, 950.39it/s]warmup run: 2045it [00:03, 1027.77it/s]warmup run: 1768it [00:03, 984.57it/s]warmup run: 1795it [00:03, 989.85it/s]warmup run: 1839it [00:03, 971.56it/s]warmup run: 1802it [00:03, 927.19it/s]warmup run: 1803it [00:03, 933.55it/s]warmup run: 2004it [00:03, 1009.28it/s]warmup run: 1875it [00:03, 949.80it/s]warmup run: 2167it [00:03, 1082.49it/s]warmup run: 1869it [00:03, 990.52it/s]warmup run: 1896it [00:03, 994.33it/s]warmup run: 1937it [00:03, 973.73it/s]warmup run: 1896it [00:03, 923.72it/s]warmup run: 1898it [00:03, 917.39it/s]warmup run: 2124it [00:03, 1064.30it/s]warmup run: 1971it [00:03, 951.05it/s]warmup run: 2289it [00:03, 1121.04it/s]warmup run: 1971it [00:03, 997.73it/s]warmup run: 1997it [00:03, 997.59it/s]warmup run: 2038it [00:03, 984.00it/s]warmup run: 1989it [00:03, 919.60it/s]warmup run: 1991it [00:03, 903.76it/s]warmup run: 2244it [00:03, 1103.57it/s]warmup run: 2082it [00:03, 997.41it/s]warmup run: 2411it [00:03, 1148.08it/s]warmup run: 2085it [00:03, 1039.89it/s]warmup run: 2117it [00:03, 1056.56it/s]warmup run: 2154it [00:03, 1035.26it/s]warmup run: 2099it [00:03, 972.02it/s]warmup run: 2096it [00:03, 944.34it/s]warmup run: 2364it [00:03, 1131.40it/s]warmup run: 2201it [00:03, 1052.96it/s]warmup run: 2532it [00:03, 1166.38it/s]warmup run: 2205it [00:03, 1087.33it/s]warmup run: 2239it [00:03, 1103.17it/s]warmup run: 2270it [00:03, 1071.32it/s]warmup run: 2213it [00:03, 1019.15it/s]warmup run: 2203it [00:03, 978.69it/s]warmup run: 2484it [00:03, 1151.16it/s]warmup run: 2320it [00:03, 1092.20it/s]warmup run: 2654it [00:04, 1179.54it/s]warmup run: 2326it [00:03, 1123.36it/s]warmup run: 2361it [00:03, 1135.59it/s]warmup run: 2386it [00:03, 1096.21it/s]warmup run: 2327it [00:03, 1052.55it/s]warmup run: 2312it [00:03, 1009.38it/s]warmup run: 2606it [00:04, 1171.32it/s]warmup run: 2439it [00:03, 1120.22it/s]warmup run: 2774it [00:04, 1184.36it/s]warmup run: 2447it [00:03, 1148.81it/s]warmup run: 2480it [00:03, 1150.00it/s]warmup run: 2507it [00:04, 1128.69it/s]warmup run: 2443it [00:04, 1084.04it/s]warmup run: 2425it [00:03, 1042.62it/s]warmup run: 2728it [00:04, 1185.70it/s]warmup run: 2558it [00:04, 1139.88it/s]warmup run: 2895it [00:04, 1191.54it/s]warmup run: 2567it [00:04, 1163.10it/s]warmup run: 2602it [00:04, 1170.78it/s]warmup run: 2628it [00:04, 1152.84it/s]warmup run: 2558it [00:04, 1102.57it/s]warmup run: 2534it [00:04, 1054.48it/s]warmup run: 2849it [00:04, 1191.72it/s]warmup run: 2677it [00:04, 1153.21it/s]warmup run: 3000it [00:04, 684.23it/s] warmup run: 2688it [00:04, 1174.63it/s]warmup run: 2724it [00:04, 1184.00it/s]warmup run: 2748it [00:04, 1164.98it/s]warmup run: 2671it [00:04, 1109.88it/s]warmup run: 2641it [00:04, 1057.24it/s]warmup run: 2969it [00:04, 1194.00it/s]warmup run: 2793it [00:04, 1155.02it/s]warmup run: 3000it [00:04, 684.62it/s] warmup run: 2807it [00:04, 1177.51it/s]warmup run: 2845it [00:04, 1189.09it/s]warmup run: 2869it [00:04, 1177.60it/s]warmup run: 2784it [00:04, 1115.77it/s]warmup run: 2751it [00:04, 1064.61it/s]warmup run: 2911it [00:04, 1162.18it/s]warmup run: 2967it [00:04, 1197.59it/s]warmup run: 2925it [00:04, 1138.95it/s]warmup run: 2992it [00:04, 1191.33it/s]warmup run: 2902it [00:04, 1133.37it/s]warmup run: 3000it [00:04, 675.12it/s] warmup run: 3000it [00:04, 678.56it/s] warmup run: 2862it [00:04, 1077.41it/s]warmup run: 3000it [00:04, 687.70it/s] warmup run: 3000it [00:04, 680.82it/s] warmup run: 3000it [00:04, 659.65it/s] warmup run: 2980it [00:04, 1106.57it/s]warmup run: 3000it [00:04, 672.45it/s] 
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]





warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1656.39it/s]warmup should be done:   5%|▌         | 162/3000 [00:00<00:01, 1617.42it/s]warmup should be done:   5%|▌         | 161/3000 [00:00<00:01, 1607.45it/s]warmup should be done:   5%|▌         | 162/3000 [00:00<00:01, 1614.55it/s]warmup should be done:   6%|▌         | 166/3000 [00:00<00:01, 1654.61it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1645.06it/s]warmup should be done:   5%|▍         | 141/3000 [00:00<00:02, 1403.70it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1660.41it/s]warmup should be done:  11%|█         | 332/3000 [00:00<00:01, 1657.79it/s]warmup should be done:  11%|█         | 333/3000 [00:00<00:01, 1663.09it/s]warmup should be done:  11%|█         | 324/3000 [00:00<00:01, 1618.43it/s]warmup should be done:  11%|█         | 326/3000 [00:00<00:01, 1626.37it/s]warmup should be done:  11%|█         | 335/3000 [00:00<00:01, 1670.21it/s]warmup should be done:  11%|█         | 326/3000 [00:00<00:01, 1624.61it/s]warmup should be done:  11%|█         | 332/3000 [00:00<00:01, 1655.34it/s]warmup should be done:   9%|▉         | 282/3000 [00:00<00:01, 1371.63it/s]warmup should be done:  17%|█▋        | 498/3000 [00:00<00:01, 1656.13it/s]warmup should be done:  16%|█▌        | 486/3000 [00:00<00:01, 1614.08it/s]warmup should be done:  17%|█▋        | 500/3000 [00:00<00:01, 1659.99it/s]warmup should be done:  17%|█▋        | 498/3000 [00:00<00:01, 1651.54it/s]warmup should be done:  17%|█▋        | 503/3000 [00:00<00:01, 1666.54it/s]warmup should be done:  16%|█▋        | 489/3000 [00:00<00:01, 1617.44it/s]warmup should be done:  15%|█▍        | 446/3000 [00:00<00:01, 1491.99it/s]warmup should be done:  16%|█▋        | 489/3000 [00:00<00:01, 1587.89it/s]warmup should be done:  22%|██▏       | 665/3000 [00:00<00:01, 1658.65it/s]warmup should be done:  22%|██▏       | 667/3000 [00:00<00:01, 1660.78it/s]warmup should be done:  22%|██▏       | 648/3000 [00:00<00:01, 1610.57it/s]warmup should be done:  22%|██▏       | 664/3000 [00:00<00:01, 1649.72it/s]warmup should be done:  22%|██▏       | 651/3000 [00:00<00:01, 1615.18it/s]warmup should be done:  22%|██▏       | 670/3000 [00:00<00:01, 1657.73it/s]warmup should be done:  20%|██        | 610/3000 [00:00<00:01, 1546.50it/s]warmup should be done:  22%|██▏       | 650/3000 [00:00<00:01, 1593.00it/s]warmup should be done:  28%|██▊       | 832/3000 [00:00<00:01, 1659.58it/s]warmup should be done:  27%|██▋       | 810/3000 [00:00<00:01, 1613.84it/s]warmup should be done:  28%|██▊       | 834/3000 [00:00<00:01, 1659.92it/s]warmup should be done:  28%|██▊       | 829/3000 [00:00<00:01, 1645.96it/s]warmup should be done:  28%|██▊       | 836/3000 [00:00<00:01, 1656.11it/s]warmup should be done:  27%|██▋       | 813/3000 [00:00<00:01, 1610.13it/s]warmup should be done:  26%|██▌       | 773/3000 [00:00<00:01, 1575.46it/s]warmup should be done:  27%|██▋       | 811/3000 [00:00<00:01, 1596.59it/s]warmup should be done:  33%|███▎      | 999/3000 [00:00<00:01, 1660.40it/s]warmup should be done:  32%|███▎      | 975/3000 [00:00<00:01, 1622.96it/s]warmup should be done:  33%|███▎      | 1001/3000 [00:00<00:01, 1660.50it/s]warmup should be done:  33%|███▎      | 994/3000 [00:00<00:01, 1642.40it/s]warmup should be done:  33%|███▎      | 1002/3000 [00:00<00:01, 1651.66it/s]warmup should be done:  32%|███▎      | 975/3000 [00:00<00:01, 1604.93it/s]warmup should be done:  31%|███       | 936/3000 [00:00<00:01, 1590.90it/s]warmup should be done:  32%|███▏      | 972/3000 [00:00<00:01, 1597.78it/s]warmup should be done:  38%|███▊      | 1139/3000 [00:00<00:01, 1626.64it/s]warmup should be done:  39%|███▉      | 1166/3000 [00:00<00:01, 1656.40it/s]warmup should be done:  39%|███▉      | 1168/3000 [00:00<00:01, 1655.68it/s]warmup should be done:  39%|███▊      | 1159/3000 [00:00<00:01, 1637.73it/s]warmup should be done:  39%|███▉      | 1168/3000 [00:00<00:01, 1646.19it/s]warmup should be done:  37%|███▋      | 1097/3000 [00:00<00:01, 1595.12it/s]warmup should be done:  38%|███▊      | 1136/3000 [00:00<00:01, 1599.08it/s]warmup should be done:  38%|███▊      | 1132/3000 [00:00<00:01, 1596.69it/s]warmup should be done:  43%|████▎     | 1303/3000 [00:00<00:01, 1630.13it/s]warmup should be done:  44%|████▍     | 1332/3000 [00:00<00:01, 1657.28it/s]warmup should be done:  44%|████▍     | 1334/3000 [00:00<00:01, 1656.29it/s]warmup should be done:  44%|████▍     | 1324/3000 [00:00<00:01, 1638.47it/s]warmup should be done:  44%|████▍     | 1334/3000 [00:00<00:01, 1648.85it/s]warmup should be done:  42%|████▏     | 1259/3000 [00:00<00:01, 1601.41it/s]warmup should be done:  43%|████▎     | 1296/3000 [00:00<00:01, 1597.56it/s]warmup should be done:  43%|████▎     | 1292/3000 [00:00<00:01, 1597.54it/s]warmup should be done:  49%|████▉     | 1467/3000 [00:00<00:00, 1631.71it/s]warmup should be done:  50%|████▉     | 1498/3000 [00:00<00:00, 1656.73it/s]warmup should be done:  50%|█████     | 1500/3000 [00:00<00:00, 1655.80it/s]warmup should be done:  50%|████▉     | 1488/3000 [00:00<00:00, 1637.93it/s]warmup should be done:  50%|█████     | 1500/3000 [00:00<00:00, 1650.27it/s]warmup should be done:  47%|████▋     | 1421/3000 [00:00<00:00, 1605.06it/s]warmup should be done:  49%|████▊     | 1456/3000 [00:00<00:00, 1596.69it/s]warmup should be done:  48%|████▊     | 1452/3000 [00:00<00:00, 1596.85it/s]warmup should be done:  54%|█████▍    | 1631/3000 [00:01<00:00, 1632.15it/s]warmup should be done:  55%|█████▌    | 1664/3000 [00:01<00:00, 1653.07it/s]warmup should be done:  56%|█████▌    | 1666/3000 [00:01<00:00, 1655.36it/s]warmup should be done:  55%|█████▌    | 1652/3000 [00:01<00:00, 1637.99it/s]warmup should be done:  56%|█████▌    | 1666/3000 [00:01<00:00, 1653.13it/s]warmup should be done:  53%|█████▎    | 1582/3000 [00:01<00:00, 1606.06it/s]warmup should be done:  54%|█████▍    | 1616/3000 [00:01<00:00, 1595.44it/s]warmup should be done:  54%|█████▎    | 1612/3000 [00:01<00:00, 1584.83it/s]warmup should be done:  60%|█████▉    | 1795/3000 [00:01<00:00, 1633.53it/s]warmup should be done:  61%|██████    | 1830/3000 [00:01<00:00, 1651.53it/s]warmup should be done:  61%|██████    | 1832/3000 [00:01<00:00, 1654.94it/s]warmup should be done:  61%|██████    | 1816/3000 [00:01<00:00, 1637.55it/s]warmup should be done:  61%|██████    | 1832/3000 [00:01<00:00, 1654.23it/s]warmup should be done:  58%|█████▊    | 1743/3000 [00:01<00:00, 1604.31it/s]warmup should be done:  59%|█████▉    | 1776/3000 [00:01<00:00, 1595.02it/s]warmup should be done:  59%|█████▉    | 1771/3000 [00:01<00:00, 1564.01it/s]warmup should be done:  65%|██████▌   | 1959/3000 [00:01<00:00, 1634.39it/s]warmup should be done:  67%|██████▋   | 1996/3000 [00:01<00:00, 1653.90it/s]warmup should be done:  67%|██████▋   | 1998/3000 [00:01<00:00, 1654.91it/s]warmup should be done:  66%|██████▌   | 1980/3000 [00:01<00:00, 1637.35it/s]warmup should be done:  67%|██████▋   | 1998/3000 [00:01<00:00, 1649.98it/s]warmup should be done:  65%|██████▍   | 1936/3000 [00:01<00:00, 1594.19it/s]warmup should be done:  63%|██████▎   | 1904/3000 [00:01<00:00, 1571.75it/s]warmup should be done:  64%|██████▍   | 1931/3000 [00:01<00:00, 1573.42it/s]warmup should be done:  71%|███████   | 2123/3000 [00:01<00:00, 1632.73it/s]warmup should be done:  72%|███████▏  | 2162/3000 [00:01<00:00, 1653.83it/s]warmup should be done:  72%|███████▏  | 2164/3000 [00:01<00:00, 1653.55it/s]warmup should be done:  72%|███████▏  | 2145/3000 [00:01<00:00, 1638.62it/s]warmup should be done:  72%|███████▏  | 2164/3000 [00:01<00:00, 1652.26it/s]warmup should be done:  70%|██████▉   | 2096/3000 [00:01<00:00, 1595.12it/s]warmup should be done:  69%|██████▉   | 2065/3000 [00:01<00:00, 1582.42it/s]warmup should be done:  70%|██████▉   | 2090/3000 [00:01<00:00, 1578.10it/s]warmup should be done:  76%|███████▌  | 2287/3000 [00:01<00:00, 1632.71it/s]warmup should be done:  78%|███████▊  | 2328/3000 [00:01<00:00, 1653.95it/s]warmup should be done:  78%|███████▊  | 2330/3000 [00:01<00:00, 1653.52it/s]warmup should be done:  77%|███████▋  | 2310/3000 [00:01<00:00, 1639.57it/s]warmup should be done:  78%|███████▊  | 2331/3000 [00:01<00:00, 1654.71it/s]warmup should be done:  75%|███████▌  | 2257/3000 [00:01<00:00, 1596.61it/s]warmup should be done:  74%|███████▍  | 2225/3000 [00:01<00:00, 1587.21it/s]warmup should be done:  75%|███████▍  | 2248/3000 [00:01<00:00, 1571.46it/s]warmup should be done:  82%|████████▏ | 2451/3000 [00:01<00:00, 1631.50it/s]warmup should be done:  83%|████████▎ | 2494/3000 [00:01<00:00, 1652.20it/s]warmup should be done:  83%|████████▎ | 2496/3000 [00:01<00:00, 1650.65it/s]warmup should be done:  82%|████████▏ | 2474/3000 [00:01<00:00, 1637.71it/s]warmup should be done:  83%|████████▎ | 2497/3000 [00:01<00:00, 1653.54it/s]warmup should be done:  81%|████████  | 2417/3000 [00:01<00:00, 1594.24it/s]warmup should be done:  80%|███████▉  | 2385/3000 [00:01<00:00, 1590.54it/s]warmup should be done:  80%|████████  | 2406/3000 [00:01<00:00, 1567.90it/s]warmup should be done:  89%|████████▊ | 2660/3000 [00:01<00:00, 1654.19it/s]warmup should be done:  87%|████████▋ | 2615/3000 [00:01<00:00, 1624.62it/s]warmup should be done:  88%|████████▊ | 2638/3000 [00:01<00:00, 1638.32it/s]warmup should be done:  89%|████████▊ | 2662/3000 [00:01<00:00, 1651.74it/s]warmup should be done:  89%|████████▉ | 2663/3000 [00:01<00:00, 1654.90it/s]warmup should be done:  86%|████████▌ | 2577/3000 [00:01<00:00, 1594.12it/s]warmup should be done:  85%|████████▍ | 2548/3000 [00:01<00:00, 1601.74it/s]warmup should be done:  85%|████████▌ | 2563/3000 [00:01<00:00, 1567.87it/s]warmup should be done:  94%|█████████▍| 2827/3000 [00:01<00:00, 1657.35it/s]warmup should be done:  94%|█████████▍| 2828/3000 [00:01<00:00, 1654.11it/s]warmup should be done:  93%|█████████▎| 2780/3000 [00:01<00:00, 1629.35it/s]warmup should be done:  93%|█████████▎| 2803/3000 [00:01<00:00, 1638.91it/s]warmup should be done:  94%|█████████▍| 2830/3000 [00:01<00:00, 1657.40it/s]warmup should be done:  91%|█████████▏| 2738/3000 [00:01<00:00, 1596.51it/s]warmup should be done:  90%|█████████ | 2711/3000 [00:01<00:00, 1608.95it/s]warmup should be done:  91%|█████████ | 2720/3000 [00:01<00:00, 1567.12it/s]warmup should be done: 100%|█████████▉| 2995/3000 [00:01<00:00, 1661.15it/s]warmup should be done: 100%|█████████▉| 2995/3000 [00:01<00:00, 1657.09it/s]warmup should be done:  99%|█████████▉| 2969/3000 [00:01<00:00, 1643.81it/s]warmup should be done:  98%|█████████▊| 2946/3000 [00:01<00:00, 1636.28it/s]warmup should be done: 100%|█████████▉| 2997/3000 [00:01<00:00, 1660.61it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1656.35it/s]warmup should be done:  97%|█████████▋| 2900/3000 [00:01<00:00, 1602.48it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1655.15it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1654.49it/s]warmup should be done:  96%|█████████▌| 2876/3000 [00:01<00:00, 1620.25it/s]warmup should be done:  96%|█████████▌| 2879/3000 [00:01<00:00, 1571.42it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1640.92it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1628.73it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1601.06it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1586.25it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1581.58it/s]




warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]


warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|▌         | 170/3000 [00:00<00:01, 1699.93it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1649.74it/s]warmup should be done:   6%|▌         | 169/3000 [00:00<00:01, 1688.01it/s]warmup should be done:   5%|▌         | 162/3000 [00:00<00:01, 1615.79it/s]warmup should be done:   6%|▌         | 170/3000 [00:00<00:01, 1695.83it/s]warmup should be done:   6%|▌         | 165/3000 [00:00<00:01, 1644.13it/s]warmup should be done:   6%|▌         | 167/3000 [00:00<00:01, 1663.62it/s]warmup should be done:   6%|▌         | 170/3000 [00:00<00:01, 1691.58it/s]warmup should be done:  11%|█▏        | 340/3000 [00:00<00:01, 1699.59it/s]warmup should be done:  11%|█▏        | 339/3000 [00:00<00:01, 1693.38it/s]warmup should be done:  11%|█         | 332/3000 [00:00<00:01, 1658.73it/s]warmup should be done:  11%|█         | 330/3000 [00:00<00:01, 1644.65it/s]warmup should be done:  11%|█         | 335/3000 [00:00<00:01, 1671.75it/s]warmup should be done:  11%|█▏        | 341/3000 [00:00<00:01, 1700.37it/s]warmup should be done:  11%|█▏        | 340/3000 [00:00<00:01, 1693.82it/s]warmup should be done:  11%|█         | 324/3000 [00:00<00:01, 1611.43it/s]warmup should be done:  17%|█▋        | 499/3000 [00:00<00:01, 1663.70it/s]warmup should be done:  17%|█▋        | 504/3000 [00:00<00:01, 1679.97it/s]warmup should be done:  17%|█▋        | 510/3000 [00:00<00:01, 1698.06it/s]warmup should be done:  16%|█▋        | 495/3000 [00:00<00:01, 1646.28it/s]warmup should be done:  17%|█▋        | 510/3000 [00:00<00:01, 1694.08it/s]warmup should be done:  17%|█▋        | 512/3000 [00:00<00:01, 1703.76it/s]warmup should be done:  16%|█▌        | 486/3000 [00:00<00:01, 1615.18it/s]warmup should be done:  17%|█▋        | 510/3000 [00:00<00:01, 1686.53it/s]warmup should be done:  22%|██▏       | 667/3000 [00:00<00:01, 1669.36it/s]warmup should be done:  22%|██▏       | 673/3000 [00:00<00:01, 1683.46it/s]warmup should be done:  23%|██▎       | 681/3000 [00:00<00:01, 1699.59it/s]warmup should be done:  23%|██▎       | 681/3000 [00:00<00:01, 1700.11it/s]warmup should be done:  23%|██▎       | 683/3000 [00:00<00:01, 1703.85it/s]warmup should be done:  22%|██▏       | 649/3000 [00:00<00:01, 1617.61it/s]warmup should be done:  23%|██▎       | 679/3000 [00:00<00:01, 1678.65it/s]warmup should be done:  22%|██▏       | 660/3000 [00:00<00:01, 1629.67it/s]warmup should be done:  28%|██▊       | 835/3000 [00:00<00:01, 1672.50it/s]warmup should be done:  28%|██▊       | 852/3000 [00:00<00:01, 1702.45it/s]warmup should be done:  28%|██▊       | 842/3000 [00:00<00:01, 1682.73it/s]warmup should be done:  28%|██▊       | 852/3000 [00:00<00:01, 1701.44it/s]warmup should be done:  27%|██▋       | 811/3000 [00:00<00:01, 1616.75it/s]warmup should be done:  28%|██▊       | 854/3000 [00:00<00:01, 1700.96it/s]warmup should be done:  28%|██▊       | 847/3000 [00:00<00:01, 1677.11it/s]warmup should be done:  28%|██▊       | 826/3000 [00:00<00:01, 1638.05it/s]warmup should be done:  33%|███▎      | 1003/3000 [00:00<00:01, 1670.77it/s]warmup should be done:  34%|███▎      | 1011/3000 [00:00<00:01, 1682.38it/s]warmup should be done:  34%|███▍      | 1023/3000 [00:00<00:01, 1700.31it/s]warmup should be done:  32%|███▏      | 973/3000 [00:00<00:01, 1614.77it/s]warmup should be done:  34%|███▍      | 1025/3000 [00:00<00:01, 1699.38it/s]warmup should be done:  34%|███▍      | 1023/3000 [00:00<00:01, 1693.79it/s]warmup should be done:  33%|███▎      | 991/3000 [00:00<00:01, 1640.49it/s]warmup should be done:  34%|███▍      | 1015/3000 [00:00<00:01, 1672.85it/s]warmup should be done:  39%|███▉      | 1180/3000 [00:00<00:01, 1682.03it/s]warmup should be done:  39%|███▉      | 1171/3000 [00:00<00:01, 1667.81it/s]warmup should be done:  38%|███▊      | 1136/3000 [00:00<00:01, 1617.40it/s]warmup should be done:  40%|███▉      | 1194/3000 [00:00<00:01, 1696.99it/s]warmup should be done:  40%|███▉      | 1195/3000 [00:00<00:01, 1693.36it/s]warmup should be done:  39%|███▊      | 1156/3000 [00:00<00:01, 1643.50it/s]warmup should be done:  40%|███▉      | 1193/3000 [00:00<00:01, 1685.78it/s]warmup should be done:  39%|███▉      | 1183/3000 [00:00<00:01, 1667.83it/s]warmup should be done:  45%|████▍     | 1349/3000 [00:00<00:00, 1683.93it/s]warmup should be done:  45%|████▍     | 1339/3000 [00:00<00:00, 1669.92it/s]warmup should be done:  46%|████▌     | 1365/3000 [00:00<00:00, 1700.40it/s]warmup should be done:  43%|████▎     | 1298/3000 [00:00<00:01, 1613.58it/s]warmup should be done:  44%|████▍     | 1321/3000 [00:00<00:01, 1643.17it/s]warmup should be done:  45%|████▌     | 1362/3000 [00:00<00:00, 1684.80it/s]warmup should be done:  46%|████▌     | 1365/3000 [00:00<00:00, 1688.10it/s]warmup should be done:  45%|████▌     | 1351/3000 [00:00<00:00, 1670.06it/s]warmup should be done:  51%|█████     | 1518/3000 [00:00<00:00, 1683.10it/s]warmup should be done:  50%|█████     | 1507/3000 [00:00<00:00, 1671.33it/s]warmup should be done:  51%|█████     | 1536/3000 [00:00<00:00, 1700.19it/s]warmup should be done:  49%|████▊     | 1461/3000 [00:00<00:00, 1616.14it/s]warmup should be done:  50%|████▉     | 1487/3000 [00:00<00:00, 1645.70it/s]warmup should be done:  51%|█████     | 1531/3000 [00:00<00:00, 1681.78it/s]warmup should be done:  51%|█████     | 1534/3000 [00:00<00:00, 1682.96it/s]warmup should be done:  51%|█████     | 1519/3000 [00:00<00:00, 1671.21it/s]warmup should be done:  56%|█████▌    | 1687/3000 [00:01<00:00, 1683.82it/s]warmup should be done:  56%|█████▌    | 1676/3000 [00:01<00:00, 1676.18it/s]warmup should be done:  57%|█████▋    | 1707/3000 [00:01<00:00, 1701.07it/s]warmup should be done:  54%|█████▍    | 1624/3000 [00:01<00:00, 1618.70it/s]warmup should be done:  55%|█████▌    | 1653/3000 [00:01<00:00, 1649.07it/s]warmup should be done:  57%|█████▋    | 1700/3000 [00:01<00:00, 1680.71it/s]warmup should be done:  56%|█████▌    | 1687/3000 [00:01<00:00, 1672.71it/s]warmup should be done:  57%|█████▋    | 1703/3000 [00:01<00:00, 1680.17it/s]warmup should be done:  62%|██████▏   | 1856/3000 [00:01<00:00, 1684.79it/s]warmup should be done:  62%|██████▏   | 1845/3000 [00:01<00:00, 1679.51it/s]warmup should be done:  63%|██████▎   | 1878/3000 [00:01<00:00, 1702.69it/s]warmup should be done:  60%|█████▉    | 1786/3000 [00:01<00:00, 1617.15it/s]warmup should be done:  61%|██████    | 1819/3000 [00:01<00:00, 1650.22it/s]warmup should be done:  62%|██████▏   | 1869/3000 [00:01<00:00, 1679.56it/s]warmup should be done:  62%|██████▏   | 1855/3000 [00:01<00:00, 1671.28it/s]warmup should be done:  62%|██████▏   | 1872/3000 [00:01<00:00, 1679.02it/s]warmup should be done:  68%|██████▊   | 2025/3000 [00:01<00:00, 1683.99it/s]warmup should be done:  67%|██████▋   | 2014/3000 [00:01<00:00, 1680.49it/s]warmup should be done:  68%|██████▊   | 2049/3000 [00:01<00:00, 1703.92it/s]warmup should be done:  65%|██████▍   | 1948/3000 [00:01<00:00, 1614.99it/s]warmup should be done:  66%|██████▌   | 1985/3000 [00:01<00:00, 1647.69it/s]warmup should be done:  68%|██████▊   | 2038/3000 [00:01<00:00, 1680.01it/s]warmup should be done:  68%|██████▊   | 2040/3000 [00:01<00:00, 1678.61it/s]warmup should be done:  67%|██████▋   | 2023/3000 [00:01<00:00, 1667.39it/s]warmup should be done:  73%|███████▎  | 2194/3000 [00:01<00:00, 1683.54it/s]warmup should be done:  73%|███████▎  | 2183/3000 [00:01<00:00, 1680.79it/s]warmup should be done:  74%|███████▍  | 2220/3000 [00:01<00:00, 1702.37it/s]warmup should be done:  70%|███████   | 2112/3000 [00:01<00:00, 1621.00it/s]warmup should be done:  72%|███████▏  | 2150/3000 [00:01<00:00, 1647.15it/s]warmup should be done:  74%|███████▎  | 2208/3000 [00:01<00:00, 1676.22it/s]warmup should be done:  74%|███████▎  | 2207/3000 [00:01<00:00, 1678.34it/s]warmup should be done:  73%|███████▎  | 2190/3000 [00:01<00:00, 1666.13it/s]warmup should be done:  79%|███████▉  | 2363/3000 [00:01<00:00, 1683.29it/s]warmup should be done:  80%|███████▉  | 2391/3000 [00:01<00:00, 1701.92it/s]warmup should be done:  78%|███████▊  | 2352/3000 [00:01<00:00, 1675.38it/s]warmup should be done:  76%|███████▌  | 2277/3000 [00:01<00:00, 1626.82it/s]warmup should be done:  77%|███████▋  | 2316/3000 [00:01<00:00, 1648.66it/s]warmup should be done:  79%|███████▉  | 2376/3000 [00:01<00:00, 1676.25it/s]warmup should be done:  79%|███████▉  | 2375/3000 [00:01<00:00, 1676.79it/s]warmup should be done:  79%|███████▊  | 2357/3000 [00:01<00:00, 1666.66it/s]warmup should be done:  84%|████████▍ | 2532/3000 [00:01<00:00, 1684.03it/s]warmup should be done:  81%|████████▏ | 2441/3000 [00:01<00:00, 1630.69it/s]warmup should be done:  85%|████████▌ | 2562/3000 [00:01<00:00, 1701.38it/s]warmup should be done:  84%|████████▍ | 2520/3000 [00:01<00:00, 1670.08it/s]warmup should be done:  83%|████████▎ | 2482/3000 [00:01<00:00, 1650.14it/s]warmup should be done:  85%|████████▍ | 2544/3000 [00:01<00:00, 1676.74it/s]warmup should be done:  85%|████████▍ | 2544/3000 [00:01<00:00, 1678.96it/s]warmup should be done:  84%|████████▍ | 2526/3000 [00:01<00:00, 1670.90it/s]warmup should be done:  87%|████████▋ | 2606/3000 [00:01<00:00, 1635.05it/s]warmup should be done:  91%|█████████ | 2733/3000 [00:01<00:00, 1701.50it/s]warmup should be done:  90%|█████████ | 2701/3000 [00:01<00:00, 1665.40it/s]warmup should be done:  88%|████████▊ | 2648/3000 [00:01<00:00, 1648.60it/s]warmup should be done:  90%|████████▉ | 2688/3000 [00:01<00:00, 1665.27it/s]warmup should be done:  90%|█████████ | 2712/3000 [00:01<00:00, 1677.36it/s]warmup should be done:  90%|█████████ | 2713/3000 [00:01<00:00, 1680.16it/s]warmup should be done:  90%|████████▉ | 2694/3000 [00:01<00:00, 1672.45it/s]warmup should be done:  92%|█████████▏| 2771/3000 [00:01<00:00, 1638.33it/s]warmup should be done:  97%|█████████▋| 2904/3000 [00:01<00:00, 1700.37it/s]warmup should be done:  96%|█████████▌| 2869/3000 [00:01<00:00, 1668.34it/s]warmup should be done:  94%|█████████▍| 2813/3000 [00:01<00:00, 1647.63it/s]warmup should be done:  96%|█████████▌| 2880/3000 [00:01<00:00, 1674.52it/s]warmup should be done:  95%|█████████▌| 2855/3000 [00:01<00:00, 1659.61it/s]warmup should be done:  95%|█████████▌| 2862/3000 [00:01<00:00, 1672.77it/s]warmup should be done:  96%|█████████▌| 2882/3000 [00:01<00:00, 1679.34it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1700.30it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1684.09it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1683.31it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1678.75it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1673.07it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1670.02it/s]warmup should be done:  98%|█████████▊| 2935/3000 [00:01<00:00, 1629.07it/s]warmup should be done:  99%|█████████▉| 2979/3000 [00:01<00:00, 1650.01it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1646.02it/s]warmup should be done: 100%|██████████| 3000/3000 [00:01<00:00, 1622.12it/s]
WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f42557c2190>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f4255ac8d30>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f42557c12b0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f42557c01c0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f4255ac5e80>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f42557b2100>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f42557c10d0>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert

WARNING:tensorflow:AutoGraph could not transform <bound method DCNHPS.call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f4255ac6730>> and will run it as-is.
Cause: mangled names are not yet supported
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
2022-12-11 23:36:50.962377: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f3d86830b40 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 23:36:50.962449: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 23:36:50.972877: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 23:36:51.095583: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f3d7a82c7c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 23:36:51.095645: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 23:36:51.105557: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 23:36:51.530176: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f3d8302d3f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 23:36:51.530245: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 23:36:51.540272: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 23:36:51.984322: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f3d83029c70 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 23:36:51.984397: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 23:36:51.984464: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f3d8e830e90 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 23:36:51.984745: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 23:36:51.992877: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 23:36:51.992976: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 23:36:52.063225: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f3d828343c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 23:36:52.063294: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 23:36:52.063498: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f3d86f92340 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 23:36:52.063562: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 23:36:52.072547: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 23:36:52.072984: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 23:36:52.113634: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f3d7f031170 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 23:36:52.113698: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 23:36:52.123142: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 23:36:58.297195: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 23:36:58.297446: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 23:36:58.406712: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 23:36:58.788846: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 23:36:58.804715: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 23:36:58.806209: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 23:36:58.828860: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 23:36:58.838822: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][23:37:57.751][ERROR][RK0][tid #139904189196032]: replica 5 reaches 1000, calling init pre replica
[HCTR][23:37:57.752][ERROR][RK0][tid #139904189196032]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][23:37:57.758][ERROR][RK0][tid #139904189196032]: coll ps creation done
[HCTR][23:37:57.758][ERROR][RK0][tid #139904189196032]: replica 5 waits for coll ps creation barrier
[HCTR][23:37:58.078][ERROR][RK0][tid #139902503081728]: replica 2 reaches 1000, calling init pre replica
[HCTR][23:37:58.078][ERROR][RK0][tid #139902503081728]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][23:37:58.083][ERROR][RK0][tid #139902503081728]: coll ps creation done
[HCTR][23:37:58.083][ERROR][RK0][tid #139902503081728]: replica 2 waits for coll ps creation barrier
[HCTR][23:37:58.099][ERROR][RK0][main]: replica 4 reaches 1000, calling init pre replica
[HCTR][23:37:58.099][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][23:37:58.105][ERROR][RK0][main]: coll ps creation done
[HCTR][23:37:58.105][ERROR][RK0][main]: replica 4 waits for coll ps creation barrier
[HCTR][23:37:58.135][ERROR][RK0][main]: replica 3 reaches 1000, calling init pre replica
[HCTR][23:37:58.135][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][23:37:58.143][ERROR][RK0][main]: coll ps creation done
[HCTR][23:37:58.143][ERROR][RK0][main]: replica 3 waits for coll ps creation barrier
[HCTR][23:37:58.332][ERROR][RK0][main]: replica 1 reaches 1000, calling init pre replica
[HCTR][23:37:58.332][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][23:37:58.337][ERROR][RK0][main]: coll ps creation done
[HCTR][23:37:58.337][ERROR][RK0][main]: replica 1 waits for coll ps creation barrier
[HCTR][23:37:58.369][ERROR][RK0][tid #139903048345344]: replica 0 reaches 1000, calling init pre replica
[HCTR][23:37:58.369][ERROR][RK0][tid #139903048345344]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][23:37:58.376][ERROR][RK0][tid #139903048345344]: coll ps creation done
[HCTR][23:37:58.377][ERROR][RK0][tid #139903048345344]: replica 0 waits for coll ps creation barrier
[HCTR][23:37:58.402][ERROR][RK0][main]: replica 6 reaches 1000, calling init pre replica
[HCTR][23:37:58.402][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][23:37:58.409][ERROR][RK0][main]: coll ps creation done
[HCTR][23:37:58.409][ERROR][RK0][main]: replica 6 waits for coll ps creation barrier
[HCTR][23:37:58.410][ERROR][RK0][tid #139902503081728]: replica 7 reaches 1000, calling init pre replica
[HCTR][23:37:58.410][ERROR][RK0][tid #139902503081728]: coll ps creation, with 8 devices, using policy coll_cache_asymm_link
[HCTR][23:37:58.417][ERROR][RK0][tid #139902503081728]: coll ps creation done
[HCTR][23:37:58.417][ERROR][RK0][tid #139902503081728]: replica 7 waits for coll ps creation barrier
[HCTR][23:37:58.418][ERROR][RK0][tid #139903048345344]: replica 0 preparing frequency
[HCTR][23:37:59.364][ERROR][RK0][tid #139903048345344]: replica 0 preparing frequency done
[HCTR][23:37:59.412][ERROR][RK0][tid #139903048345344]: replica 0 calling init per replica
[HCTR][23:37:59.412][ERROR][RK0][tid #139902503081728]: replica 7 calling init per replica
[HCTR][23:37:59.412][ERROR][RK0][tid #139904189196032]: replica 5 calling init per replica
[HCTR][23:37:59.412][ERROR][RK0][main]: replica 4 calling init per replica
[HCTR][23:37:59.412][ERROR][RK0][main]: replica 1 calling init per replica
[HCTR][23:37:59.412][ERROR][RK0][main]: replica 6 calling init per replica
[HCTR][23:37:59.412][ERROR][RK0][main]: replica 3 calling init per replica
[HCTR][23:37:59.412][ERROR][RK0][tid #139902503081728]: replica 2 calling init per replica
[HCTR][23:37:59.412][ERROR][RK0][tid #139903048345344]: Calling build_v2
[HCTR][23:37:59.412][ERROR][RK0][tid #139902503081728]: Calling build_v2
[HCTR][23:37:59.412][ERROR][RK0][tid #139904189196032]: Calling build_v2
[HCTR][23:37:59.412][ERROR][RK0][main]: Calling build_v2
[HCTR][23:37:59.412][ERROR][RK0][main]: Calling build_v2
[HCTR][23:37:59.412][ERROR][RK0][main]: Calling build_v2
[HCTR][23:37:59.412][ERROR][RK0][main]: Calling build_v2
[HCTR][23:37:59.412][ERROR][RK0][tid #139902503081728]: Calling build_v2
[HCTR][23:37:59.412][ERROR][RK0][tid #139903048345344]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][23:37:59.412][ERROR][RK0][tid #139902503081728]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][23:37:59.412][ERROR][RK0][tid #139904189196032]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][23:37:59.412][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][23:37:59.412][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][23:37:59.412][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][23:37:59.412][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][23:37:59.412][ERROR][RK0][tid #139902503081728]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[[[[[[[[2022-12-11 23:37:592022-12-11 23:37:592022-12-11 23:37:592022-12-11 23:37:592022-12-11 23:37:592022-12-11 23:37:592022-12-11 23:37:59.......4129104129102022-12-11 23:37:59412917412917412917412917412910: : .: : : : : EE412930EEEEE  :      /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.ccE/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:: :::::136136/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc136136136136136] ] :] 136] ] ] ] using concurrent impl MPSusing concurrent impl MPSusing concurrent impl MPS] using concurrent impl MPSusing concurrent impl MPSusing concurrent impl MPSusing concurrent impl MPS


using concurrent impl MPS




[2022-12-11 23:37:59.417364: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-11 23:37:59.417403: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[:2022-12-11 23:37:59196.] 417408assigning 8 to cpu: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[[2022-12-11 23:37:59[2022-12-11 23:37:59.2022-12-11 23:37:59.417459.417459: 417458: E[: E 2022-12-11 23:37:59E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc. /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:417503/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:[212: :1962022-12-11 23:37:59] E178] [.build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8 ] assigning 8 to cpu2022-12-11 23:37:59417549
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccv100x8, slow pcie
.: [:
417598E2022-12-11 23:37:59[[178:  .2022-12-11 23:37:592022-12-11 23:37:59] E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc417646.[.v100x8, slow pcie :: [4176802022-12-11 23:37:59417669
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc178E2022-12-11 23:37:59: .: :] [ .E417710E178v100x8, slow pcie2022-12-11 23:37:59/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc417713 :  ] 
.:: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccv100x8, slow pcie417780178[E: :
: ] 2022-12-11 23:37:59[ 196/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc213Ev100x8, slow pcie.2022-12-11 23:37:59/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] :]  
417867.:assigning 8 to cpu212remote time is 8.68421/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 417916[178
] 
:E: 2022-12-11 23:37:59] build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8196 [E.v100x8, slow pcie
] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[2022-12-11 23:37:59 417977
assigning 8 to cpu:2022-12-11 23:37:59.[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 
196.4180302022-12-11 23:37:59[:E] 418052: .2022-12-11 23:37:59196 assigning 8 to cpu: E418087.[] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
E : 4181052022-12-11 23:37:59assigning 8 to cpu: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE: .
196/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: [E418156] :214/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 23:37:59 : assigning 8 to cpu212] :.[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE
] cpu time is 97.05882134182262022-12-11 23:37:59: build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8
] : .196/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
remote time is 8.68421E418278[] :
 : [2022-12-11 23:37:59assigning 8 to cpu212/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[E2022-12-11 23:37:59.
] :2022-12-11 23:37:59 .418359build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8212./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc418383: 
] 418403:: [Ebuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8: 212E[2022-12-11 23:37:59 
E]  2022-12-11 23:37:59./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[.418471:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
:2022-12-11 23:37:59418494: 212:213.: [E] 214] 418543E2022-12-11 23:37:59 build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8] remote time is 8.68421:  ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
cpu time is 97.0588
E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc418596:
 [:[: 212/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 23:37:592132022-12-11 23:37:59E] :.] . build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8213418689remote time is 8.68421418692/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
] : 
: :remote time is 8.68421EE213[
 [ ] 2022-12-11 23:37:59/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 23:37:59[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccremote time is 8.68421.:.2022-12-11 23:37:59:
418783213418782.214: ] [: 418807] Eremote time is 8.684212022-12-11 23:37:59E: cpu time is 97.0588 
. E
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc418878/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[ :: :2022-12-11 23:37:59/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc214E213.:]  ] 418934214cpu time is 97.0588/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccremote time is 8.68421: ] 
:
Ecpu time is 97.0588214 
] [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cccpu time is 97.05882022-12-11 23:37:59:
.214419034] : cpu time is 97.0588E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:214] cpu time is 97.0588
[2022-12-11 23:39:16.342675: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-11 23:39:16.382944: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 381.47 MB
[2022-12-11 23:39:16.383039: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 381.47 MB
[2022-12-11 23:39:16.384144: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:74] mapping nid to rank...
[2022-12-11 23:39:16.457497: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:91] counting slots...
[2022-12-11 23:39:16.860571: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:105] Final num slot is 49
[2022-12-11 23:39:16.860659: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:109] counting blocks...
[2022-12-11 23:39:24.621903: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:118] Final num block is 1024
[2022-12-11 23:39:24.621994: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:123] counting freq and density...
[2022-12-11 23:39:26.343031: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:154] averaging freq and density...
[2022-12-11 23:39:26.343142: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:155] 1024
[2022-12-11 23:39:26.345773: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
[2022-12-11 23:39:26.345832: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:354] constructing optimal solver, device=8, stream=1
1024 blocks, 8 devices
[2022-12-11 23:39:26.620935: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:527] Add Var...
[2022-12-11 23:39:26.649529: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:545] Capacity...
[2022-12-11 23:39:26.650950: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:548] Connect CPU...
[2022-12-11 23:39:26.672089: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:550] Connect Access To Storage...
[2022-12-11 23:39:27.196177: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:554] Time...
[2022-12-11 23:39:44.572781: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:569] Coll Cache init block placement array
[2022-12-11 23:39:44.580735: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:645] Coll Cache init block placement array done
[2022-12-11 23:39:44.582347: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:647] Coll Cache model reset done
[2022-12-11 23:39:44.625413: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-11 23:39:44.625510: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-11 23:39:44.625540: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-11 23:39:44.625568: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-11 23:39:44.626294: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 23:39:44.626345: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:39:44.627244: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:39:44.627921: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:39:44.640856: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 4 solved
[2022-12-11 23:39:44.640925: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 4 initing device 4
[[2022-12-11 23:39:442022-12-11 23:39:44..641126641141: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::202202] ] 6 solved7 solved

[[[2022-12-11 23:39:44[2022-12-11 23:39:442022-12-11 23:39:44.2022-12-11 23:39:44..641200.641214641215: 641212: : E: EE E  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc[:/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::2022-12-11 23:39:44202:205205.] 202] ] 6412653 solved] worker 0 thread 6 initing device 6worker 0 thread 7 initing device 7: 

2 solved
E
[ 2022-12-11 23:39:44/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc.[:6413412022-12-11 23:39:44202: .] E641350 1 solved: /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc
E:[ 205[2022-12-11 23:39:44/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc] 2022-12-11 23:39:44.:worker 0 thread 3 initing device 3.641381205
641388: ] : Eworker 0 thread 2 initing device 2E 
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::1815205] ] Building Coll Cache with ... num gpu device is 8worker 0 thread 1 initing device 1

[2022-12-11 23:39:44.641485: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:39:44.641669: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 5 solved
[[[2022-12-11 23:39:442022-12-11 23:39:442022-12-11 23:39:44...641744641744641746: : : EE[E  2022-12-11 23:39:44 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu./hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::[641811:181518152022-12-11 23:39:44: [205] ] .E2022-12-11 23:39:44] Building Coll Cache with ... num gpu device is 8Building Coll Cache with ... num gpu device is 8641840 .worker 0 thread 5 initing device 5

: /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu641863
E::  1815E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[[]  :2022-12-11 23:39:442022-12-11 23:39:44Building Coll Cache with ... num gpu device is 8/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1815..
:] 6419436419431815Building Coll Cache with ... num gpu device is 8: : ] 
EEBuilding Coll Cache with ... num gpu device is 8[  
2022-12-11 23:39:44/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.:[:64199319802022-12-11 23:39:441980: ] .[] Eeager alloc mem 381.47 MB6420112022-12-11 23:39:44eager alloc mem 381.47 MB 
: .
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE642024: : 1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE] : eager alloc mem 381.47 MB1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
] :eager alloc mem 381.47 MB1980
] eager alloc mem 381.47 MB
[2022-12-11 23:39:44.642383: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 23:39:44.642438: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:39:44.645482: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:39:44.646530: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:39:44.646579: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:39:44.646637: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:39:44.646692: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:39:44.646755: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:39:44.646871: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:39:44.649791: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:39:44.650751: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:39:44.650801: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:39:44.650851: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:39:44.650903: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:39:44.650965: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:39:44.651061: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 23:39:44.702122: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-11 23:39:44.707365: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 23:39:44.707464: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 23:39:44.708237: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.83 MB
[2022-12-11 23:39:44.708811: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:39:44.709805: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:39:44.709849: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 11.42 MB
[2022-12-11 23:39:44.712167: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 23:39:44.712917: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 23:39:44.712962: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.43 GB
[2022-12-11 23:39:44.731402: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-11 23:39:44.731560: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[[[[:2022-12-11 23:39:442022-12-11 23:39:442022-12-11 23:39:442022-12-11 23:39:441980....] 731613731613731613731613eager alloc mem 1024.00 Bytes: : : : 
EEEE    /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::1980198019801980] ] ] ] eager alloc mem 1024.00 Byteseager alloc mem 1024.00 Byteseager alloc mem 1024.00 Byteseager alloc mem 1024.00 Bytes



[2022-12-11 23:39:44.732039: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-11 23:39:44.746544: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 23:39:44.746623: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 23:39:44.747354: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638[] 2022-12-11 23:39:44eager release cuda mem 1024.
747397: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.83 MB
[2022-12-11 23:39:44.747438[: 2022-12-11 23:39:44E. 747467/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :E638 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 1024:
638] eager release cuda mem 400000000
[2022-12-11 23:39:44.747512: E[ 2022-12-11 23:39:44/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.:747539638: ] Eeager release cuda mem 1024 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 23:39:44.747598: E[ 2022-12-11 23:39:44/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.:747596638: ] Eeager release cuda mem 400000000 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 23:39:44.747684: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[:2022-12-11 23:39:44638.] 747684eager release cuda mem 400000000: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 23:39:44.747770: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] [eager release cuda mem 4000000002022-12-11 23:39:44
.747776: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-11 23:39:44.747860: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 23:39:44.748168: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:39:44.748690: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.83 MB
[2022-12-11 23:39:44.749145: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:39:44.749191: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 11.43 MB
[2022-12-11 23:39:44.749447: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.83 MB
[2022-12-11 23:39:44.750190: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.83 MB
[2022-12-11 23:39:44.750720: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.83 MB
[2022-12-11 23:39:44.751300: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.83 MB
[2022-12-11 23:39:44.751878: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.83 MB
[2022-12-11 23:39:44.752634: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:39:44.752838: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:39:44.752993: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:39:44.753102: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:39:44.753148: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:39:44.753196: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:39:44.753588: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:39:44.753634: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 11.39 MB
[2022-12-11 23:39:44.753823: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:39:44.753868: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 11.40 MB
[2022-12-11 23:39:44.753959: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:39:44.754004: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 11.41 MB
[2022-12-11 23:39:44.754068: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:39:44[.2022-12-11 23:39:44754113.: 754113W:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc43:] 638WORKER[0] alloc host memory 11.34 MB] 
eager release cuda mem 625663
[[2022-12-11 23:39:442022-12-11 23:39:44..754177754182: : EW  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc::63843] ] eager release cuda mem 625663WORKER[0] alloc host memory 11.35 MB

[2022-12-11 23:39:44.754245: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cpu/cpu_device.cc:43] WORKER[0] alloc host memory 11.43 MB
[2022-12-11 23:39:44.757377: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 23:39:44.758105: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 23:39:44.758147: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.42 GB
[2022-12-11 23:39:44.758249: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 23:39:44.758983: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 23:39:44.759025: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.43 GB
[2022-12-11 23:39:44.761942: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 23:39:44.[7621612022-12-11 23:39:44: .E762170 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: 1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] :eager alloc mem 25.25 KB1980
] eager alloc mem 25.25 KB
[2022-12-11 23:39:44.762370: [E2022-12-11 23:39:44 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu762382:: 1980E]  eager alloc mem 25.25 KB/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
:1980] eager alloc mem 25.25 KB
[2022-12-11 23:39:44.762557: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 23:39:44.762601: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.43 GB
[[2022-12-11 23:39:442022-12-11 23:39:44..762800762801: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 25855eager release cuda mem 25855

[2022-12-11 23:39:44[.2022-12-11 23:39:44762878.: 762881E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980:] 1980eager alloc mem 1.43 GB] 
eager alloc mem 1.43 GB
[2022-12-11 23:39:44.762995: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 23:39:44.763038: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.42 GB
[2022-12-11 23:39:44.763112: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 23:39:44.763168: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1.43 GB
[[[[[[[[2022-12-11 23:39:452022-12-11 23:39:452022-12-11 23:39:452022-12-11 23:39:452022-12-11 23:39:452022-12-11 23:39:452022-12-11 23:39:452022-12-11 23:39:45........200306200307200305200305200305200305200309200306: : : : : : : : EEEEEEEE        /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::::::19261926192619261926192619261926] ] ] ] ] ] ] ] Device 1 init p2p of link 7Device 3 init p2p of link 2Device 7 init p2p of link 4Device 2 init p2p of link 1Device 4 init p2p of link 5Device 6 init p2p of link 0Device 0 init p2p of link 3Device 5 init p2p of link 6







[[[[2022-12-11 23:39:452022-12-11 23:39:452022-12-11 23:39:452022-12-11 23:39:45..[[..2008142008142022-12-11 23:39:45[2022-12-11 23:39:45200813200818: : .[2022-12-11 23:39:45.: : EE2008292022-12-11 23:39:45.200830EE  : .200838:   /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE200852: E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:: : E ::19801980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu19801980] ] : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:] ] eager alloc mem 611.00 KBeager alloc mem 611.00 KB1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980eager alloc mem 611.00 KBeager alloc mem 611.00 KB

] :1980] 

eager alloc mem 611.00 KB1980] eager alloc mem 611.00 KB
] eager alloc mem 611.00 KB
eager alloc mem 611.00 KB

[[2022-12-11 23:39:452022-12-11 23:39:45..201823201826: [: E2022-12-11 23:39:45E . [[/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc201840/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-11 23:39:452022-12-11 23:39:45:: [:..638E2022-12-11 23:39:45638201854[201853]  .[] : 2022-12-11 23:39:45: eager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2018702022-12-11 23:39:45eager release cuda mem 625663E.E
:: .
 201884 638E201892/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc]  : :E:eager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE638 638
: ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 625663:eager release cuda mem 625663] :
638
eager release cuda mem 625663638] 
] eager release cuda mem 625663eager release cuda mem 625663

[2022-12-11 23:39:45.215188: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 0
[2022-12-11 23:39:45.215360: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:39:45.215541: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 1
[2022-12-11 23:39:45.215658: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 3
[2022-12-11 23:39:45.215698: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:[19802022-12-11 23:39:45] .eager alloc mem 611.00 KB215703
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 5
[2022-12-11 23:39:45.215806: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:39:45.215865: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:39:45.215928: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 2
[2022-12-11 23:39:45.216017: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 7
[2022-12-11 23:39:45.216071: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:39:45.[2161652022-12-11 23:39:45: .E216170 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: 1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] :eager alloc mem 611.00 KB638
] eager release cuda mem 625663
[2022-12-11 23:39:45.216498: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:39:45.216572: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:[19262022-12-11 23:39:45] .Device 0 init p2p of link 6216605
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:39:45[.2022-12-11 23:39:45216660.: 216679E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc1926:] 638Device 5 init p2p of link 4] [
eager release cuda mem 6256632022-12-11 23:39:45
.216759: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:39:45.216865: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[:2022-12-11 23:39:45638.] 216882eager release cuda mem 625663: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:39:45.216976: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:39:45.217567: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:39:45.217705: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:39:45.228729: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 6
[2022-12-11 23:39:45.228861: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:39:45.228912: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 5
[2022-12-11 23:39:45.229033: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:39:45.229485: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 0
[2022-12-11 23:39:45.229538: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 4
[2022-12-11 23:39:45.229605: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:39:45[.2022-12-11 23:39:45229664.: 229667E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc1980:] 638eager alloc mem 611.00 KB] 
eager release cuda mem 625663
[2022-12-11 23:39:45.229787: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926[] 2022-12-11 23:39:45Device 1 init p2p of link 3.
229817: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:39:45.229908: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[[2022-12-11 23:39:452022-12-11 23:39:45..229938229938: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19261926] ] Device 0 init p2p of link 1Device 4 init p2p of link 2

[2022-12-11 23:39:45.230132: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[:2022-12-11 23:39:451980.] 230144eager alloc mem 611.00 KB: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:39:45.230399: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:39:45.230492: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:39:45.230637: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 7
[2022-12-11 23:39:45.230703: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:39:45.230760: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:39:45.230950: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638[] 2022-12-11 23:39:45eager release cuda mem 625663.
230969: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:39:45.231563: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:39:45.244538: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 1
[2022-12-11 23:39:45.244654: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:39:45.244755: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 5
[2022-12-11 23:39:45.244867: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:39:45.245456: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:39:45.245661: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:39:45.246052: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] [Device 6 init p2p of link 72022-12-11 23:39:45
.246076: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 4
[2022-12-11 23:39:45.246176: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] [eager alloc mem 611.00 KB2022-12-11 23:39:45
.246194: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:39:45.246234: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 6
[2022-12-11 23:39:45.246354: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:39:45.246549: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 0
[2022-12-11 23:39:45.246659: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:39:45.246761: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 2
[2022-12-11 23:39:45.246896: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:39:45[.2022-12-11 23:39:45246970.: 246976E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638:] 638eager release cuda mem 625663] 
eager release cuda mem 625663
[2022-12-11 23:39:45.247152: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:39:45.247311: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 3
[2022-12-11 23:39:45.247432: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 23:39:45.247453: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:39:45.247690: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:39:45.248212: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 23:39:45.260415: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 12399996
[2022-12-11 23:39:45.260699: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 12399996
[2022-12-11 23:39:45.260924: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 12399996
[2022-12-11 23:39:45.261456: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 12399996
[2022-12-11 23:39:45.262225: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 12399996
[2022-12-11 23:39:45.262281: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 2986979 / 100000000 nodes ( 2.99 %~3.00 %) | remote 8964726 / 100000000 nodes ( 8.96 %) | cpu 88048295 / 100000000 nodes ( 88.05 %) | 1.43 GB | 0.620297 secs 
[2022-12-11 23:39:45.262366: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 2991988 / 100000000 nodes ( 2.99 %~3.00 %) | remote 8959717 / 100000000 nodes ( 8.96 %) | cpu 88048295 / 100000000 nodes ( 88.05 %) | 1.43 GB | 0.620435 secs 
[2022-12-11 23:39:45.262451: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 12399996
[2022-12-11 23:39:45.262756: [E2022-12-11 23:39:45 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc262773:: 638E]  eager release cuda mem 12399996/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
:[19552022-12-11 23:39:45] .Asymm Coll cache (policy: coll_cache_asymm_link) | local 2974319 / 100000000 nodes ( 2.97 %~3.00 %) | remote 8977386 / 100000000 nodes ( 8.98 %) | cpu 88048295 / 100000000 nodes ( 88.05 %) | 1.42 GB | 0.620773 secs 262831
: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-11 23:39:45:.638262859] : eager release cuda mem 12399996E
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 2989254 / 100000000 nodes ( 2.99 %~3.00 %) | remote 8962451 / 100000000 nodes ( 8.96 %) | cpu 88048295 / 100000000 nodes ( 88.05 %) | 1.43 GB | 0.62093 secs 
[2022-12-11 23:39:45.263494: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 2973362 / 100000000 nodes ( 2.97 %~3.00 %) | remote 8978343 / 100000000 nodes ( 8.98 %) | cpu 88048295 / 100000000 nodes ( 88.05 %) | 1.42 GB | 0.62203 secs 
[2022-12-11 23:39:45.263710: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 2997101 / 100000000 nodes ( 3.00 %~3.00 %) | remote 8954604 / 100000000 nodes ( 8.95 %) | cpu 88048295 / 100000000 nodes ( 88.05 %) | 1.43 GB | 0.621288 secs 
[2022-12-11 23:39:45.264118: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 2993614 / 100000000 nodes ( 2.99 %~3.00 %) | remote 8958091 / 100000000 nodes ( 8.96 %) | cpu 88048295 / 100000000 nodes ( 88.05 %) | 1.43 GB | 0.637788 secs 
[2022-12-11 23:39:45.264743: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: coll_cache_asymm_link) | local 2996793 / 100000000 nodes ( 3.00 %~3.00 %) | remote 8954912 / 100000000 nodes ( 8.95 %) | cpu 88048295 / 100000000 nodes ( 88.05 %) | 1.43 GB | 0.622729 secs 
[2022-12-11 23:39:45.266370: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 9.75 GB
[2022-12-11 23:39:46.603578: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 10.02 GB
[2022-12-11 23:39:46.603870: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 10.02 GB
[2022-12-11 23:39:46.604157: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 10.02 GB
[2022-12-11 23:39:47.993667: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 10.28 GB
[2022-12-11 23:39:47.993794: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 10.28 GB
[2022-12-11 23:39:47.994080: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 10.28 GB
[2022-12-11 23:39:49.357515: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 10.49 GB
[2022-12-11 23:39:49.357648: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 10.49 GB
[2022-12-11 23:39:49.357938: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 10.49 GB
[2022-12-11 23:39:50.636061: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 10.71 GB
[2022-12-11 23:39:50.637080: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 10.71 GB
[2022-12-11 23:39:50.638663: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 10.71 GB
[2022-12-11 23:39:52.108711: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 11.17 GB
[2022-12-11 23:39:52.109744: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 11.17 GB
[2022-12-11 23:39:52.110510: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 11.17 GB
[2022-12-11 23:39:53.998634: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 11.36 GB
[2022-12-11 23:39:53.999687: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 11.36 GB
[HCTR][23:39:55.502][ERROR][RK0][tid #139902503081728]: replica 2 calling init per replica done, doing barrier
[HCTR][23:39:55.502][ERROR][RK0][tid #139902503081728]: replica 7 calling init per replica done, doing barrier
[HCTR][23:39:55.502][ERROR][RK0][main]: replica 1 calling init per replica done, doing barrier
[HCTR][23:39:55.502][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier
[HCTR][23:39:55.502][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier
[HCTR][23:39:55.502][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier
[HCTR][23:39:55.502][ERROR][RK0][tid #139904189196032]: replica 5 calling init per replica done, doing barrier
[HCTR][23:39:55.502][ERROR][RK0][tid #139903048345344]: replica 0 calling init per replica done, doing barrier
[HCTR][23:39:55.503][ERROR][RK0][tid #139902503081728]: replica 7 calling init per replica done, doing barrier done
[HCTR][23:39:55.503][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier done
[HCTR][23:39:55.503][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier done
[HCTR][23:39:55.503][ERROR][RK0][main]: replica 1 calling init per replica done, doing barrier done
[HCTR][23:39:55.503][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier done
[HCTR][23:39:55.503][ERROR][RK0][tid #139904189196032]: replica 5 calling init per replica done, doing barrier done
[HCTR][23:39:55.503][ERROR][RK0][tid #139902503081728]: replica 2 calling init per replica done, doing barrier done
[HCTR][23:39:55.503][ERROR][RK0][tid #139903048345344]: replica 0 calling init per replica done, doing barrier done
[HCTR][23:39:55.503][ERROR][RK0][tid #139902503081728]: init per replica done
[HCTR][23:39:55.503][ERROR][RK0][main]: init per replica done
[HCTR][23:39:55.503][ERROR][RK0][main]: init per replica done
[HCTR][23:39:55.503][ERROR][RK0][main]: init per replica done
[HCTR][23:39:55.503][ERROR][RK0][main]: init per replica done
[HCTR][23:39:55.503][ERROR][RK0][tid #139904189196032]: init per replica done
[HCTR][23:39:55.503][ERROR][RK0][tid #139902503081728]: init per replica done
[HCTR][23:39:55.505][ERROR][RK0][tid #139903048345344]: init per replica done
[HCTR][23:39:55.541][ERROR][RK0][tid #139902503081728]: 2 allocated 3276800 at 0x7f20bc238400
[HCTR][23:39:55.541][ERROR][RK0][tid #139902503081728]: 2 allocated 6553600 at 0x7f20bc558400
[HCTR][23:39:55.541][ERROR][RK0][tid #139902503081728]: 2 allocated 3276800 at 0x7f20bcb98400
[HCTR][23:39:55.541][ERROR][RK0][tid #139902503081728]: 2 allocated 6553600 at 0x7f20bceb8400
[HCTR][23:39:55.541][ERROR][RK0][main]: 3 allocated 3276800 at 0x7f21f8238400
[HCTR][23:39:55.541][ERROR][RK0][main]: 3 allocated 6553600 at 0x7f21f8558400
[HCTR][23:39:55.541][ERROR][RK0][main]: 3 allocated 3276800 at 0x7f21f8b98400
[HCTR][23:39:55.541][ERROR][RK0][main]: 3 allocated 6553600 at 0x7f21f8eb8400
[HCTR][23:39:55.541][ERROR][RK0][tid #139904189196032]: 5 allocated 3276800 at 0x7f20f4238400
[HCTR][23:39:55.541][ERROR][RK0][tid #139904189196032]: 5 allocated 6553600 at 0x7f20f4558400
[HCTR][23:39:55.541][ERROR][RK0][tid #139904189196032]: 5 allocated 3276800 at 0x7f20f4b98400
[HCTR][23:39:55.541][ERROR][RK0][tid #139904189196032]: 5 allocated 6553600 at 0x7f20f4eb8400
[HCTR][23:39:55.541][ERROR][RK0][tid #139902503081728]: 7 allocated 3276800 at 0x7f21fe238400
[HCTR][23:39:55.541][ERROR][RK0][tid #139902503081728]: 7 allocated 6553600 at 0x7f21fe558400
[HCTR][23:39:55.541][ERROR][RK0][tid #139902503081728]: 7 allocated 3276800 at 0x7f21feb98400
[HCTR][23:39:55.541][ERROR][RK0][tid #139902503081728]: 7 allocated 6553600 at 0x7f21feeb8400
[HCTR][23:39:55.541][ERROR][RK0][main]: 6 allocated 3276800 at 0x7f2168238400
[HCTR][23:39:55.541][ERROR][RK0][main]: 6 allocated 6553600 at 0x7f2168558400
[HCTR][23:39:55.541][ERROR][RK0][main]: 6 allocated 3276800 at 0x7f2168b98400
[HCTR][23:39:55.541][ERROR][RK0][main]: 6 allocated 6553600 at 0x7f2168eb8400
[HCTR][23:39:55.541][ERROR][RK0][tid #139902511474432]: 4 allocated 3276800 at 0x7f21c8238400
[HCTR][23:39:55.542][ERROR][RK0][tid #139902511474432]: 4 allocated 6553600 at 0x7f21c8558400
[HCTR][23:39:55.542][ERROR][RK0][tid #139902511474432]: 4 allocated 3276800 at 0x7f21c8b98400
[HCTR][23:39:55.542][ERROR][RK0][tid #139902511474432]: 4 allocated 6553600 at 0x7f21c8eb8400
[HCTR][23:39:55.542][ERROR][RK0][main]: 1 allocated 3276800 at 0x7f20e8238400
[HCTR][23:39:55.542][ERROR][RK0][main]: 1 allocated 6553600 at 0x7f20e8558400
[HCTR][23:39:55.542][ERROR][RK0][main]: 1 allocated 3276800 at 0x7f20e8b98400
[HCTR][23:39:55.542][ERROR][RK0][main]: 1 allocated 6553600 at 0x7f20e8eb8400
[HCTR][23:39:55.544][ERROR][RK0][main]: 0 allocated 3276800 at 0x7f21b0320000
[HCTR][23:39:55.544][ERROR][RK0][main]: 0 allocated 6553600 at 0x7f21b0640000
[HCTR][23:39:55.544][ERROR][RK0][main]: 0 allocated 3276800 at 0x7f21b0c80000
[HCTR][23:39:55.544][ERROR][RK0][main]: 0 allocated 6553600 at 0x7f21b0fa0000
