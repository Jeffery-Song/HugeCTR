2022-12-12 06:22:30.043688: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.051698: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.057427: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.062610: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.068246: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.079281: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.086899: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.100177: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.149928: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.155326: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.158021: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.158977: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.160023: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.161082: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.162278: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.164165: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.165215: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.165324: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.166997: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.166998: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.168736: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.168845: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.170309: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.170512: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.171702: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.172207: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.173417: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.173853: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.174823: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.175596: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.176278: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.177358: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.178556: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.179662: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.180616: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.181652: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.182687: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.183763: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.184957: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.186645: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.187351: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.188669: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.189625: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.190659: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.191759: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.192115: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 06:22:30.192742: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.193728: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.194774: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.198565: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.199925: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.201073: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.201145: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.202615: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.202655: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.204049: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.204150: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.205716: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.207740: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.210266: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.210977: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.213290: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.215583: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.216159: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.217577: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.218336: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.219796: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.220971: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.222552: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.222776: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.223851: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.224642: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.225546: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.225790: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.227326: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.229282: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.229821: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.230023: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.230156: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.231862: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.232750: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.233287: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.233305: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.235006: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.239684: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.256421: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.271250: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.271598: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.271828: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.273762: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.274044: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.274421: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.275064: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.275253: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.275789: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.276890: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.278438: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.278824: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.279333: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.279476: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.281087: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.281708: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.282878: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.284113: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.284608: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.285199: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.285945: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.286460: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.287661: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.288034: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.288662: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.289581: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.290738: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.291145: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.291794: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.292282: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.293000: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.295390: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.295481: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.296051: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.296322: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.297159: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.299096: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.300319: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.300627: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.301315: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.302749: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.303019: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.303389: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.304099: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.305477: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.305838: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.306072: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.307049: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.308554: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.309061: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.309395: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.310278: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.311465: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.312075: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.312345: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.313245: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.314674: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.315220: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.315505: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.316941: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.317357: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.317598: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.318188: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.319948: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.320136: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.320769: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.321017: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.322472: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.323455: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.323475: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.324147: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.324278: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.326240: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.326900: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.326994: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.328762: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.328762: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.329669: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.330280: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.330431: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.332141: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.332367: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.333191: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.333867: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.335380: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.335465: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.335751: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.336100: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.337028: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.337928: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.339232: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.339341: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.339427: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.339655: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.340905: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.342124: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.342185: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.343404: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.343494: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.343683: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.344127: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.346761: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.346914: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.347736: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.347874: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.348026: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.348120: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.349827: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 06:22:30.350320: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.350726: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.351786: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.351898: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.351991: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.352256: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.354356: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.354531: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.355866: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.355909: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.355977: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.356301: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.358306: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.358513: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.358948: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.359876: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.359931: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.362199: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.362405: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.363070: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.363748: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.363912: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 06:22:30.363955: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.364703: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 06:22:30.365729: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.365998: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.366635: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.367427: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.369163: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.369324: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.370331: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.370761: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 06:22:30.371781: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.371937: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.372779: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.373194: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.373445: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.375447: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.375765: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.376899: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.377146: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.377518: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.379171: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.379394: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.379798: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.381040: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.381531: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.382623: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.383113: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.383293: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.383604: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.386751: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.387242: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.387625: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.387926: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.391185: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.391816: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.392093: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.425207: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.425872: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.426438: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.429873: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.430500: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.431653: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.434334: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.435288: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.435606: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.439362: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.441942: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.442189: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.446891: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.448331: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.448458: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.453302: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.453355: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.454668: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 06:22:30.463906: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.485696: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.485869: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.487614: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.490408: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.490935: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.507867: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.512267: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.512546: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.518009: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.521653: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 06:22:30.532756: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.584286: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.585884: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-12 06:22:30.595022: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.619678: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.620385: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:30.628717: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:31.512724: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:31.513565: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:31.514111: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:31.514585: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 06:22:31.514646: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 06:22:31.533152: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:31.533820: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:31.534976: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:31.535570: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:31.536099: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:31.536776: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-12 06:22:31.584091: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:22:31.584298: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:22:31.615783: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-12 06:22:31.740801: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:31.742182: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:31.742710: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:31.743201: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 06:22:31.743262: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 06:22:31.761896: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:31.762514: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:31.763444: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:31.764033: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:31.764942: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:31.765410: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-12 06:22:31.784414: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:31.784414: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:31.785704: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:31.785793: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:31.786611: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:31.787027: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:31.787584: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 06:22:31.787643: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 06:22:31.787782: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 06:22:31.787831: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 06:22:31.798037: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:31.798634: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:31.799175: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:31.799650: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 06:22:31.799727: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 06:22:31.805301: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:31.805586: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:31.806145: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:31.806583: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:31.807143: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:31.807443: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:31.808532: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:31.808744: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:31.809393: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:31.809678: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:31.810255: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-12 06:22:31.810445: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-12 06:22:31.812517: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:31.812697: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:31.813780: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:31.813854: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:31.814716: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:31.814804: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:31.815574: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 06:22:31.815642: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 06:22:31.815687: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 06:22:31.815742: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 06:22:31.816852: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:31.817442: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:31.817963: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:31.818521: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:31.819042: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:31.819523: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-12 06:22:31.833437: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:31.833824: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:31.834241: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:31.835066: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:31.835487: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:31.835505: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:31.836504: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:31.837310: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:31.837413: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:31.837912: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:31.838870: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:31.839032: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:31.839566: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:31.840357: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-12 06:22:31.840420: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 06:22:31.840493: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-12 06:22:31.840874: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-12 06:22:31.849484: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:22:31.849675: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:22:31.851478: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-12 06:22:31.856613: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:22:31.856772: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:22:31.858581: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
2022-12-12 06:22:31.858966: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:31.859667: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:31.860189: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:31.860775: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:31.861302: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-12 06:22:31.861772: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-12 06:22:31.865744: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:22:31.865895: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:22:31.867675: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
2022-12-12 06:22:31.886439: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:22:31.886639: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:22:31.887862: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:22:31.888052: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:22:31.888413: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
2022-12-12 06:22:31.889891: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
2022-12-12 06:22:31.904821: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:22:31.904994: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:22:31.906568: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
2022-12-12 06:22:31.907700: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:22:31.907888: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-12 06:22:31.909630: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
[HCTR][06:22:33.177][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][06:22:33.177][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][06:22:33.177][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][06:22:33.177][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][06:22:33.177][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][06:22:33.178][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][06:22:33.178][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][06:22:33.178][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:01,  1.54s/it]warmup run: 1it [00:01,  1.56s/it]warmup run: 1it [00:01,  1.53s/it]warmup run: 98it [00:01, 82.88it/s]warmup run: 97it [00:01, 82.35it/s]warmup run: 93it [00:01, 77.83it/s]warmup run: 1it [00:01,  1.52s/it]warmup run: 198it [00:01, 181.84it/s]warmup run: 1it [00:01,  1.53s/it]warmup run: 197it [00:01, 181.75it/s]warmup run: 187it [00:01, 170.00it/s]warmup run: 100it [00:01, 85.74it/s]warmup run: 297it [00:01, 289.63it/s]warmup run: 87it [00:01, 74.11it/s]warmup run: 1it [00:01,  1.49s/it]warmup run: 1it [00:01,  1.55s/it]warmup run: 1it [00:01,  1.50s/it]warmup run: 281it [00:01, 272.03it/s]warmup run: 297it [00:01, 291.12it/s]warmup run: 200it [00:01, 185.70it/s]warmup run: 395it [00:01, 399.95it/s]warmup run: 174it [00:01, 160.56it/s]warmup run: 100it [00:01, 87.21it/s]warmup run: 97it [00:01, 81.49it/s]warmup run: 100it [00:01, 86.57it/s]warmup run: 396it [00:01, 403.13it/s]warmup run: 376it [00:01, 379.40it/s]warmup run: 299it [00:01, 293.99it/s]warmup run: 493it [00:02, 507.11it/s]warmup run: 255it [00:01, 247.71it/s]warmup run: 199it [00:01, 187.42it/s]warmup run: 197it [00:01, 179.99it/s]warmup run: 199it [00:01, 186.18it/s]warmup run: 469it [00:02, 480.86it/s]warmup run: 496it [00:02, 513.45it/s]warmup run: 396it [00:01, 403.02it/s]warmup run: 589it [00:02, 601.71it/s]warmup run: 328it [00:01, 320.04it/s]warmup run: 300it [00:01, 299.80it/s]warmup run: 296it [00:01, 287.66it/s]warmup run: 298it [00:01, 295.37it/s]warmup run: 564it [00:02, 578.39it/s]warmup run: 597it [00:02, 616.84it/s]warmup run: 494it [00:02, 511.09it/s]warmup run: 416it [00:02, 423.28it/s]warmup run: 683it [00:02, 654.85it/s]warmup run: 400it [00:01, 414.18it/s]warmup run: 394it [00:01, 398.02it/s]warmup run: 398it [00:01, 409.31it/s]warmup run: 658it [00:02, 660.26it/s]warmup run: 697it [00:02, 704.38it/s]warmup run: 590it [00:02, 605.37it/s]warmup run: 508it [00:02, 525.87it/s]warmup run: 773it [00:02, 707.04it/s]warmup run: 500it [00:01, 524.34it/s]warmup run: 493it [00:02, 507.21it/s]warmup run: 494it [00:02, 512.89it/s]warmup run: 796it [00:02, 774.52it/s]warmup run: 684it [00:02, 666.76it/s]warmup run: 600it [00:02, 615.59it/s]warmup run: 750it [00:02, 671.46it/s]warmup run: 862it [00:02, 752.83it/s]warmup run: 601it [00:02, 626.29it/s]warmup run: 594it [00:02, 611.44it/s]warmup run: 591it [00:02, 608.72it/s]warmup run: 895it [00:02, 830.52it/s]warmup run: 786it [00:02, 751.30it/s]warmup run: 692it [00:02, 690.03it/s]warmup run: 851it [00:02, 753.64it/s]warmup run: 957it [00:02, 803.99it/s]warmup run: 701it [00:02, 712.65it/s]warmup run: 694it [00:02, 700.49it/s]warmup run: 688it [00:02, 692.42it/s]warmup run: 995it [00:02, 874.27it/s]warmup run: 887it [00:02, 817.50it/s]warmup run: 785it [00:02, 750.53it/s]warmup run: 954it [00:02, 823.32it/s]warmup run: 1055it [00:02, 851.41it/s]warmup run: 799it [00:02, 779.12it/s]warmup run: 794it [00:02, 773.30it/s]warmup run: 786it [00:02, 762.33it/s]warmup run: 1096it [00:02, 910.70it/s]warmup run: 986it [00:02, 862.63it/s]warmup run: 876it [00:02, 793.00it/s]warmup run: 1055it [00:02, 872.38it/s]warmup run: 1154it [00:02, 888.11it/s]warmup run: 897it [00:02, 830.60it/s]warmup run: 894it [00:02, 831.47it/s]warmup run: 884it [00:02, 816.93it/s]warmup run: 1196it [00:02, 933.71it/s]warmup run: 1086it [00:02, 898.63it/s]warmup run: 966it [00:02, 818.71it/s]warmup run: 1155it [00:02, 906.29it/s]warmup run: 1253it [00:02, 916.72it/s]warmup run: 996it [00:02, 873.22it/s]warmup run: 993it [00:02, 874.11it/s]warmup run: 982it [00:02, 860.46it/s]warmup run: 1295it [00:02, 906.99it/s]warmup run: 1184it [00:02, 916.51it/s]warmup run: 1057it [00:02, 843.74it/s]warmup run: 1256it [00:02, 933.81it/s]warmup run: 1349it [00:02, 924.85it/s]warmup run: 1096it [00:02, 907.02it/s]warmup run: 1093it [00:02, 907.24it/s]warmup run: 1082it [00:02, 899.11it/s]warmup run: 1397it [00:02, 937.31it/s]warmup run: 1284it [00:02, 938.97it/s]warmup run: 1149it [00:02, 864.97it/s]warmup run: 1357it [00:02, 954.07it/s]warmup run: 1448it [00:03, 942.27it/s]warmup run: 1193it [00:02, 932.30it/s]warmup run: 1195it [00:02, 913.84it/s]warmup run: 1184it [00:02, 931.70it/s]warmup run: 1500it [00:03, 962.68it/s]warmup run: 1383it [00:02, 953.63it/s]warmup run: 1242it [00:02, 881.93it/s]warmup run: 1458it [00:03, 969.57it/s]warmup run: 1546it [00:03, 952.60it/s]warmup run: 1292it [00:02, 944.09it/s]warmup run: 1294it [00:02, 933.47it/s]warmup run: 1285it [00:02, 952.09it/s]warmup run: 1482it [00:03, 964.04it/s]warmup run: 1603it [00:03, 979.74it/s]warmup run: 1339it [00:03, 904.84it/s]warmup run: 1560it [00:03, 983.19it/s]warmup run: 1645it [00:03, 961.08it/s]warmup run: 1391it [00:02, 949.10it/s]warmup run: 1392it [00:02, 946.27it/s]warmup run: 1386it [00:02, 966.39it/s]warmup run: 1581it [00:03, 970.46it/s]warmup run: 1706it [00:03, 992.78it/s]warmup run: 1435it [00:03, 920.49it/s]warmup run: 1663it [00:03, 994.56it/s]warmup run: 1745it [00:03, 972.23it/s]warmup run: 1490it [00:03, 955.65it/s]warmup run: 1489it [00:03, 953.52it/s]warmup run: 1488it [00:03, 980.24it/s]warmup run: 1809it [00:03, 1000.57it/s]warmup run: 1680it [00:03, 970.99it/s]warmup run: 1532it [00:03, 932.77it/s]warmup run: 1765it [00:03, 1000.29it/s]warmup run: 1845it [00:03, 979.94it/s]warmup run: 1589it [00:03, 963.43it/s]warmup run: 1587it [00:03, 956.76it/s]warmup run: 1589it [00:03, 987.13it/s]warmup run: 1780it [00:03, 978.16it/s]warmup run: 1910it [00:03, 994.48it/s] warmup run: 1630it [00:03, 944.76it/s]warmup run: 1867it [00:03, 1004.94it/s]warmup run: 1944it [00:03, 981.21it/s]warmup run: 1688it [00:03, 971.18it/s]warmup run: 1685it [00:03, 957.17it/s]warmup run: 1690it [00:03, 991.60it/s]warmup run: 1881it [00:03, 985.39it/s]warmup run: 2011it [00:03, 993.84it/s]warmup run: 1731it [00:03, 962.35it/s]warmup run: 1969it [00:03, 1008.01it/s]warmup run: 2050it [00:03, 1003.91it/s]warmup run: 1787it [00:03, 973.35it/s]warmup run: 1783it [00:03, 960.88it/s]warmup run: 1791it [00:03, 996.93it/s]warmup run: 1982it [00:03, 990.20it/s]warmup run: 2130it [00:03, 1049.80it/s]warmup run: 1831it [00:03, 972.57it/s]warmup run: 2081it [00:03, 1040.70it/s]warmup run: 2168it [00:03, 1054.27it/s]warmup run: 1886it [00:03, 975.79it/s]warmup run: 1881it [00:03, 964.44it/s]warmup run: 1893it [00:03, 1003.21it/s]warmup run: 2100it [00:03, 1044.49it/s]warmup run: 2249it [00:03, 1089.35it/s]warmup run: 1931it [00:03, 979.65it/s]warmup run: 2199it [00:03, 1081.96it/s]warmup run: 2286it [00:03, 1090.14it/s]warmup run: 1986it [00:03, 980.32it/s]warmup run: 1979it [00:03, 966.48it/s]warmup run: 1994it [00:03, 1003.43it/s]warmup run: 2222it [00:03, 1095.30it/s]warmup run: 2368it [00:03, 1116.50it/s]warmup run: 2032it [00:03, 986.96it/s]warmup run: 2318it [00:03, 1112.18it/s]warmup run: 2402it [00:03, 1108.89it/s]warmup run: 2103it [00:03, 1035.34it/s]warmup run: 2093it [00:03, 1017.96it/s]warmup run: 2110it [00:03, 1047.94it/s]warmup run: 2344it [00:03, 1131.61it/s]warmup run: 2483it [00:03, 1124.62it/s]warmup run: 2148it [00:03, 1036.56it/s]warmup run: 2437it [00:04, 1134.07it/s]warmup run: 2519it [00:04, 1126.02it/s]warmup run: 2224it [00:03, 1085.45it/s]warmup run: 2213it [00:03, 1070.64it/s]warmup run: 2226it [00:03, 1079.82it/s]warmup run: 2466it [00:03, 1157.05it/s]warmup run: 2599it [00:04, 1133.82it/s]warmup run: 2266it [00:03, 1077.28it/s]warmup run: 2556it [00:04, 1148.25it/s]warmup run: 2636it [00:04, 1137.53it/s]warmup run: 2345it [00:03, 1120.19it/s]warmup run: 2333it [00:03, 1107.73it/s]warmup run: 2347it [00:03, 1117.43it/s]warmup run: 2588it [00:04, 1174.87it/s]warmup run: 2716it [00:04, 1141.97it/s]warmup run: 2387it [00:04, 1114.57it/s]warmup run: 2673it [00:04, 1154.19it/s]warmup run: 2754it [00:04, 1149.24it/s]warmup run: 2465it [00:03, 1141.91it/s]warmup run: 2453it [00:03, 1133.46it/s]warmup run: 2468it [00:03, 1143.75it/s]warmup run: 2709it [00:04, 1184.04it/s]warmup run: 2834it [00:04, 1150.86it/s]warmup run: 2507it [00:04, 1139.92it/s]warmup run: 2792it [00:04, 1162.57it/s]warmup run: 2872it [00:04, 1156.91it/s]warmup run: 2584it [00:04, 1154.03it/s]warmup run: 2573it [00:04, 1151.23it/s]warmup run: 2589it [00:04, 1163.27it/s]warmup run: 2831it [00:04, 1194.25it/s]warmup run: 2952it [00:04, 1157.22it/s]warmup run: 2628it [00:04, 1158.34it/s]warmup run: 2911it [00:04, 1168.04it/s]warmup run: 2988it [00:04, 1152.24it/s]warmup run: 2701it [00:04, 1156.94it/s]warmup run: 2693it [00:04, 1165.10it/s]warmup run: 3000it [00:04, 666.20it/s] warmup run: 2709it [00:04, 1174.15it/s]warmup run: 3000it [00:04, 678.57it/s] warmup run: 3000it [00:04, 668.32it/s] warmup run: 2953it [00:04, 1199.44it/s]warmup run: 2746it [00:04, 1162.04it/s]warmup run: 2817it [00:04, 1157.54it/s]warmup run: 2810it [00:04, 1157.09it/s]warmup run: 3000it [00:04, 683.43it/s] warmup run: 2827it [00:04, 1173.91it/s]warmup run: 2866it [00:04, 1171.81it/s]warmup run: 2933it [00:04, 1158.10it/s]warmup run: 2931it [00:04, 1169.91it/s]warmup run: 2947it [00:04, 1179.05it/s]warmup run: 3000it [00:04, 685.90it/s] warmup run: 3000it [00:04, 686.85it/s] warmup run: 3000it [00:04, 675.19it/s] warmup run: 2985it [00:04, 1176.88it/s]warmup run: 3000it [00:04, 655.69it/s] 
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]





warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   5%|         | 163/3000 [00:00<00:01, 1629.27it/s]warmup should be done:   5%|         | 163/3000 [00:00<00:01, 1627.66it/s]warmup should be done:   5%|         | 164/3000 [00:00<00:01, 1634.22it/s]warmup should be done:   5%|         | 163/3000 [00:00<00:01, 1627.29it/s]warmup should be done:   5%|         | 164/3000 [00:00<00:01, 1635.97it/s]warmup should be done:   5%|         | 161/3000 [00:00<00:01, 1605.07it/s]warmup should be done:   5%|         | 159/3000 [00:00<00:01, 1583.49it/s]warmup should be done:   5%|         | 164/3000 [00:00<00:01, 1635.21it/s]warmup should be done:  11%|         | 328/3000 [00:00<00:01, 1640.48it/s]warmup should be done:  11%|         | 327/3000 [00:00<00:01, 1633.29it/s]warmup should be done:  11%|         | 325/3000 [00:00<00:01, 1625.26it/s]warmup should be done:  11%|         | 330/3000 [00:00<00:01, 1646.74it/s]warmup should be done:  11%|         | 327/3000 [00:00<00:01, 1631.10it/s]warmup should be done:  11%|         | 330/3000 [00:00<00:01, 1646.87it/s]warmup should be done:  11%|         | 328/3000 [00:00<00:01, 1631.83it/s]warmup should be done:  11%|         | 320/3000 [00:00<00:01, 1593.88it/s]warmup should be done:  16%|        | 488/3000 [00:00<00:01, 1625.51it/s]warmup should be done:  16%|        | 493/3000 [00:00<00:01, 1641.42it/s]warmup should be done:  17%|        | 496/3000 [00:00<00:01, 1650.09it/s]warmup should be done:  16%|        | 491/3000 [00:00<00:01, 1629.16it/s]warmup should be done:  17%|        | 496/3000 [00:00<00:01, 1648.31it/s]warmup should be done:  16%|        | 491/3000 [00:00<00:01, 1627.54it/s]warmup should be done:  16%|        | 480/3000 [00:00<00:01, 1591.78it/s]warmup should be done:  16%|        | 492/3000 [00:00<00:01, 1625.76it/s]warmup should be done:  22%|       | 651/3000 [00:00<00:01, 1622.33it/s]warmup should be done:  22%|       | 661/3000 [00:00<00:01, 1648.46it/s]warmup should be done:  22%|       | 658/3000 [00:00<00:01, 1639.41it/s]warmup should be done:  22%|       | 662/3000 [00:00<00:01, 1650.15it/s]warmup should be done:  22%|       | 654/3000 [00:00<00:01, 1623.88it/s]warmup should be done:  22%|       | 654/3000 [00:00<00:01, 1624.15it/s]warmup should be done:  22%|       | 655/3000 [00:00<00:01, 1624.62it/s]warmup should be done:  21%|       | 640/3000 [00:00<00:01, 1587.89it/s]warmup should be done:  28%|       | 826/3000 [00:00<00:01, 1646.99it/s]warmup should be done:  27%|       | 822/3000 [00:00<00:01, 1637.49it/s]warmup should be done:  28%|       | 828/3000 [00:00<00:01, 1648.56it/s]warmup should be done:  27%|       | 819/3000 [00:00<00:01, 1629.03it/s]warmup should be done:  27%|       | 814/3000 [00:00<00:01, 1614.86it/s]warmup should be done:  27%|       | 799/3000 [00:00<00:01, 1586.01it/s]warmup should be done:  27%|       | 817/3000 [00:00<00:01, 1619.41it/s]warmup should be done:  27%|       | 817/3000 [00:00<00:01, 1595.90it/s]warmup should be done:  33%|      | 993/3000 [00:00<00:01, 1645.09it/s]warmup should be done:  33%|      | 986/3000 [00:00<00:01, 1630.28it/s]warmup should be done:  33%|      | 983/3000 [00:00<00:01, 1629.56it/s]warmup should be done:  33%|      | 991/3000 [00:00<00:01, 1635.10it/s]warmup should be done:  33%|      | 976/3000 [00:00<00:01, 1609.68it/s]warmup should be done:  32%|      | 958/3000 [00:00<00:01, 1579.06it/s]warmup should be done:  33%|      | 979/3000 [00:00<00:01, 1611.55it/s]warmup should be done:  33%|      | 978/3000 [00:00<00:01, 1598.69it/s]warmup should be done:  39%|      | 1158/3000 [00:00<00:01, 1646.19it/s]warmup should be done:  38%|      | 1148/3000 [00:00<00:01, 1633.84it/s]warmup should be done:  38%|      | 1155/3000 [00:00<00:01, 1636.07it/s]warmup should be done:  38%|      | 1137/3000 [00:00<00:01, 1606.95it/s]warmup should be done:  37%|      | 1116/3000 [00:00<00:01, 1577.31it/s]warmup should be done:  38%|      | 1150/3000 [00:00<00:01, 1623.09it/s]warmup should be done:  38%|      | 1141/3000 [00:00<00:01, 1608.01it/s]warmup should be done:  38%|      | 1138/3000 [00:00<00:01, 1595.86it/s]warmup should be done:  44%|     | 1323/3000 [00:00<00:01, 1646.01it/s]warmup should be done:  44%|     | 1313/3000 [00:00<00:01, 1637.32it/s]warmup should be done:  44%|     | 1320/3000 [00:00<00:01, 1638.30it/s]warmup should be done:  42%|     | 1274/3000 [00:00<00:01, 1577.61it/s]warmup should be done:  44%|     | 1313/3000 [00:00<00:01, 1624.64it/s]warmup should be done:  43%|     | 1299/3000 [00:00<00:01, 1608.43it/s]warmup should be done:  43%|     | 1302/3000 [00:00<00:01, 1606.54it/s]warmup should be done:  43%|     | 1299/3000 [00:00<00:01, 1597.42it/s]warmup should be done:  50%|     | 1488/3000 [00:00<00:00, 1646.54it/s]warmup should be done:  49%|     | 1478/3000 [00:00<00:00, 1639.19it/s]warmup should be done:  50%|     | 1485/3000 [00:00<00:00, 1639.72it/s]warmup should be done:  48%|     | 1432/3000 [00:00<00:00, 1577.25it/s]warmup should be done:  49%|     | 1476/3000 [00:00<00:00, 1625.50it/s]warmup should be done:  49%|     | 1462/3000 [00:00<00:00, 1612.47it/s]warmup should be done:  49%|     | 1463/3000 [00:00<00:00, 1606.78it/s]warmup should be done:  49%|     | 1460/3000 [00:00<00:00, 1600.78it/s]warmup should be done:  55%|    | 1653/3000 [00:01<00:00, 1647.28it/s]warmup should be done:  55%|    | 1642/3000 [00:01<00:00, 1638.82it/s]warmup should be done:  55%|    | 1650/3000 [00:01<00:00, 1641.36it/s]warmup should be done:  55%|    | 1639/3000 [00:01<00:00, 1626.50it/s]warmup should be done:  53%|    | 1590/3000 [00:01<00:00, 1576.37it/s]warmup should be done:  54%|    | 1625/3000 [00:01<00:00, 1615.45it/s]warmup should be done:  54%|    | 1624/3000 [00:01<00:00, 1604.89it/s]warmup should be done:  54%|    | 1621/3000 [00:01<00:00, 1601.90it/s]warmup should be done:  61%|    | 1818/3000 [00:01<00:00, 1647.79it/s]warmup should be done:  60%|    | 1807/3000 [00:01<00:00, 1640.80it/s]warmup should be done:  60%|    | 1815/3000 [00:01<00:00, 1642.41it/s]warmup should be done:  60%|    | 1802/3000 [00:01<00:00, 1627.18it/s]warmup should be done:  58%|    | 1748/3000 [00:01<00:00, 1577.19it/s]warmup should be done:  60%|    | 1789/3000 [00:01<00:00, 1620.48it/s]warmup should be done:  60%|    | 1785/3000 [00:01<00:00, 1603.80it/s]warmup should be done:  59%|    | 1782/3000 [00:01<00:00, 1601.83it/s]warmup should be done:  66%|   | 1983/3000 [00:01<00:00, 1646.52it/s]warmup should be done:  66%|   | 1972/3000 [00:01<00:00, 1640.24it/s]warmup should be done:  66%|   | 1965/3000 [00:01<00:00, 1626.30it/s]warmup should be done:  64%|   | 1908/3000 [00:01<00:00, 1583.33it/s]warmup should be done:  65%|   | 1953/3000 [00:01<00:00, 1624.62it/s]warmup should be done:  65%|   | 1946/3000 [00:01<00:00, 1604.05it/s]warmup should be done:  65%|   | 1943/3000 [00:01<00:00, 1602.42it/s]warmup should be done:  66%|   | 1980/3000 [00:01<00:00, 1619.48it/s]warmup should be done:  72%|  | 2148/3000 [00:01<00:00, 1644.87it/s]warmup should be done:  71%|   | 2137/3000 [00:01<00:00, 1641.57it/s]warmup should be done:  69%|   | 2070/3000 [00:01<00:00, 1592.96it/s]warmup should be done:  71%|   | 2128/3000 [00:01<00:00, 1624.73it/s]warmup should be done:  71%|   | 2117/3000 [00:01<00:00, 1627.85it/s]warmup should be done:  70%|   | 2107/3000 [00:01<00:00, 1605.34it/s]warmup should be done:  70%|   | 2104/3000 [00:01<00:00, 1602.11it/s]warmup should be done:  71%|  | 2143/3000 [00:01<00:00, 1614.37it/s]warmup should be done:  77%|  | 2313/3000 [00:01<00:00, 1640.06it/s]warmup should be done:  74%|  | 2231/3000 [00:01<00:00, 1598.02it/s]warmup should be done:  77%|  | 2302/3000 [00:01<00:00, 1640.76it/s]warmup should be done:  76%|  | 2291/3000 [00:01<00:00, 1621.53it/s]warmup should be done:  76%|  | 2268/3000 [00:01<00:00, 1603.84it/s]warmup should be done:  76%|  | 2280/3000 [00:01<00:00, 1616.05it/s]warmup should be done:  76%|  | 2265/3000 [00:01<00:00, 1595.78it/s]warmup should be done:  77%|  | 2305/3000 [00:01<00:00, 1608.03it/s]warmup should be done:  83%| | 2478/3000 [00:01<00:00, 1640.49it/s]warmup should be done:  80%|  | 2393/3000 [00:01<00:00, 1603.66it/s]warmup should be done:  82%| | 2467/3000 [00:01<00:00, 1641.54it/s]warmup should be done:  82%| | 2454/3000 [00:01<00:00, 1619.63it/s]warmup should be done:  81%|  | 2429/3000 [00:01<00:00, 1604.59it/s]warmup should be done:  81%| | 2443/3000 [00:01<00:00, 1618.85it/s]warmup should be done:  81%|  | 2425/3000 [00:01<00:00, 1596.72it/s]warmup should be done:  82%| | 2469/3000 [00:01<00:00, 1615.48it/s]warmup should be done:  85%| | 2555/3000 [00:01<00:00, 1607.62it/s]warmup should be done:  88%| | 2632/3000 [00:01<00:00, 1643.23it/s]warmup should be done:  87%| | 2617/3000 [00:01<00:00, 1620.62it/s]warmup should be done:  86%| | 2590/3000 [00:01<00:00, 1605.50it/s]warmup should be done:  88%| | 2643/3000 [00:01<00:00, 1628.34it/s]warmup should be done:  87%| | 2605/3000 [00:01<00:00, 1619.08it/s]warmup should be done:  86%| | 2586/3000 [00:01<00:00, 1599.34it/s]warmup should be done:  88%| | 2634/3000 [00:01<00:00, 1624.42it/s]warmup should be done:  91%| | 2717/3000 [00:01<00:00, 1609.68it/s]warmup should be done:  93%|| 2797/3000 [00:01<00:00, 1643.22it/s]warmup should be done:  93%|| 2780/3000 [00:01<00:00, 1621.44it/s]warmup should be done:  92%|| 2751/3000 [00:01<00:00, 1605.90it/s]warmup should be done:  94%|| 2808/3000 [00:01<00:00, 1632.58it/s]warmup should be done:  92%|| 2767/3000 [00:01<00:00, 1614.45it/s]warmup should be done:  92%|| 2747/3000 [00:01<00:00, 1602.24it/s]warmup should be done:  93%|| 2797/3000 [00:01<00:00, 1613.74it/s]warmup should be done:  96%|| 2879/3000 [00:01<00:00, 1612.66it/s]warmup should be done:  99%|| 2963/3000 [00:01<00:00, 1645.23it/s]warmup should be done:  98%|| 2944/3000 [00:01<00:00, 1625.59it/s]warmup should be done:  97%|| 2914/3000 [00:01<00:00, 1610.93it/s]warmup should be done:  99%|| 2975/3000 [00:01<00:00, 1640.85it/s]warmup should be done:  98%|| 2930/3000 [00:01<00:00, 1617.43it/s]warmup should be done:  97%|| 2910/3000 [00:01<00:00, 1610.03it/s]warmup should be done:  99%|| 2960/3000 [00:01<00:00, 1617.32it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1642.97it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1638.45it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1628.68it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1625.71it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1616.91it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1610.71it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1605.14it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1594.27it/s]






warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|         | 168/3000 [00:00<00:01, 1679.25it/s]warmup should be done:   6%|         | 168/3000 [00:00<00:01, 1678.82it/s]warmup should be done:   6%|         | 168/3000 [00:00<00:01, 1677.92it/s]warmup should be done:   6%|         | 167/3000 [00:00<00:01, 1665.63it/s]warmup should be done:   6%|         | 165/3000 [00:00<00:01, 1646.14it/s]warmup should be done:   6%|         | 168/3000 [00:00<00:01, 1674.63it/s]warmup should be done:   5%|         | 163/3000 [00:00<00:01, 1624.36it/s]warmup should be done:   6%|         | 166/3000 [00:00<00:01, 1650.27it/s]warmup should be done:  11%|        | 338/3000 [00:00<00:01, 1689.61it/s]warmup should be done:  11%|         | 331/3000 [00:00<00:01, 1654.18it/s]warmup should be done:  11%|        | 338/3000 [00:00<00:01, 1687.99it/s]warmup should be done:  11%|         | 335/3000 [00:00<00:01, 1671.41it/s]warmup should be done:  11%|         | 328/3000 [00:00<00:01, 1637.54it/s]warmup should be done:  11%|        | 338/3000 [00:00<00:01, 1685.37it/s]warmup should be done:  11%|         | 337/3000 [00:00<00:01, 1679.87it/s]warmup should be done:  11%|         | 332/3000 [00:00<00:01, 1648.39it/s]warmup should be done:  16%|        | 494/3000 [00:00<00:01, 1647.41it/s]warmup should be done:  17%|        | 509/3000 [00:00<00:01, 1695.69it/s]warmup should be done:  17%|        | 509/3000 [00:00<00:01, 1695.22it/s]warmup should be done:  17%|        | 506/3000 [00:00<00:01, 1683.76it/s]warmup should be done:  17%|        | 499/3000 [00:00<00:01, 1662.10it/s]warmup should be done:  17%|        | 504/3000 [00:00<00:01, 1675.91it/s]warmup should be done:  17%|        | 508/3000 [00:00<00:01, 1689.29it/s]warmup should be done:  17%|        | 497/3000 [00:00<00:01, 1645.47it/s]warmup should be done:  23%|       | 679/3000 [00:00<00:01, 1696.48it/s]warmup should be done:  22%|       | 667/3000 [00:00<00:01, 1668.93it/s]warmup should be done:  22%|       | 661/3000 [00:00<00:01, 1654.20it/s]warmup should be done:  23%|       | 680/3000 [00:00<00:01, 1698.97it/s]warmup should be done:  22%|       | 672/3000 [00:00<00:01, 1676.50it/s]warmup should be done:  23%|       | 677/3000 [00:00<00:01, 1686.87it/s]warmup should be done:  22%|       | 675/3000 [00:00<00:01, 1680.48it/s]warmup should be done:  22%|       | 662/3000 [00:00<00:01, 1639.40it/s]warmup should be done:  28%|       | 849/3000 [00:00<00:01, 1695.44it/s]warmup should be done:  28%|       | 835/3000 [00:00<00:01, 1670.53it/s]warmup should be done:  28%|       | 840/3000 [00:00<00:01, 1676.82it/s]warmup should be done:  28%|       | 827/3000 [00:00<00:01, 1653.77it/s]warmup should be done:  28%|       | 850/3000 [00:00<00:01, 1696.86it/s]warmup should be done:  28%|       | 846/3000 [00:00<00:01, 1687.13it/s]warmup should be done:  28%|       | 844/3000 [00:00<00:01, 1679.43it/s]warmup should be done:  28%|       | 828/3000 [00:00<00:01, 1646.11it/s]warmup should be done:  33%|      | 1003/3000 [00:00<00:01, 1672.14it/s]warmup should be done:  34%|      | 1016/3000 [00:00<00:01, 1690.85it/s]warmup should be done:  33%|      | 993/3000 [00:00<00:01, 1653.96it/s]warmup should be done:  34%|      | 1019/3000 [00:00<00:01, 1693.35it/s]warmup should be done:  34%|      | 1009/3000 [00:00<00:01, 1678.01it/s]warmup should be done:  34%|      | 1020/3000 [00:00<00:01, 1694.34it/s]warmup should be done:  34%|      | 1014/3000 [00:00<00:01, 1683.08it/s]warmup should be done:  33%|      | 995/3000 [00:00<00:01, 1654.07it/s]warmup should be done:  39%|      | 1171/3000 [00:00<00:01, 1672.74it/s]warmup should be done:  39%|      | 1177/3000 [00:00<00:01, 1677.96it/s]warmup should be done:  40%|      | 1186/3000 [00:00<00:01, 1690.98it/s]warmup should be done:  39%|      | 1160/3000 [00:00<00:01, 1656.32it/s]warmup should be done:  40%|      | 1189/3000 [00:00<00:01, 1692.56it/s]warmup should be done:  40%|      | 1190/3000 [00:00<00:01, 1693.70it/s]warmup should be done:  39%|      | 1183/3000 [00:00<00:01, 1683.42it/s]warmup should be done:  39%|      | 1161/3000 [00:00<00:01, 1654.25it/s]warmup should be done:  45%|     | 1339/3000 [00:00<00:00, 1674.08it/s]warmup should be done:  45%|     | 1346/3000 [00:00<00:00, 1679.75it/s]warmup should be done:  45%|     | 1359/3000 [00:00<00:00, 1694.39it/s]warmup should be done:  45%|     | 1356/3000 [00:00<00:00, 1691.04it/s]warmup should be done:  44%|     | 1326/3000 [00:00<00:01, 1653.35it/s]warmup should be done:  45%|     | 1361/3000 [00:00<00:00, 1695.94it/s]warmup should be done:  45%|     | 1352/3000 [00:00<00:00, 1681.61it/s]warmup should be done:  44%|     | 1327/3000 [00:00<00:01, 1646.47it/s]warmup should be done:  50%|     | 1507/3000 [00:00<00:00, 1675.76it/s]warmup should be done:  51%|     | 1529/3000 [00:00<00:00, 1696.08it/s]warmup should be done:  50%|     | 1515/3000 [00:00<00:00, 1680.42it/s]warmup should be done:  51%|     | 1526/3000 [00:00<00:00, 1691.51it/s]warmup should be done:  51%|     | 1532/3000 [00:00<00:00, 1698.61it/s]warmup should be done:  50%|     | 1493/3000 [00:00<00:00, 1655.49it/s]warmup should be done:  51%|     | 1521/3000 [00:00<00:00, 1681.10it/s]warmup should be done:  50%|     | 1492/3000 [00:00<00:00, 1641.19it/s]warmup should be done:  57%|    | 1699/3000 [00:01<00:00, 1697.10it/s]warmup should be done:  56%|    | 1676/3000 [00:01<00:00, 1677.93it/s]warmup should be done:  57%|    | 1696/3000 [00:01<00:00, 1693.83it/s]warmup should be done:  56%|    | 1684/3000 [00:01<00:00, 1680.65it/s]warmup should be done:  57%|    | 1703/3000 [00:01<00:00, 1699.63it/s]warmup should be done:  55%|    | 1660/3000 [00:01<00:00, 1656.95it/s]warmup should be done:  56%|    | 1690/3000 [00:01<00:00, 1682.59it/s]warmup should be done:  55%|    | 1658/3000 [00:01<00:00, 1646.04it/s]warmup should be done:  61%|   | 1844/3000 [00:01<00:00, 1678.25it/s]warmup should be done:  62%|   | 1869/3000 [00:01<00:00, 1696.30it/s]warmup should be done:  62%|   | 1866/3000 [00:01<00:00, 1693.55it/s]warmup should be done:  62%|   | 1853/3000 [00:01<00:00, 1680.19it/s]warmup should be done:  62%|   | 1874/3000 [00:01<00:00, 1699.97it/s]warmup should be done:  61%|    | 1826/3000 [00:01<00:00, 1656.59it/s]warmup should be done:  62%|   | 1860/3000 [00:01<00:00, 1685.08it/s]warmup should be done:  61%|    | 1825/3000 [00:01<00:00, 1652.23it/s]warmup should be done:  68%|   | 2039/3000 [00:01<00:00, 1696.03it/s]warmup should be done:  67%|   | 2012/3000 [00:01<00:00, 1674.45it/s]warmup should be done:  68%|   | 2044/3000 [00:01<00:00, 1699.74it/s]warmup should be done:  67%|   | 2022/3000 [00:01<00:00, 1678.91it/s]warmup should be done:  68%|   | 2036/3000 [00:01<00:00, 1687.58it/s]warmup should be done:  66%|   | 1992/3000 [00:01<00:00, 1653.85it/s]warmup should be done:  68%|   | 2029/3000 [00:01<00:00, 1686.15it/s]warmup should be done:  66%|   | 1992/3000 [00:01<00:00, 1655.67it/s]warmup should be done:  74%|  | 2209/3000 [00:01<00:00, 1695.03it/s]warmup should be done:  73%|  | 2180/3000 [00:01<00:00, 1673.08it/s]warmup should be done:  74%|  | 2214/3000 [00:01<00:00, 1697.56it/s]warmup should be done:  72%|  | 2159/3000 [00:01<00:00, 1658.33it/s]warmup should be done:  73%|  | 2190/3000 [00:01<00:00, 1677.18it/s]warmup should be done:  73%|  | 2198/3000 [00:01<00:00, 1685.96it/s]warmup should be done:  74%|  | 2205/3000 [00:01<00:00, 1680.36it/s]warmup should be done:  72%|  | 2159/3000 [00:01<00:00, 1657.18it/s]warmup should be done:  79%|  | 2379/3000 [00:01<00:00, 1692.97it/s]warmup should be done:  78%|  | 2351/3000 [00:01<00:00, 1681.38it/s]warmup should be done:  79%|  | 2384/3000 [00:01<00:00, 1697.04it/s]warmup should be done:  79%|  | 2358/3000 [00:01<00:00, 1676.67it/s]warmup should be done:  78%|  | 2328/3000 [00:01<00:00, 1666.20it/s]warmup should be done:  79%|  | 2368/3000 [00:01<00:00, 1687.49it/s]warmup should be done:  79%|  | 2374/3000 [00:01<00:00, 1677.26it/s]warmup should be done:  78%|  | 2327/3000 [00:01<00:00, 1661.52it/s]warmup should be done:  85%| | 2549/3000 [00:01<00:00, 1691.86it/s]warmup should be done:  84%| | 2521/3000 [00:01<00:00, 1685.88it/s]warmup should be done:  85%| | 2554/3000 [00:01<00:00, 1696.68it/s]warmup should be done:  83%| | 2497/3000 [00:01<00:00, 1671.57it/s]warmup should be done:  84%| | 2527/3000 [00:01<00:00, 1678.37it/s]warmup should be done:  85%| | 2538/3000 [00:01<00:00, 1689.42it/s]warmup should be done:  85%| | 2542/3000 [00:01<00:00, 1677.59it/s]warmup should be done:  83%| | 2495/3000 [00:01<00:00, 1665.38it/s]warmup should be done:  91%| | 2719/3000 [00:01<00:00, 1692.59it/s]warmup should be done:  90%| | 2691/3000 [00:01<00:00, 1688.37it/s]warmup should be done:  91%| | 2725/3000 [00:01<00:00, 1698.54it/s]warmup should be done:  90%| | 2695/3000 [00:01<00:00, 1678.70it/s]warmup should be done:  89%| | 2665/3000 [00:01<00:00, 1672.81it/s]warmup should be done:  90%| | 2708/3000 [00:01<00:00, 1689.77it/s]warmup should be done:  90%| | 2710/3000 [00:01<00:00, 1677.68it/s]warmup should be done:  89%| | 2662/3000 [00:01<00:00, 1665.68it/s]warmup should be done:  95%|| 2861/3000 [00:01<00:00, 1691.47it/s]warmup should be done:  96%|| 2895/3000 [00:01<00:00, 1696.66it/s]warmup should be done:  95%|| 2863/3000 [00:01<00:00, 1677.88it/s]warmup should be done:  94%|| 2833/3000 [00:01<00:00, 1674.04it/s]warmup should be done:  96%|| 2889/3000 [00:01<00:00, 1688.18it/s]warmup should be done:  96%|| 2877/3000 [00:01<00:00, 1688.26it/s]warmup should be done:  96%|| 2878/3000 [00:01<00:00, 1673.07it/s]warmup should be done:  94%|| 2829/3000 [00:01<00:00, 1665.04it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1696.26it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1692.44it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1685.05it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1682.90it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1678.26it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1677.71it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1661.63it/s]warmup should be done: 100%|| 2997/3000 [00:01<00:00, 1666.68it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1655.55it/s]2022-12-12 06:24:08.977219: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f40ff9567a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 06:24:08.977280: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 06:24:09.032972: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f228802d1c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 06:24:09.033037: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 06:24:09.859487: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x1c7ff0c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 06:24:09.859555: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 06:24:09.860115: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f22c40302d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 06:24:09.860160: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 06:24:09.876363: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f40f7f91780 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 06:24:09.876433: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 06:24:10.060422: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f40f782fe10 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 06:24:10.060486: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 06:24:10.182476: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f228002c5f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 06:24:10.182543: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 06:24:10.225577: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7f21c00302f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-12 06:24:10.225640: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-12 06:24:11.228947: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 06:24:11.332228: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 06:24:12.094909: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 06:24:12.110452: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 06:24:12.182589: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 06:24:12.363711: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 06:24:12.516208: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 06:24:12.537850: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-12 06:24:14.090732: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 06:24:14.274687: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 06:24:15.043740: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 06:24:15.064773: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 06:24:15.164564: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 06:24:15.363721: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 06:24:15.454312: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-12 06:24:15.502882: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][06:24:39.114][ERROR][RK0][tid #139918047160064]: replica 3 reaches 1000, calling init pre replica
[HCTR][06:24:39.114][ERROR][RK0][tid #139918047160064]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][06:24:39.120][ERROR][RK0][tid #139918047160064]: coll ps creation done
[HCTR][06:24:39.120][ERROR][RK0][tid #139918047160064]: replica 3 waits for coll ps creation barrier
[HCTR][06:24:39.150][ERROR][RK0][main]: replica 5 reaches 1000, calling init pre replica
[HCTR][06:24:39.150][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][06:24:39.155][ERROR][RK0][main]: coll ps creation done
[HCTR][06:24:39.155][ERROR][RK0][main]: replica 5 waits for coll ps creation barrier
[HCTR][06:24:39.193][ERROR][RK0][tid #139918114268928]: replica 2 reaches 1000, calling init pre replica
[HCTR][06:24:39.194][ERROR][RK0][tid #139918114268928]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][06:24:39.201][ERROR][RK0][tid #139918114268928]: coll ps creation done
[HCTR][06:24:39.201][ERROR][RK0][tid #139918114268928]: replica 2 waits for coll ps creation barrier
[HCTR][06:24:39.210][ERROR][RK0][main]: replica 4 reaches 1000, calling init pre replica
[HCTR][06:24:39.210][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][06:24:39.214][ERROR][RK0][main]: coll ps creation done
[HCTR][06:24:39.214][ERROR][RK0][main]: replica 4 waits for coll ps creation barrier
[HCTR][06:24:39.234][ERROR][RK0][main]: replica 1 reaches 1000, calling init pre replica
[HCTR][06:24:39.234][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][06:24:39.240][ERROR][RK0][main]: coll ps creation done
[HCTR][06:24:39.240][ERROR][RK0][main]: replica 1 waits for coll ps creation barrier
[HCTR][06:24:39.248][ERROR][RK0][main]: replica 7 reaches 1000, calling init pre replica
[HCTR][06:24:39.248][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][06:24:39.250][ERROR][RK0][tid #139917703223040]: replica 0 reaches 1000, calling init pre replica
[HCTR][06:24:39.250][ERROR][RK0][tid #139917703223040]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][06:24:39.253][ERROR][RK0][main]: coll ps creation done
[HCTR][06:24:39.253][ERROR][RK0][main]: replica 7 waits for coll ps creation barrier
[HCTR][06:24:39.259][ERROR][RK0][tid #139917703223040]: coll ps creation done
[HCTR][06:24:39.259][ERROR][RK0][tid #139917703223040]: replica 0 waits for coll ps creation barrier
[HCTR][06:24:39.304][ERROR][RK0][tid #139918105876224]: replica 6 reaches 1000, calling init pre replica
[HCTR][06:24:39.304][ERROR][RK0][tid #139918105876224]: coll ps creation, with 8 devices, using policy rep_cache
[HCTR][06:24:39.313][ERROR][RK0][tid #139918105876224]: coll ps creation done
[HCTR][06:24:39.313][ERROR][RK0][tid #139918105876224]: replica 6 waits for coll ps creation barrier
[HCTR][06:24:39.313][ERROR][RK0][tid #139917703223040]: replica 0 preparing frequency
[HCTR][06:24:40.160][ERROR][RK0][tid #139917703223040]: replica 0 preparing frequency done
[HCTR][06:24:40.218][ERROR][RK0][tid #139917703223040]: replica 0 calling init per replica
[HCTR][06:24:40.218][ERROR][RK0][main]: replica 7 calling init per replica
[HCTR][06:24:40.218][ERROR][RK0][main]: replica 1 calling init per replica
[HCTR][06:24:40.218][ERROR][RK0][main]: replica 5 calling init per replica
[HCTR][06:24:40.218][ERROR][RK0][main]: replica 4 calling init per replica
[HCTR][06:24:40.218][ERROR][RK0][tid #139918114268928]: replica 2 calling init per replica
[HCTR][06:24:40.218][ERROR][RK0][tid #139918105876224]: replica 6 calling init per replica
[HCTR][06:24:40.218][ERROR][RK0][tid #139918047160064]: replica 3 calling init per replica
[HCTR][06:24:40.218][ERROR][RK0][tid #139917703223040]: Calling build_v2
[HCTR][06:24:40.218][ERROR][RK0][main]: Calling build_v2
[HCTR][06:24:40.218][ERROR][RK0][main]: Calling build_v2
[HCTR][06:24:40.218][ERROR][RK0][main]: Calling build_v2
[HCTR][06:24:40.218][ERROR][RK0][main]: Calling build_v2
[HCTR][06:24:40.218][ERROR][RK0][tid #139918114268928]: Calling build_v2
[HCTR][06:24:40.218][ERROR][RK0][tid #139918105876224]: Calling build_v2
[HCTR][06:24:40.218][ERROR][RK0][tid #139917703223040]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][06:24:40.218][ERROR][RK0][tid #139918114268928]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][06:24:40.218][ERROR][RK0][tid #139918047160064]: Calling build_v2
[HCTR][06:24:40.218][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][06:24:40.218][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][06:24:40.218][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][06:24:40.218][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][06:24:40.218][ERROR][RK0][tid #139918105876224]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][06:24:40.218][ERROR][RK0][tid #139918047160064]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[[[[[[[[2022-12-12 06:24:402022-12-12 06:24:402022-12-12 06:24:402022-12-12 06:24:402022-12-12 06:24:402022-12-12 06:24:40.2022-12-12 06:24:40.....218821.2022-12-12 06:24:40218816218817218822218821218824: 218821.: : : : : E: 218834EEEEE E:      /hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc E/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc:/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc :::::136:/hugectr_dev/third_party/collcachelib/coll_cache_lib/run_config.cc136136136136136] 136:] ] ] ] ] using concurrent impl MPS] 136using concurrent impl MPSusing concurrent impl MPS
using concurrent impl MPSusing concurrent impl MPS
using concurrent impl MPS
using concurrent impl MPS] 



using concurrent impl MPS
[2022-12-12 06:24:40.223085: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-12 06:24:40.223135: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:196] [assigning 8 to cpu2022-12-12 06:24:40
.223149: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:178] v100x8, slow pcie
[2022-12-12 06:24:40.223198: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:[1962022-12-12 06:24:40] [.assigning 8 to cpu2022-12-12 06:24:40223205
.: 223220E:  [E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 06:24:40 [:./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 06:24:40178223256:.] [: 212223278v100x8, slow pcie2022-12-12 06:24:40E] : 
. build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8[E223302/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
[2022-12-12 06:24:40 : :2022-12-12 06:24:40./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE178[.223355: ] [2022-12-12 06:24:40223353: 212/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccv100x8, slow pcie2022-12-12 06:24:40.: E] :
[.223392E build asymm link desc with 8X Tesla V100-SXM2-32GB out of 81782022-12-12 06:24:40223400[:  /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
] .: 2022-12-12 06:24:40E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:v100x8, slow pcie223443E. [:196
:  223475/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 06:24:40178] E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[: :.] assigning 8 to cpu :2022-12-12 06:24:40E213223530v100x8, slow pcie
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc178. ] : 
:] 223573[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccremote time is 8.68421E178v100x8, slow pcie[: 2022-12-12 06:24:40:
 ] 
2022-12-12 06:24:40E.196/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccv100x8, slow pcie[. [223652] :
2022-12-12 06:24:40223653/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 06:24:40: assigning 8 to cpu213.[: :.E
] 2236972022-12-12 06:24:40E196223703 remote time is 8.68421: . ] : /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
E223756/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccassigning 8 to cpuE: [[: :
 196/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 06:24:402022-12-12 06:24:40E212/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] :.. ] :assigning 8 to cpu[214223851223854/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccbuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8196
2022-12-12 06:24:40] : : :
] .cpu time is 97.0588EE196assigning 8 to cpu223912
  [] 
: [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 06:24:40assigning 8 to cpuE2022-12-12 06:24:40::.
 .212214[223980/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc223992] ] 2022-12-12 06:24:40: :: [build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8cpu time is 97.0588.E212E2022-12-12 06:24:40

224054 ]  .: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccbuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc224108E:
2022-12-12 06:24:40::  213.212E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[] 224155]  :2022-12-12 06:24:40remote time is 8.68421: build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc212.
E
:[] 224197 2122022-12-12 06:24:40build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8[: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] .
2022-12-12 06:24:40E:build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8224269. 213[
: 224279/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] 2022-12-12 06:24:40E: :[remote time is 8.68421. E2132022-12-12 06:24:40
224319/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc [] .: :/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-12 06:24:40remote time is 8.68421224348E214:.
:  ] 213224398E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[cpu time is 97.0588] :  :2022-12-12 06:24:40
remote time is 8.68421E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc213.
 :] 224445/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc213[remote time is 8.68421: :] 2022-12-12 06:24:40
E214remote time is 8.68421. ] [
224518/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cccpu time is 97.05882022-12-12 06:24:40: :[
.E2142022-12-12 06:24:40224560 ] .: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cccpu time is 97.0588224583E:
:  214E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc]  :cpu time is 97.0588/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc214
:] 214cpu time is 97.0588] 
cpu time is 97.0588
[2022-12-12 06:25:59.267792: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-12 06:25:59.307879: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
[2022-12-12 06:25:59.307948: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:919] num_cached_nodes = 25000000
[2022-12-12 06:25:59.432899: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-12 06:25:59.432988: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-12 06:25:59.588228: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-12 06:25:59.588265: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-12 06:25:59.588680: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:25:59.589615: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:25:59.590435: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:25:59.603673: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 6 solved
[2022-12-12 06:25:59.603731: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 6 initing device 6
[[2022-12-12 06:25:592022-12-12 06:25:59..603807603818: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::202202] ] 4 solved1 solved

[[2022-12-12 06:25:592022-12-12 06:25:59..603907603909: : [EE2022-12-12 06:25:59  ./hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc603911::: 205205E] ]  worker 0 thread 4 initing device 4worker 0 thread 1 initing device 1/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc

:202] 7 solved
[2022-12-12 06:25:59.603998: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 7 initing device 7
[2022-12-12 06:25:59.604128: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[[2022-12-12 06:25:592022-12-12 06:25:59..604345604347: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 381.47 MBeager alloc mem 381.47 MB[

2022-12-12 06:25:59.604389: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:25:59.606887: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:25:59.607001: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:25:59.607048: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:25:59.607103: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:25:59.607616: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 2 solved
[2022-12-12 06:25:59.607666: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 2 initing device 2
[2022-12-12 06:25:59.608028: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:25:59.610066: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:25:59.610121: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:25:59.610252: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:25:59.610346: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:25:59.611299: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[[2022-12-12 06:25:592022-12-12 06:25:59..613499613499: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc::202202] ] 3 solved5 solved

[[2022-12-12 06:25:592022-12-12 06:25:59..613579613579: : E[E 2022-12-12 06:25:59 /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc./hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:613589:205: 205] E] worker 0 thread 3 initing device 3 worker 0 thread 5 initing device 5
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
:1980] eager alloc mem 381.47 MB
[[2022-12-12 06:25:592022-12-12 06:25:59..614119614119: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 381.47 MBeager alloc mem 381.47 MB

[2022-12-12 06:25:59.615916: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:25:59.615980: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:25:59.617499: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:25:59.617631: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-12 06:25:59.672468: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.00 Bytes
[2022-12-12 06:25:59.672838: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-12 06:25:59.678981: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 06:25:59.679082: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-12 06:25:59.679147: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 06:25:59.680002: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 95.75 MB
[2022-12-12 06:25:59.680638: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:25:59.681745: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:25:59.681834: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 06:25:59.682510: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 06:25:59.682552: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.92 GB
[2022-12-12 06:25:59.693251: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.00 Bytes
[[[2022-12-12 06:25:592022-12-12 06:25:592022-12-12 06:25:59...693313693312693312: : : EEE   /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:::198019801980] ] ] eager alloc mem 2.00 Byteseager alloc mem 2.00 Byteseager alloc mem 2.00 Bytes


[2022-12-12 06:25:59.693581: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-12 06:25:59.693666[: [2022-12-12 06:25:59E2022-12-12 06:25:59. .693673/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu693674: :: E1980E ]  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 1024.00 Bytes/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:[
:19802022-12-12 06:25:591980] .] eager alloc mem 1024.00 Bytes693701eager alloc mem 1024.00 Bytes
: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.00 Bytes
[2022-12-12 06:25:59.694021: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-12 06:25:59.702308: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 06:25:59.702388: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-12 06:25:59.702430: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 06:25:59.702599: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 06:25:59.702667: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-12 06:25:59.[7026842022-12-12 06:25:59: .E702710 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE: 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] :638eager release cuda mem 1024] 
eager release cuda mem 400000000
[2022-12-12 06:25:59.[7027732022-12-12 06:25:59: .E702765 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE: 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] :eager release cuda mem 2638
] eager release cuda mem 1024
[2022-12-12 06:25:59.[7028442022-12-12 06:25:59: .E[702849 2022-12-12 06:25:59: /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.E:702845 638: /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] E:eager release cuda mem 400000000 638
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] :eager release cuda mem 2638
] eager release cuda mem 1024
[2022-12-12 06:25:59.702950[: 2022-12-12 06:25:59E. 702957/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :E638 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 400000000:
638] eager release cuda mem 2
[2022-12-12 06:25:59.703017: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 06:25:59.703747: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 95.75 MB
[2022-12-12 06:25:59.704813: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 95.75 MB
[2022-12-12 06:25:59.705599: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 95.75 MB
[2022-12-12 06:25:59.706273: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 95.75 MB
[2022-12-12 06:25:59.706782: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 95.75 MB
[2022-12-12 06:25:59.707653: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:25:59.708004: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:25:59.708215: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:25:59.708258: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:25:59.708355: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:25:59.708737: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:25:59.708823: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 06:25:59.709082: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:25:59.709180: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 06:25:59.709280: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:25:59.709333: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:25:59.709364: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 06:25:59.709419: [E2022-12-12 06:25:59 ./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu709426:: 1980E]  eager alloc mem 25.25 KB/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
:638] eager release cuda mem 625663
[2022-12-12 06:25:59.709487: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[[2022-12-12 06:25:592022-12-12 06:25:59..709526709525: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 11.92 GBeager alloc mem 25.25 KB

[2022-12-12 06:25:59.709845: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 06:25:59.709883: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.92 GB
[2022-12-12 06:25:59.710039: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 06:25:59.710078: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.92 GB
[2022-12-12 06:25:59.710110: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 06:25:59.710160: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.92 GB
[2022-12-12 06:25:59.710225: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 06:25:59.710265: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.92 GB
[2022-12-12 06:25:59.710301: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.00 Bytes
[2022-12-12 06:25:59.710581: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-12 06:25:59.711616: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 2.00 Bytes
[2022-12-12 06:25:59.711890: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 1024.00 Bytes
[2022-12-12 06:25:59.780053: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 06:25:59.780122: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-12 06:25:59.780164: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 06:25:59.780184: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 1024
[2022-12-12 06:25:59.780246: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 2
[2022-12-12 06:25:59.780286: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-12 06:25:59.781047: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 95.75 MB
[2022-12-12 06:25:59.781869: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 95.75 MB
[2022-12-12 06:25:59.782453: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:25:59.782636: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:25:59.783533: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:25:59.783620: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 06:25:59.783702: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:25:59.783787: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-12 06:25:59.784286: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 06:25:59.784325: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.92 GB
[2022-12-12 06:25:59.784455: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-12 06:25:59.784495: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 11.92 GB
[[[[[[[2022-12-12 06:26:022022-12-12 06:26:022022-12-12 06:26:02[2022-12-12 06:26:022022-12-12 06:26:022022-12-12 06:26:022022-12-12 06:26:02...2022-12-12 06:26:02....268225268224268224.268224268224268224268226: : : 268265: : : : EEE: EEEE   E    /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:::/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::198019801980:1980198019801980] ] ] 1980] ] ] ] eager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KB] eager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KBeager alloc mem 611.00 KB


eager alloc mem 611.00 KB




[2022-12-12 06:26:02.[269373[[[2022-12-12 06:26:02: [2022-12-12 06:26:022022-12-12 06:26:022022-12-12 06:26:02[.E2022-12-12 06:26:02...[2022-12-12 06:26:02269380 .2693832693842693892022-12-12 06:26:02.: /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc269387: : : .269395E:: EEE269406:  638E   : E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc]  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE :eager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::: /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638
:638638638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:] 638] ] ] :638eager release cuda mem 625663] eager release cuda mem 625663eager release cuda mem 625663eager release cuda mem 625663638] 
eager release cuda mem 625663


] eager release cuda mem 625663[
eager release cuda mem 625663
2022-12-12 06:26:02
.269618: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[[2022-12-12 06:26:02[2022-12-12 06:26:02.[2022-12-12 06:26:02.2696732022-12-12 06:26:02.[269674: .2696772022-12-12 06:26:02[: E269681: .[2022-12-12 06:26:02E : E2696922022-12-12 06:26:02. /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE : .269701/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE269712: :1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: : E1980] :1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE ] eager alloc mem 611.00 KB1980] : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 611.00 KB
] eager alloc mem 611.00 KB1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:
eager alloc mem 611.00 KB
] :1980
eager alloc mem 611.00 KB1980] 
] eager alloc mem 611.00 KBeager alloc mem 611.00 KB

[2022-12-12 06:26:02.270403: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:26:02.270477: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:26:02.270579: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] [eager release cuda mem 6256632022-12-12 06:26:02
.[2706002022-12-12 06:26:02: .[E2706082022-12-12 06:26:02 [: ./hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-12 06:26:02E270620:. : 638270626[[/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE] [: 2022-12-12 06:26:022022-12-12 06:26:02: eager release cuda mem 6256632022-12-12 06:26:02E..638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
. 270648270649] :270658/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: : eager release cuda mem 625663638: :EE
] E638  [eager release cuda mem 625663 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-12 06:26:02
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager release cuda mem 625663::.:
638[6382707591980] 2022-12-12 06:26:02] : ] eager release cuda mem 625663.eager release cuda mem 625663[Eeager alloc mem 611.00 KB
270807
2022-12-12 06:26:02 [
: ./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-12 06:26:02E270842:. : 1980270862/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE] : : eager alloc mem 611.00 KBE1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[
 [] :2022-12-12 06:26:02/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-12 06:26:02eager alloc mem 611.00 KB
1980.:.] 2709161980270920eager alloc mem 611.00 KB: ] : 
Eeager alloc mem 611.00 KBE 
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 611.00 KBeager alloc mem 611.00 KB

[2022-12-12 06:26:02.271227: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:26:02.271300: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:26:02.271611: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:26:02.271673: E[ 2022-12-12 06:26:02/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.:271685638: ] E[eager release cuda mem 625663 2022-12-12 06:26:02
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.:2717031980: ] Eeager alloc mem 611.00 KB 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[[2022-12-12 06:26:022022-12-12 06:26:02.[.2717632022-12-12 06:26:02271764: .: E271767E [:  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-12 06:26:02E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:. [:1980271794/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[2022-12-12 06:26:02638] : :2022-12-12 06:26:02.] eager alloc mem 611.00 KBE638.271811eager release cuda mem 625663
 ] 271819: 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager release cuda mem 625663: E:
E 1980 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[:eager alloc mem 611.00 KB:2022-12-12 06:26:02638
638.] eager release cuda mem 625663[] 271937
2022-12-12 06:26:02eager release cuda mem 625663: .
E271961 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: 1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] :eager alloc mem 611.00 KB1980
[] 2022-12-12 06:26:02eager alloc mem 611.00 KB.
[2720392022-12-12 06:26:02: .E272049 [: /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu2022-12-12 06:26:02E:. 1980272064/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] : :eager alloc mem 611.00 KBE638
 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager release cuda mem 625663:
1980] eager alloc mem 611.00 KB
[2022-12-12 06:26:02.272197: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:26:02.272464: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:26:02.272535: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:26:02.272607: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:26:02.272675: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:[19802022-12-12 06:26:02] .eager alloc mem 611.00 KB272688
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[[2022-12-12 06:26:022022-12-12 06:26:02..272765272766: : EE[  2022-12-12 06:26:02/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.::2727861980638: ] ] Eeager alloc mem 611.00 KBeager release cuda mem 625663 

/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:26:02.272861: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-12 06:26:02:.638272873] : [[eager release cuda mem 625663E2022-12-12 06:26:022022-12-12 06:26:02
 ../hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu272886272887:: [: 1980E [2022-12-12 06:26:02] E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-12 06:26:02.eager alloc mem 611.00 KB :.272949
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu638272961: :] : E1980eager release cuda mem 625663E ] 
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager alloc mem 611.00 KB/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:
:6381980] ] eager release cuda mem 625663eager alloc mem 611.00 KB

[2022-12-12 06:26:02.273108: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:26:02.273134: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:26:02.273279: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:26:02.273347: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:26:02.273431: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:26:02.273498: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:26:02.273553: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:26:02.273621: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:26:02.273740: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:26:02[.2022-12-12 06:26:02273806.[: 2738082022-12-12 06:26:02E: . E273819/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu : :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE1980: ] 638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc[eager alloc mem 611.00 KB] :2022-12-12 06:26:02
eager release cuda mem 625663638.[
] 2738582022-12-12 06:26:02eager release cuda mem 625663: .
E273884 : /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE: [638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-12 06:26:02] :[.eager release cuda mem 6256636382022-12-12 06:26:02273948
] .: eager release cuda mem 625663273969E
:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980:] 1980eager alloc mem 611.00 KB] 
eager alloc mem 611.00 KB
[2022-12-12 06:26:02.274054: E[ 2022-12-12 06:26:02/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.:2740651980: ] E[eager alloc mem 611.00 KB 2022-12-12 06:26:02
/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.:2740961980: ] Eeager alloc mem 611.00 KB 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:26:02.274195: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:26:02.274243: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:26:02.274308: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:26:02.274367: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:26:02.274433: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-12 06:26:02.274608: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:26:02.274675: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[[2022-12-12 06:26:022022-12-12 06:26:02..274780274782: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 625663eager release cuda mem 625663

[2022-12-12 06:26:02.274846: E [/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc2022-12-12 06:26:02:.638[274862[] 2022-12-12 06:26:02: 2022-12-12 06:26:02eager release cuda mem 625663.E.
274880 274883: /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: E:E 638[ /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu] 2022-12-12 06:26:02/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:eager release cuda mem 625663.274943[2022-12-12 06:26:02.274968:1980
: : 1980] EE] eager alloc mem 611.00 KB[  eager alloc mem 611.00 KB
2022-12-12 06:26:02/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu
.::275005638[1980: ] 2022-12-12 06:26:02] Eeager release cuda mem 625663.eager alloc mem 611.00 KB 
275053
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :E[638 2022-12-12 06:26:02] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.eager release cuda mem 100400000:275113
638: ] Eeager release cuda mem 625663 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 100400000
[2022-12-12 06:26:02.275172: E[ 2022-12-12 06:26:02/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.:275181638: ] Eeager release cuda mem 100400000 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:26:02.275234: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 100400000
[2022-12-12 06:26:02.275422: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:26:02.275460: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 100400000
[2022-12-12 06:26:02.275757: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] [eager release cuda mem 6256632022-12-12 06:26:02
.275776: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638[] 2022-12-12 06:26:02eager release cuda mem 625663.
275800: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] [eager release cuda mem 1004000002022-12-12 06:26:02
.275827: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:[6382022-12-12 06:26:02] .eager release cuda mem 100400000275844
: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-12 06:26:02.275905: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 100400000
[2022-12-12 06:26:02.276099: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 25000000 / 100000000 nodes ( 25.00 %~25.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 75000000 / 100000000 nodes ( 75.00 %) | 11.92 GB | 2.67176 secs 
[2022-12-12 06:26:02.276599: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 25000000 / 100000000 nodes ( 25.00 %~25.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 75000000 / 100000000 nodes ( 75.00 %) | 11.92 GB | 2.6625 secs 
[2022-12-12 06:26:02.276784: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 25000000 / 100000000 nodes ( 25.00 %~25.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 75000000 / 100000000 nodes ( 75.00 %) | 11.92 GB | 2.66876 secs 
[2022-12-12 06:26:02.276967: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 25000000 / 100000000 nodes ( 25.00 %~25.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 75000000 / 100000000 nodes ( 75.00 %) | 11.92 GB | 2.66286 secs 
[2022-12-12 06:26:02.278022: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 25000000 / 100000000 nodes ( 25.00 %~25.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 75000000 / 100000000 nodes ( 75.00 %) | 11.92 GB | 2.68935 secs 
[2022-12-12 06:26:02.278344: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 25000000 / 100000000 nodes ( 25.00 %~25.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 75000000 / 100000000 nodes ( 75.00 %) | 11.92 GB | 2.67396 secs 
[2022-12-12 06:26:02.278533: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 25000000 / 100000000 nodes ( 25.00 %~25.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 75000000 / 100000000 nodes ( 75.00 %) | 11.92 GB | 2.67443 secs 
[2022-12-12 06:26:02.280207: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1793] Collaborative GPU cache (policy: rep_cache) | local 25000000 / 100000000 nodes ( 25.00 %~25.00 %) | remote 0 / 100000000 nodes ( 0.00 %) | cpu 75000000 / 100000000 nodes ( 75.00 %) | 11.92 GB | 2.67587 secs 
[2022-12-12 06:26:02.280798: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 18.56 GB
[2022-12-12 06:26:03.674911: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 18.82 GB
[2022-12-12 06:26:03.675264: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 18.82 GB
[2022-12-12 06:26:03.676466: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 18.82 GB
[2022-12-12 06:26:05.245588: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 19.08 GB
[2022-12-12 06:26:05.245873: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 19.08 GB
[2022-12-12 06:26:05.246246: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 19.08 GB
[2022-12-12 06:26:06.570391: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 19.30 GB
[2022-12-12 06:26:06.570538: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 19.30 GB
[2022-12-12 06:26:06.570863: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 19.30 GB
[2022-12-12 06:26:08.589024: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 19.51 GB
[2022-12-12 06:26:08.589949: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 19.51 GB
[2022-12-12 06:26:08.590903: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 19.51 GB
[2022-12-12 06:26:10.191256: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 19.97 GB
[2022-12-12 06:26:10.191702: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 19.97 GB
[2022-12-12 06:26:10.192352: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2060] before create ctx, mem is 19.97 GB
[2022-12-12 06:26:11.122377: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2066] after create ctx, mem is 20.17 GB
[2022-12-12 06:26:11.122505: W /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:2073] after create stream, mem is 20.17 GB
[HCTR][06:26:11.275][ERROR][RK0][tid #139917703223040]: replica 0 calling init per replica done, doing barrier
[HCTR][06:26:11.275][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier
[HCTR][06:26:11.275][ERROR][RK0][tid #139918105876224]: replica 6 calling init per replica done, doing barrier
[HCTR][06:26:11.275][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier
[HCTR][06:26:11.275][ERROR][RK0][tid #139918114268928]: replica 2 calling init per replica done, doing barrier
[HCTR][06:26:11.275][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier
[HCTR][06:26:11.275][ERROR][RK0][main]: replica 1 calling init per replica done, doing barrier
[HCTR][06:26:11.275][ERROR][RK0][tid #139918047160064]: replica 3 calling init per replica done, doing barrier
[HCTR][06:26:11.275][ERROR][RK0][tid #139918047160064]: replica 3 calling init per replica done, doing barrier done
[HCTR][06:26:11.275][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier done
[HCTR][06:26:11.275][ERROR][RK0][main]: replica 1 calling init per replica done, doing barrier done
[HCTR][06:26:11.275][ERROR][RK0][tid #139917703223040]: replica 0 calling init per replica done, doing barrier done
[HCTR][06:26:11.275][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier done
[HCTR][06:26:11.275][ERROR][RK0][tid #139918105876224]: replica 6 calling init per replica done, doing barrier done
[HCTR][06:26:11.275][ERROR][RK0][tid #139918114268928]: replica 2 calling init per replica done, doing barrier done
[HCTR][06:26:11.275][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier done
[HCTR][06:26:11.275][ERROR][RK0][tid #139918047160064]: init per replica done
[HCTR][06:26:11.275][ERROR][RK0][main]: init per replica done
[HCTR][06:26:11.275][ERROR][RK0][main]: init per replica done
[HCTR][06:26:11.275][ERROR][RK0][main]: init per replica done
[HCTR][06:26:11.275][ERROR][RK0][tid #139918105876224]: init per replica done
[HCTR][06:26:11.275][ERROR][RK0][tid #139918114268928]: init per replica done
[HCTR][06:26:11.275][ERROR][RK0][main]: init per replica done
[HCTR][06:26:11.278][ERROR][RK0][tid #139917703223040]: init per replica done
[HCTR][06:26:11.282][ERROR][RK0][tid #139918105876224]: 6 allocated 3276800 at 0x7f42f1f20000
[HCTR][06:26:11.282][ERROR][RK0][tid #139917644506880]: 5 allocated 3276800 at 0x7f42f1f20000
[HCTR][06:26:11.282][ERROR][RK0][tid #139918047160064]: 7 allocated 3276800 at 0x7f42f1f20000
[HCTR][06:26:11.282][ERROR][RK0][tid #139918105876224]: 6 allocated 6553600 at 0x7f42f2400000
[HCTR][06:26:11.282][ERROR][RK0][tid #139917644506880]: 5 allocated 6553600 at 0x7f42f2400000
[HCTR][06:26:11.282][ERROR][RK0][tid #139918047160064]: 7 allocated 6553600 at 0x7f42f2400000
[HCTR][06:26:11.282][ERROR][RK0][tid #139918105876224]: 6 allocated 3276800 at 0x7f42f2a40000
[HCTR][06:26:11.282][ERROR][RK0][tid #139917644506880]: 5 allocated 3276800 at 0x7f42f2a40000
[HCTR][06:26:11.282][ERROR][RK0][tid #139918047160064]: 7 allocated 3276800 at 0x7f42f2a40000
[HCTR][06:26:11.282][ERROR][RK0][tid #139918105876224]: 6 allocated 6553600 at 0x7f42f2d60000
[HCTR][06:26:11.282][ERROR][RK0][tid #139917644506880]: 5 allocated 6553600 at 0x7f42f2d60000
[HCTR][06:26:11.282][ERROR][RK0][tid #139918047160064]: 7 allocated 6553600 at 0x7f42f2d60000
[HCTR][06:26:11.282][ERROR][RK0][tid #139918047160064]: 3 allocated 3276800 at 0x7f42f1f20000
[HCTR][06:26:11.282][ERROR][RK0][tid #139918114268928]: 2 allocated 3276800 at 0x7f42f1f20000
[HCTR][06:26:11.282][ERROR][RK0][tid #139918047160064]: 3 allocated 6553600 at 0x7f42f2400000
[HCTR][06:26:11.282][ERROR][RK0][tid #139918114268928]: 2 allocated 6553600 at 0x7f42f2400000
[HCTR][06:26:11.282][ERROR][RK0][tid #139918047160064]: 3 allocated 3276800 at 0x7f42f2a40000
[HCTR][06:26:11.282][ERROR][RK0][tid #139918114268928]: 2 allocated 3276800 at 0x7f42f2a40000
[HCTR][06:26:11.282][ERROR][RK0][tid #139918047160064]: 3 allocated 6553600 at 0x7f42f2d60000
[HCTR][06:26:11.282][ERROR][RK0][tid #139918114268928]: 2 allocated 6553600 at 0x7f42f2d60000
[HCTR][06:26:11.282][ERROR][RK0][tid #139918105876224]: 1 allocated 3276800 at 0x7f42f1f20000
[HCTR][06:26:11.282][ERROR][RK0][tid #139918105876224]: 1 allocated 6553600 at 0x7f42f2400000
[HCTR][06:26:11.282][ERROR][RK0][tid #139918105876224]: 1 allocated 3276800 at 0x7f42f2a40000
[HCTR][06:26:11.282][ERROR][RK0][tid #139918105876224]: 1 allocated 6553600 at 0x7f42f2d60000
[HCTR][06:26:11.283][ERROR][RK0][tid #139917711615744]: 4 allocated 3276800 at 0x7f42f1f20000
[HCTR][06:26:11.283][ERROR][RK0][tid #139917711615744]: 4 allocated 6553600 at 0x7f42f2400000
[HCTR][06:26:11.283][ERROR][RK0][tid #139917711615744]: 4 allocated 3276800 at 0x7f42f2a40000
[HCTR][06:26:11.283][ERROR][RK0][tid #139917711615744]: 4 allocated 6553600 at 0x7f42f2d60000
[HCTR][06:26:11.285][ERROR][RK0][main]: 0 allocated 3276800 at 0x7f42f6b20000
[HCTR][06:26:11.285][ERROR][RK0][main]: 0 allocated 6553600 at 0x7f42f7000000
[HCTR][06:26:11.285][ERROR][RK0][main]: 0 allocated 3276800 at 0x7f24c650e800
[HCTR][06:26:11.285][ERROR][RK0][main]: 0 allocated 6553600 at 0x7f24c682e800








