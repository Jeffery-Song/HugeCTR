2022-12-11 21:25:40.683054: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.688068: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.695858: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.706306: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.713691: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.726172: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.732710: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.737433: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.793712: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.793723: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.795377: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.795661: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.797053: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.797327: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.798824: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.799145: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.800326: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.800807: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.801697: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.802403: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.803055: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.804004: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.804439: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.805529: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.805928: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.807247: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.808177: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.809124: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.810125: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.811017: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.811903: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.812841: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.814777: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.816073: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.817103: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.818127: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.819160: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.820142: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.826391: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.827384: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.830935: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.832647: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.832893: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.833210: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 21:25:40.834588: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.835401: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.837170: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.837668: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.837768: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.839392: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.839930: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.840141: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.841565: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.842429: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.842617: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.843265: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.843669: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.845079: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.845335: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.845831: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.846470: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.848047: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.848494: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.848739: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.851421: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.851916: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.853079: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.853751: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.855387: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.856912: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.857086: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.857446: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.858491: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.859899: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.859981: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.860773: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.862963: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.863071: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.863898: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.864689: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.865436: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.865810: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.866544: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.867493: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.867992: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.868599: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.869315: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.870323: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.870585: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.871444: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.872158: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.873201: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.873246: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.874195: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.874957: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.875789: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.875882: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.877511: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.877826: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.878146: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.882298: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.882558: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.883706: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.883930: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.885399: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.893197: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.898781: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.905034: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.919912: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.921816: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.921850: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.921928: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.921996: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.923035: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.924977: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.925662: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.925707: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.925849: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.925940: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.926740: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.929375: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.930039: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.930172: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.930268: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.931271: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.932000: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.933860: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.934892: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.934942: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.935145: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.935302: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.935828: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.938240: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.938903: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.939087: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.939239: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.939592: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.940007: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.942557: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.943289: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.943535: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.943581: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.944083: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.944410: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.946450: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.947206: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.947695: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.947761: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.948303: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.948567: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.950630: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.951118: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.951553: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.951644: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.952265: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.952364: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.954456: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.955050: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.955411: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.955620: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.956108: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.956264: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.958762: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.959764: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.960051: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.960692: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.961057: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.961069: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.963564: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.963613: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.964512: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.964891: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.965199: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.965547: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.965559: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.968774: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.969501: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.970081: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.970434: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.970600: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.970996: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.973196: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.973366: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.974128: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.974409: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.974549: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.975048: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.975322: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 21:25:40.977263: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.977530: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.978106: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.978243: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.978449: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.979120: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.981320: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.981535: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.982043: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.982253: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.982437: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.983086: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.984866: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.985218: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.985957: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.987254: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.987830: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.987914: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.989856: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.990453: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.990709: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.990958: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.992072: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.992527: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.992763: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.994715: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.995322: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.995522: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.996799: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.997092: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.997231: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.997306: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.999059: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:40.999684: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:41.001956: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:41.002318: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:41.002369: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:41.002431: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:41.004125: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:41.004635: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:41.006574: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:41.006918: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:41.006993: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:41.007036: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:41.009698: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:41.010412: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:41.012765: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:41.013048: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:41.013111: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:41.013223: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:41.015006: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:41.015192: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:41.017553: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:41.017903: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:41.017935: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:41.018463: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:41.019625: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:41.019633: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:41.021952: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:41.022619: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:41.022638: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:41.023319: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:41.024602: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:41.024657: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:41.027105: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:41.028486: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:41.030799: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 21:25:41.030800: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 21:25:41.030921: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:41.031338: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 21:25:41.031634: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 21:25:41.032063: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:41.035454: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:41.036939: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 21:25:41.037569: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:41.040947: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:41.041130: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:41.041506: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:41.041574: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:41.046740: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:41.047675: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:41.047953: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:41.048007: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:41.048032: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:41.048038: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:41.052725: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:41.054046: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:41.054081: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:41.054222: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:41.054267: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:41.054315: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:41.085840: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:41.087827: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:41.092785: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:41.126734: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:41.132488: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:41.143395: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-11 21:25:41.152993: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:41.157300: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:41.162180: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:42.179715: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:42.180660: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:42.181580: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:42.182065: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 21:25:42.182121: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-11 21:25:42.201326: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:42.202166: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:42.202845: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:42.203464: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:42.203994: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:42.204472: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:09.0, compute capability: 7.0
2022-12-11 21:25:42.251751: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:25:42.251961: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:25:42.299769: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12340
2022-12-11 21:25:42.420991: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:42.421787: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:42.422484: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:42.423082: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 21:25:42.423167: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-11 21:25:42.443499: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:42.444136: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:42.444637: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:42.445210: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:42.445738: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:42.446210: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:1/device:GPU:0 with 30914 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0a.0, compute capability: 7.0
2022-12-11 21:25:42.479111: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:42.479990: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:42.480884: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:42.481759: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 21:25:42.481811: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-11 21:25:42.498741: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:42.499516: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:42.500645: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:42.501225: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:42.501869: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:42.502344: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:2/device:GPU:0 with 30914 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0b.0, compute capability: 7.0
2022-12-11 21:25:42.504991: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:42.505586: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:42.505908: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:42.506022: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:42.506424: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:42.507473: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:42.507537: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:42.507763: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 21:25:42.507817: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-11 21:25:42.508794: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:42.508887: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:42.509646: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 21:25:42.509693: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-11 21:25:42.509766: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 21:25:42.509811: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-11 21:25:42.520487: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:25:42.520690: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:25:42.522482: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12341
2022-12-11 21:25:42.524099: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:42.524710: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:42.525219: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:42.526034: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:42.526552: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:42.526837: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:42.527045: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:42.527288: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:7/device:GPU:0 with 30914 MB memory:  -> device: 7, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:10.0, compute capability: 7.0
2022-12-11 21:25:42.527471: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:42.528231: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:42.528524: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:42.528919: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:42.529621: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:42.529931: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:42.530355: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:42.531176: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:42.531283: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 21:25:42.531329: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-11 21:25:42.531893: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:42.532349: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:42.532803: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:42.533212: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:4/device:GPU:0 with 30914 MB memory:  -> device: 4, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0d.0, compute capability: 7.0
2022-12-11 21:25:42.533488: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:5/device:GPU:0 with 30914 MB memory:  -> device: 5, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0e.0, compute capability: 7.0
2022-12-11 21:25:42.539580: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:42.540200: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:42.540721: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:42.541197: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2022-12-11 21:25:42.541248: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-11 21:25:42.548354: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:42.549021: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:42.549531: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:42.550118: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:42.550632: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:42.551110: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:6/device:GPU:0 with 30914 MB memory:  -> device: 6, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0f.0, compute capability: 7.0
2022-12-11 21:25:42.559156: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:42.559882: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:42.560464: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:42.561064: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:42.561576: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-12-11 21:25:42.562050: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:worker/replica:0/task:3/device:GPU:0 with 30914 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:0c.0, compute capability: 7.0
2022-12-11 21:25:42.570585: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:25:42.570766: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:25:42.572531: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:25:42.572646: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12342
2022-12-11 21:25:42.572709: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:25:42.574375: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12347
2022-12-11 21:25:42.578297: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:25:42.578465: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:25:42.579369: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:25:42.579535: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:25:42.580337: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12344
2022-12-11 21:25:42.581355: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12345
2022-12-11 21:25:42.596926: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:25:42.597117: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:25:42.598958: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12346
2022-12-11 21:25:42.612239: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:25:42.612429: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12340, 1 -> localhost:12341, 2 -> localhost:12342, 3 -> localhost:12343, 4 -> localhost:12344, 5 -> localhost:12345, 6 -> localhost:12346, 7 -> localhost:12347}
2022-12-11 21:25:42.614443: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://localhost:12343
[HCTR][21:25:43.887][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][21:25:43.887][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][21:25:43.887][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][21:25:43.887][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][21:25:43.887][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][21:25:43.887][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][21:25:43.895][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
[HCTR][21:25:43.895][ERROR][RK0][main]: using mock embedding with 100000000 * 128 elements
warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 0it [00:00, ?it/s]warmup run: 1it [00:01,  1.56s/it]warmup run: 97it [00:01, 80.91it/s]warmup run: 1it [00:01,  1.53s/it]warmup run: 1it [00:01,  1.54s/it]warmup run: 195it [00:01, 176.83it/s]warmup run: 1it [00:01,  1.52s/it]warmup run: 1it [00:01,  1.54s/it]warmup run: 100it [00:01, 85.34it/s]warmup run: 1it [00:01,  1.56s/it]warmup run: 1it [00:01,  1.57s/it]warmup run: 97it [00:01, 81.97it/s]warmup run: 292it [00:01, 281.43it/s]warmup run: 100it [00:01, 85.81it/s]warmup run: 99it [00:01, 83.96it/s]warmup run: 200it [00:01, 184.88it/s]warmup run: 93it [00:01, 77.97it/s]warmup run: 103it [00:01, 85.83it/s]warmup run: 193it [00:01, 176.64it/s]warmup run: 389it [00:01, 390.58it/s]warmup run: 200it [00:01, 185.89it/s]warmup run: 200it [00:01, 184.13it/s]warmup run: 1it [00:01,  1.52s/it]warmup run: 300it [00:01, 294.54it/s]warmup run: 186it [00:01, 169.24it/s]warmup run: 206it [00:01, 186.48it/s]warmup run: 289it [00:01, 281.06it/s]warmup run: 488it [00:02, 500.62it/s]warmup run: 300it [00:01, 295.91it/s]warmup run: 300it [00:01, 293.31it/s]warmup run: 93it [00:01, 79.82it/s]warmup run: 400it [00:01, 408.03it/s]warmup run: 279it [00:01, 270.11it/s]warmup run: 308it [00:01, 296.44it/s]warmup run: 385it [00:01, 389.25it/s]warmup run: 588it [00:02, 604.56it/s]warmup run: 399it [00:01, 408.17it/s]warmup run: 397it [00:01, 401.95it/s]warmup run: 186it [00:01, 172.80it/s]warmup run: 498it [00:02, 514.17it/s]warmup run: 379it [00:01, 386.31it/s]warmup run: 408it [00:01, 407.79it/s]warmup run: 480it [00:02, 493.16it/s]warmup run: 688it [00:02, 694.66it/s]warmup run: 498it [00:02, 516.91it/s]warmup run: 491it [00:02, 502.28it/s]warmup run: 279it [00:01, 275.10it/s]warmup run: 598it [00:02, 615.45it/s]warmup run: 476it [00:02, 493.80it/s]warmup run: 509it [00:02, 518.29it/s]warmup run: 574it [00:02, 586.60it/s]warmup run: 787it [00:02, 766.70it/s]warmup run: 600it [00:02, 622.10it/s]warmup run: 593it [00:02, 610.90it/s]warmup run: 371it [00:01, 378.81it/s]warmup run: 696it [00:02, 699.52it/s]warmup run: 570it [00:02, 586.51it/s]warmup run: 611it [00:02, 621.63it/s]warmup run: 666it [00:02, 662.29it/s]warmup run: 884it [00:02, 818.35it/s]warmup run: 701it [00:02, 711.11it/s]warmup run: 694it [00:02, 702.06it/s]warmup run: 464it [00:02, 481.31it/s]warmup run: 795it [00:02, 771.01it/s]warmup run: 672it [00:02, 686.06it/s]warmup run: 713it [00:02, 712.88it/s]warmup run: 758it [00:02, 724.49it/s]warmup run: 982it [00:02, 860.27it/s]warmup run: 802it [00:02, 785.10it/s]warmup run: 793it [00:02, 773.01it/s]warmup run: 558it [00:02, 577.14it/s]warmup run: 892it [00:02, 818.51it/s]warmup run: 775it [00:02, 770.41it/s]warmup run: 816it [00:02, 790.54it/s]warmup run: 851it [00:02, 777.15it/s]warmup run: 1082it [00:02, 896.77it/s]warmup run: 901it [00:02, 836.42it/s]warmup run: 892it [00:02, 828.05it/s]warmup run: 653it [00:02, 662.28it/s]warmup run: 989it [00:02, 857.95it/s]warmup run: 877it [00:02, 833.47it/s]warmup run: 917it [00:02, 845.82it/s]warmup run: 944it [00:02, 816.75it/s]warmup run: 1183it [00:02, 927.57it/s]warmup run: 990it [00:02, 863.15it/s]warmup run: 1000it [00:02, 867.13it/s]warmup run: 748it [00:02, 732.46it/s]warmup run: 1086it [00:02, 888.56it/s]warmup run: 979it [00:02, 882.08it/s]warmup run: 1018it [00:02, 888.25it/s]warmup run: 1036it [00:02, 845.41it/s]warmup run: 1284it [00:02, 949.42it/s]warmup run: 1088it [00:02, 895.24it/s]warmup run: 842it [00:02, 786.10it/s]warmup run: 1098it [00:02, 888.06it/s]warmup run: 1186it [00:02, 918.14it/s]warmup run: 1083it [00:02, 925.47it/s]warmup run: 1119it [00:02, 920.26it/s]warmup run: 1128it [00:02, 866.28it/s]warmup run: 1386it [00:02, 967.97it/s]warmup run: 1187it [00:02, 919.60it/s]warmup run: 937it [00:02, 828.51it/s]warmup run: 1195it [00:02, 903.43it/s]warmup run: 1288it [00:02, 945.54it/s]warmup run: 1187it [00:02, 956.36it/s]warmup run: 1222it [00:02, 949.52it/s]warmup run: 1221it [00:02, 882.46it/s]warmup run: 1487it [00:03, 977.94it/s]warmup run: 1285it [00:02, 935.42it/s]warmup run: 1031it [00:02, 858.51it/s]warmup run: 1291it [00:02, 918.61it/s]warmup run: 1387it [00:02, 951.04it/s]warmup run: 1290it [00:02, 975.33it/s]warmup run: 1325it [00:02, 970.19it/s]warmup run: 1314it [00:02, 894.76it/s]warmup run: 1587it [00:03, 979.92it/s]warmup run: 1383it [00:02, 944.23it/s]warmup run: 1125it [00:02, 875.15it/s]warmup run: 1389it [00:02, 934.79it/s]warmup run: 1395it [00:02, 994.51it/s]warmup run: 1427it [00:02, 984.41it/s]warmup run: 1485it [00:03, 875.19it/s]warmup run: 1407it [00:03, 901.60it/s]warmup run: 1687it [00:03, 980.75it/s]warmup run: 1481it [00:03, 953.83it/s]warmup run: 1220it [00:02, 894.22it/s]warmup run: 1489it [00:03, 951.04it/s]warmup run: 1498it [00:03, 998.15it/s]warmup run: 1529it [00:03, 992.63it/s]warmup run: 1576it [00:03, 878.95it/s]warmup run: 1500it [00:03, 909.60it/s]warmup run: 1787it [00:03, 985.02it/s]warmup run: 1581it [00:03, 966.61it/s]warmup run: 1316it [00:02, 910.79it/s]warmup run: 1590it [00:03, 967.92it/s]warmup run: 1600it [00:03, 981.66it/s]warmup run: 1631it [00:03, 994.55it/s]warmup run: 1675it [00:03, 910.03it/s]warmup run: 1593it [00:03, 913.66it/s]warmup run: 1889it [00:03, 993.60it/s]warmup run: 1680it [00:03, 965.83it/s]warmup run: 1418it [00:03, 940.27it/s]warmup run: 1693it [00:03, 984.20it/s]warmup run: 1700it [00:03, 971.01it/s]warmup run: 1732it [00:03, 991.50it/s]warmup run: 1777it [00:03, 939.05it/s]warmup run: 1686it [00:03, 916.37it/s]warmup run: 1989it [00:03, 992.68it/s]warmup run: 1778it [00:03, 965.57it/s]warmup run: 1518it [00:03, 955.75it/s]warmup run: 1796it [00:03, 996.23it/s]warmup run: 1799it [00:03, 959.26it/s]warmup run: 1833it [00:03, 994.07it/s]warmup run: 1874it [00:03, 946.32it/s]warmup run: 1783it [00:03, 930.27it/s]warmup run: 2106it [00:03, 1043.30it/s]warmup run: 1878it [00:03, 975.08it/s]warmup run: 1617it [00:03, 964.48it/s]warmup run: 1900it [00:03, 1007.95it/s]warmup run: 1896it [00:03, 954.09it/s]warmup run: 1971it [00:03, 950.57it/s]warmup run: 1934it [00:03, 990.46it/s]warmup run: 1882it [00:03, 946.97it/s]warmup run: 2224it [00:03, 1083.84it/s]warmup run: 1977it [00:03, 977.19it/s]warmup run: 2003it [00:03, 1014.06it/s]warmup run: 1715it [00:03, 959.91it/s]warmup run: 1992it [00:03, 946.50it/s]warmup run: 2083it [00:03, 999.86it/s]warmup run: 2038it [00:03, 1003.46it/s]warmup run: 1982it [00:03, 961.18it/s]warmup run: 2343it [00:03, 1114.04it/s]warmup run: 2093it [00:03, 1031.20it/s]warmup run: 2124it [00:03, 1070.69it/s]warmup run: 1815it [00:03, 971.70it/s]warmup run: 2105it [00:03, 999.56it/s]warmup run: 2203it [00:03, 1058.39it/s]warmup run: 2157it [00:03, 1056.49it/s]warmup run: 2096it [00:03, 1013.09it/s]warmup run: 2463it [00:03, 1137.54it/s]warmup run: 2213it [00:03, 1081.04it/s]warmup run: 2245it [00:03, 1110.06it/s]warmup run: 1917it [00:03, 985.89it/s]warmup run: 2221it [00:03, 1046.43it/s]warmup run: 2323it [00:03, 1099.96it/s]warmup run: 2276it [00:03, 1094.53it/s]warmup run: 2214it [00:03, 1060.33it/s]warmup run: 2583it [00:04, 1154.58it/s]warmup run: 2333it [00:03, 1113.85it/s]warmup run: 2365it [00:03, 1135.68it/s]warmup run: 2022it [00:03, 1003.05it/s]warmup run: 2337it [00:03, 1079.42it/s]warmup run: 2444it [00:03, 1130.00it/s]warmup run: 2395it [00:03, 1120.38it/s]warmup run: 2332it [00:03, 1094.04it/s]warmup run: 2701it [00:04, 1162.07it/s]warmup run: 2452it [00:03, 1135.61it/s]warmup run: 2485it [00:03, 1153.23it/s]warmup run: 2141it [00:03, 1058.65it/s]warmup run: 2453it [00:03, 1103.23it/s]warmup run: 2564it [00:04, 1150.64it/s]warmup run: 2514it [00:03, 1138.42it/s]warmup run: 2449it [00:04, 1114.86it/s]warmup run: 2821it [00:04, 1171.03it/s]warmup run: 2571it [00:04, 1150.42it/s]warmup run: 2605it [00:04, 1166.01it/s]warmup run: 2260it [00:03, 1096.91it/s]warmup run: 2570it [00:04, 1120.68it/s]warmup run: 2683it [00:04, 1161.06it/s]warmup run: 2632it [00:04, 1148.51it/s]warmup run: 2565it [00:04, 1127.24it/s]warmup run: 2941it [00:04, 1177.63it/s]warmup run: 2689it [00:04, 1157.65it/s]warmup run: 2724it [00:04, 1171.37it/s]warmup run: 2379it [00:03, 1122.37it/s]warmup run: 3000it [00:04, 675.92it/s] warmup run: 2684it [00:04, 1126.15it/s]warmup run: 2803it [00:04, 1171.65it/s]warmup run: 2751it [00:04, 1158.48it/s]warmup run: 2679it [00:04, 1130.46it/s]warmup run: 2808it [00:04, 1167.08it/s]warmup run: 2844it [00:04, 1179.85it/s]warmup run: 2498it [00:04, 1141.23it/s]warmup run: 2800it [00:04, 1135.80it/s]warmup run: 2923it [00:04, 1178.96it/s]warmup run: 2870it [00:04, 1165.11it/s]warmup run: 2793it [00:04, 1126.40it/s]warmup run: 2928it [00:04, 1176.26it/s]warmup run: 2965it [00:04, 1188.09it/s]warmup run: 2617it [00:04, 1154.50it/s]warmup run: 3000it [00:04, 674.50it/s] warmup run: 3000it [00:04, 685.49it/s] warmup run: 2916it [00:04, 1140.89it/s]warmup run: 2989it [00:04, 1170.26it/s]warmup run: 3000it [00:04, 678.69it/s] warmup run: 3000it [00:04, 681.05it/s] warmup run: 2909it [00:04, 1134.74it/s]warmup run: 2733it [00:04, 1154.94it/s]warmup run: 3000it [00:04, 671.04it/s] warmup run: 3000it [00:04, 660.39it/s] warmup run: 2850it [00:04, 1158.97it/s]warmup run: 2969it [00:04, 1167.09it/s]warmup run: 3000it [00:04, 671.25it/s] 

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]




warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   5%|         | 163/3000 [00:00<00:01, 1628.70it/s]warmup should be done:   5%|         | 162/3000 [00:00<00:01, 1615.55it/s]warmup should be done:   5%|         | 152/3000 [00:00<00:01, 1518.03it/s]warmup should be done:   6%|         | 166/3000 [00:00<00:01, 1655.45it/s]warmup should be done:   5%|         | 161/3000 [00:00<00:01, 1605.69it/s]warmup should be done:   5%|         | 162/3000 [00:00<00:01, 1614.94it/s]warmup should be done:   5%|         | 163/3000 [00:00<00:01, 1623.81it/s]warmup should be done:   5%|         | 162/3000 [00:00<00:01, 1610.53it/s]warmup should be done:  11%|         | 324/3000 [00:00<00:01, 1619.78it/s]warmup should be done:  10%|         | 305/3000 [00:00<00:01, 1523.90it/s]warmup should be done:  11%|         | 332/3000 [00:00<00:01, 1657.57it/s]warmup should be done:  11%|         | 327/3000 [00:00<00:01, 1630.11it/s]warmup should be done:  11%|         | 324/3000 [00:00<00:01, 1612.33it/s]warmup should be done:  11%|         | 326/3000 [00:00<00:01, 1625.38it/s]warmup should be done:  11%|         | 324/3000 [00:00<00:01, 1613.48it/s]warmup should be done:  11%|         | 327/3000 [00:00<00:01, 1629.54it/s]warmup should be done:  16%|        | 486/3000 [00:00<00:01, 1619.58it/s]warmup should be done:  17%|        | 498/3000 [00:00<00:01, 1656.03it/s]warmup should be done:  15%|        | 458/3000 [00:00<00:01, 1523.81it/s]warmup should be done:  16%|        | 490/3000 [00:00<00:01, 1628.61it/s]warmup should be done:  16%|        | 491/3000 [00:00<00:01, 1627.84it/s]warmup should be done:  16%|        | 486/3000 [00:00<00:01, 1609.23it/s]warmup should be done:  16%|        | 489/3000 [00:00<00:01, 1617.07it/s]warmup should be done:  16%|        | 486/3000 [00:00<00:01, 1604.89it/s]warmup should be done:  22%|       | 648/3000 [00:00<00:01, 1619.30it/s]warmup should be done:  22%|       | 664/3000 [00:00<00:01, 1655.23it/s]warmup should be done:  22%|       | 653/3000 [00:00<00:01, 1627.08it/s]warmup should be done:  20%|        | 611/3000 [00:00<00:01, 1518.84it/s]warmup should be done:  22%|       | 654/3000 [00:00<00:01, 1625.76it/s]warmup should be done:  22%|       | 647/3000 [00:00<00:01, 1606.47it/s]warmup should be done:  22%|       | 652/3000 [00:00<00:01, 1620.20it/s]warmup should be done:  22%|       | 647/3000 [00:00<00:01, 1602.15it/s]warmup should be done:  27%|       | 811/3000 [00:00<00:01, 1620.00it/s]warmup should be done:  27%|       | 816/3000 [00:00<00:01, 1624.84it/s]warmup should be done:  25%|       | 763/3000 [00:00<00:01, 1518.81it/s]warmup should be done:  28%|       | 830/3000 [00:00<00:01, 1649.11it/s]warmup should be done:  27%|       | 817/3000 [00:00<00:01, 1623.11it/s]warmup should be done:  27%|       | 815/3000 [00:00<00:01, 1621.37it/s]warmup should be done:  27%|       | 808/3000 [00:00<00:01, 1603.93it/s]warmup should be done:  27%|       | 809/3000 [00:00<00:01, 1607.80it/s]warmup should be done:  30%|       | 915/3000 [00:00<00:01, 1516.82it/s]warmup should be done:  32%|      | 974/3000 [00:00<00:01, 1614.35it/s]warmup should be done:  33%|      | 979/3000 [00:00<00:01, 1619.47it/s]warmup should be done:  32%|      | 970/3000 [00:00<00:01, 1607.16it/s]warmup should be done:  33%|      | 978/3000 [00:00<00:01, 1617.91it/s]warmup should be done:  33%|      | 980/3000 [00:00<00:01, 1616.90it/s]warmup should be done:  32%|      | 969/3000 [00:00<00:01, 1597.21it/s]warmup should be done:  33%|      | 995/3000 [00:00<00:01, 1635.21it/s]warmup should be done:  38%|      | 1136/3000 [00:00<00:01, 1613.81it/s]warmup should be done:  38%|      | 1141/3000 [00:00<00:01, 1619.43it/s]warmup should be done:  38%|      | 1140/3000 [00:00<00:01, 1616.70it/s]warmup should be done:  38%|      | 1142/3000 [00:00<00:01, 1616.98it/s]warmup should be done:  38%|      | 1131/3000 [00:00<00:01, 1601.60it/s]warmup should be done:  38%|      | 1131/3000 [00:00<00:01, 1597.08it/s]warmup should be done:  39%|      | 1159/3000 [00:00<00:01, 1630.17it/s]warmup should be done:  36%|      | 1067/3000 [00:00<00:01, 1488.21it/s]warmup should be done:  43%|     | 1298/3000 [00:00<00:01, 1613.64it/s]warmup should be done:  43%|     | 1304/3000 [00:00<00:01, 1621.73it/s]warmup should be done:  43%|     | 1304/3000 [00:00<00:01, 1616.04it/s]warmup should be done:  43%|     | 1292/3000 [00:00<00:01, 1604.03it/s]warmup should be done:  43%|     | 1302/3000 [00:00<00:01, 1611.66it/s]warmup should be done:  43%|     | 1291/3000 [00:00<00:01, 1596.79it/s]warmup should be done:  44%|     | 1323/3000 [00:00<00:01, 1623.39it/s]warmup should be done:  41%|      | 1221/3000 [00:00<00:01, 1502.11it/s]warmup should be done:  49%|     | 1460/3000 [00:00<00:00, 1613.47it/s]warmup should be done:  49%|     | 1469/3000 [00:00<00:00, 1627.74it/s]warmup should be done:  49%|     | 1466/3000 [00:00<00:00, 1616.00it/s]warmup should be done:  48%|     | 1453/3000 [00:00<00:00, 1605.03it/s]warmup should be done:  48%|     | 1452/3000 [00:00<00:00, 1599.16it/s]warmup should be done:  49%|     | 1464/3000 [00:00<00:00, 1611.38it/s]warmup should be done:  50%|     | 1486/3000 [00:00<00:00, 1620.13it/s]warmup should be done:  46%|     | 1375/3000 [00:00<00:01, 1512.10it/s]warmup should be done:  54%|    | 1634/3000 [00:01<00:00, 1633.90it/s]warmup should be done:  54%|    | 1622/3000 [00:01<00:00, 1612.68it/s]warmup should be done:  54%|    | 1628/3000 [00:01<00:00, 1615.74it/s]warmup should be done:  54%|    | 1614/3000 [00:01<00:00, 1605.35it/s]warmup should be done:  54%|    | 1612/3000 [00:01<00:00, 1598.99it/s]warmup should be done:  54%|    | 1626/3000 [00:01<00:00, 1607.88it/s]warmup should be done:  51%|     | 1528/3000 [00:01<00:00, 1517.36it/s]warmup should be done:  55%|    | 1649/3000 [00:01<00:00, 1619.39it/s]warmup should be done:  60%|    | 1799/3000 [00:01<00:00, 1638.05it/s]warmup should be done:  59%|    | 1784/3000 [00:01<00:00, 1612.72it/s]warmup should be done:  60%|    | 1790/3000 [00:01<00:00, 1616.64it/s]warmup should be done:  59%|    | 1776/3000 [00:01<00:00, 1606.82it/s]warmup should be done:  59%|    | 1773/3000 [00:01<00:00, 1601.68it/s]warmup should be done:  60%|    | 1787/3000 [00:01<00:00, 1605.90it/s]warmup should be done:  56%|    | 1682/3000 [00:01<00:00, 1522.03it/s]warmup should be done:  60%|    | 1811/3000 [00:01<00:00, 1618.18it/s]warmup should be done:  65%|   | 1964/3000 [00:01<00:00, 1639.72it/s]warmup should be done:  65%|   | 1946/3000 [00:01<00:00, 1612.26it/s]warmup should be done:  65%|   | 1955/3000 [00:01<00:00, 1626.57it/s]warmup should be done:  65%|   | 1938/3000 [00:01<00:00, 1608.22it/s]warmup should be done:  64%|   | 1935/3000 [00:01<00:00, 1604.84it/s]warmup should be done:  65%|   | 1948/3000 [00:01<00:00, 1604.84it/s]warmup should be done:  61%|    | 1835/3000 [00:01<00:00, 1524.23it/s]warmup should be done:  66%|   | 1973/3000 [00:01<00:00, 1615.97it/s]warmup should be done:  71%|   | 2129/3000 [00:01<00:00, 1640.41it/s]warmup should be done:  70%|   | 2108/3000 [00:01<00:00, 1612.58it/s]warmup should be done:  71%|   | 2121/3000 [00:01<00:00, 1633.81it/s]warmup should be done:  70%|   | 2100/3000 [00:01<00:00, 1611.31it/s]warmup should be done:  70%|   | 2097/3000 [00:01<00:00, 1606.76it/s]warmup should be done:  70%|   | 2109/3000 [00:01<00:00, 1599.12it/s]warmup should be done:  66%|   | 1989/3000 [00:01<00:00, 1525.90it/s]warmup should be done:  71%|   | 2135/3000 [00:01<00:00, 1614.35it/s]warmup should be done:  76%|  | 2294/3000 [00:01<00:00, 1639.37it/s]warmup should be done:  76%|  | 2285/3000 [00:01<00:00, 1634.25it/s]warmup should be done:  75%|  | 2262/3000 [00:01<00:00, 1613.41it/s]warmup should be done:  75%|  | 2258/3000 [00:01<00:00, 1607.44it/s]warmup should be done:  76%|  | 2269/3000 [00:01<00:00, 1597.78it/s]warmup should be done:  71%|  | 2142/3000 [00:01<00:00, 1526.80it/s]warmup should be done:  76%|  | 2270/3000 [00:01<00:00, 1588.84it/s]warmup should be done:  77%|  | 2297/3000 [00:01<00:00, 1612.06it/s]warmup should be done:  82%| | 2459/3000 [00:01<00:00, 1640.91it/s]warmup should be done:  82%| | 2450/3000 [00:01<00:00, 1636.85it/s]warmup should be done:  81%|  | 2425/3000 [00:01<00:00, 1618.07it/s]warmup should be done:  81%|  | 2419/3000 [00:01<00:00, 1605.99it/s]warmup should be done:  81%|  | 2430/3000 [00:01<00:00, 1599.89it/s]warmup should be done:  76%|  | 2295/3000 [00:01<00:00, 1525.36it/s]warmup should be done:  81%|  | 2432/3000 [00:01<00:00, 1597.23it/s]warmup should be done:  82%| | 2459/3000 [00:01<00:00, 1611.05it/s]warmup should be done:  87%| | 2624/3000 [00:01<00:00, 1642.97it/s]warmup should be done:  87%| | 2615/3000 [00:01<00:00, 1640.11it/s]warmup should be done:  86%| | 2589/3000 [00:01<00:00, 1621.78it/s]warmup should be done:  86%| | 2580/3000 [00:01<00:00, 1606.97it/s]warmup should be done:  86%| | 2593/3000 [00:01<00:00, 1605.93it/s]warmup should be done:  82%| | 2449/3000 [00:01<00:00, 1527.67it/s]warmup should be done:  86%| | 2594/3000 [00:01<00:00, 1602.19it/s]warmup should be done:  87%| | 2621/3000 [00:01<00:00, 1612.41it/s]warmup should be done:  93%|| 2789/3000 [00:01<00:00, 1644.28it/s]warmup should be done:  93%|| 2780/3000 [00:01<00:00, 1640.39it/s]warmup should be done:  92%|| 2752/3000 [00:01<00:00, 1623.18it/s]warmup should be done:  91%|| 2742/3000 [00:01<00:00, 1608.57it/s]warmup should be done:  92%|| 2755/3000 [00:01<00:00, 1609.96it/s]warmup should be done:  92%|| 2756/3000 [00:01<00:00, 1605.73it/s]warmup should be done:  87%| | 2603/3000 [00:01<00:00, 1528.68it/s]warmup should be done:  93%|| 2783/3000 [00:01<00:00, 1612.78it/s]warmup should be done:  99%|| 2956/3000 [00:01<00:00, 1649.98it/s]warmup should be done:  98%|| 2946/3000 [00:01<00:00, 1645.75it/s]warmup should be done:  97%|| 2916/3000 [00:01<00:00, 1627.97it/s]warmup should be done:  97%|| 2905/3000 [00:01<00:00, 1612.18it/s]warmup should be done:  97%|| 2917/3000 [00:01<00:00, 1612.32it/s]warmup should be done:  97%|| 2919/3000 [00:01<00:00, 1612.41it/s]warmup should be done:  92%|| 2757/3000 [00:01<00:00, 1529.22it/s]warmup should be done:  98%|| 2945/3000 [00:01<00:00, 1613.20it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1636.08it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1630.15it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1623.10it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1613.58it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1610.78it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1610.68it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1605.86it/s]warmup should be done:  97%|| 2912/3000 [00:01<00:00, 1533.73it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1522.15it/s]

warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]




warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]
warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   0%|          | 0/3000 [00:00<?, ?it/s]warmup should be done:   6%|         | 165/3000 [00:00<00:01, 1648.26it/s]warmup should be done:   6%|         | 165/3000 [00:00<00:01, 1648.66it/s]warmup should be done:   6%|         | 167/3000 [00:00<00:01, 1667.72it/s]warmup should be done:   6%|         | 168/3000 [00:00<00:01, 1676.89it/s]warmup should be done:   6%|         | 165/3000 [00:00<00:01, 1647.20it/s]warmup should be done:   5%|         | 158/3000 [00:00<00:01, 1571.78it/s]warmup should be done:   5%|         | 164/3000 [00:00<00:01, 1634.73it/s]warmup should be done:   6%|         | 165/3000 [00:00<00:01, 1640.82it/s]warmup should be done:  11%|         | 331/3000 [00:00<00:01, 1654.00it/s]warmup should be done:  11%|         | 337/3000 [00:00<00:01, 1683.88it/s]warmup should be done:  11%|         | 316/3000 [00:00<00:01, 1575.86it/s]warmup should be done:  11%|         | 331/3000 [00:00<00:01, 1651.42it/s]warmup should be done:  11%|         | 336/3000 [00:00<00:01, 1675.44it/s]warmup should be done:  11%|         | 332/3000 [00:00<00:01, 1655.16it/s]warmup should be done:  11%|         | 330/3000 [00:00<00:01, 1645.14it/s]warmup should be done:  11%|         | 330/3000 [00:00<00:01, 1631.50it/s]warmup should be done:  17%|        | 498/3000 [00:00<00:01, 1660.10it/s]warmup should be done:  17%|        | 497/3000 [00:00<00:01, 1655.20it/s]warmup should be done:  16%|        | 474/3000 [00:00<00:01, 1575.31it/s]warmup should be done:  17%|        | 499/3000 [00:00<00:01, 1661.48it/s]warmup should be done:  17%|        | 505/3000 [00:00<00:01, 1680.21it/s]warmup should be done:  17%|        | 506/3000 [00:00<00:01, 1680.41it/s]warmup should be done:  17%|        | 498/3000 [00:00<00:01, 1659.65it/s]warmup should be done:  16%|        | 494/3000 [00:00<00:01, 1631.73it/s]warmup should be done:  22%|       | 663/3000 [00:00<00:01, 1656.04it/s]warmup should be done:  22%|       | 665/3000 [00:00<00:01, 1661.60it/s]warmup should be done:  21%|        | 632/3000 [00:00<00:01, 1573.73it/s]warmup should be done:  22%|       | 674/3000 [00:00<00:01, 1680.76it/s]warmup should be done:  22%|       | 665/3000 [00:00<00:01, 1660.97it/s]warmup should be done:  22%|       | 675/3000 [00:00<00:01, 1678.01it/s]warmup should be done:  22%|       | 666/3000 [00:00<00:01, 1651.76it/s]warmup should be done:  22%|       | 658/3000 [00:00<00:01, 1630.87it/s]warmup should be done:  28%|       | 832/3000 [00:00<00:01, 1661.02it/s]warmup should be done:  28%|       | 829/3000 [00:00<00:01, 1653.14it/s]warmup should be done:  26%|       | 790/3000 [00:00<00:01, 1575.10it/s]warmup should be done:  28%|       | 832/3000 [00:00<00:01, 1661.35it/s]warmup should be done:  28%|       | 843/3000 [00:00<00:01, 1679.85it/s]warmup should be done:  28%|       | 843/3000 [00:00<00:01, 1674.70it/s]warmup should be done:  28%|       | 832/3000 [00:00<00:01, 1654.33it/s]warmup should be done:  27%|       | 822/3000 [00:00<00:01, 1632.47it/s]warmup should be done:  33%|      | 999/3000 [00:00<00:01, 1663.06it/s]warmup should be done:  33%|      | 998/3000 [00:00<00:01, 1662.86it/s]warmup should be done:  32%|      | 949/3000 [00:00<00:01, 1577.11it/s]warmup should be done:  34%|      | 1012/3000 [00:00<00:01, 1682.12it/s]warmup should be done:  34%|      | 1011/3000 [00:00<00:01, 1675.18it/s]warmup should be done:  33%|      | 999/3000 [00:00<00:01, 1658.13it/s]warmup should be done:  33%|      | 999/3000 [00:00<00:01, 1655.60it/s]warmup should be done:  33%|      | 986/3000 [00:00<00:01, 1630.87it/s]warmup should be done:  39%|      | 1167/3000 [00:00<00:01, 1669.73it/s]warmup should be done:  37%|      | 1107/3000 [00:00<00:01, 1577.07it/s]warmup should be done:  39%|      | 1167/3000 [00:00<00:01, 1664.07it/s]warmup should be done:  39%|      | 1181/3000 [00:00<00:01, 1682.70it/s]warmup should be done:  39%|      | 1165/3000 [00:00<00:01, 1658.08it/s]warmup should be done:  39%|      | 1179/3000 [00:00<00:01, 1672.52it/s]warmup should be done:  38%|      | 1150/3000 [00:00<00:01, 1630.73it/s]warmup should be done:  39%|      | 1165/3000 [00:00<00:01, 1649.06it/s]warmup should be done:  44%|     | 1334/3000 [00:00<00:01, 1664.73it/s]warmup should be done:  42%|     | 1265/3000 [00:00<00:01, 1575.90it/s]warmup should be done:  45%|     | 1336/3000 [00:00<00:00, 1673.74it/s]warmup should be done:  45%|     | 1350/3000 [00:00<00:00, 1682.53it/s]warmup should be done:  45%|     | 1347/3000 [00:00<00:00, 1673.64it/s]warmup should be done:  44%|     | 1331/3000 [00:00<00:01, 1653.19it/s]warmup should be done:  44%|     | 1333/3000 [00:00<00:01, 1656.87it/s]warmup should be done:  44%|     | 1314/3000 [00:00<00:01, 1623.03it/s]warmup should be done:  50%|     | 1505/3000 [00:00<00:00, 1676.58it/s]warmup should be done:  47%|     | 1423/3000 [00:00<00:01, 1574.99it/s]warmup should be done:  51%|     | 1519/3000 [00:00<00:00, 1681.46it/s]warmup should be done:  50%|     | 1501/3000 [00:00<00:00, 1657.85it/s]warmup should be done:  50%|     | 1515/3000 [00:00<00:00, 1672.73it/s]warmup should be done:  50%|     | 1500/3000 [00:00<00:00, 1659.62it/s]warmup should be done:  50%|     | 1497/3000 [00:00<00:00, 1644.79it/s]warmup should be done:  49%|     | 1477/3000 [00:00<00:00, 1621.61it/s]warmup should be done:  56%|    | 1674/3000 [00:01<00:00, 1680.40it/s]warmup should be done:  53%|    | 1581/3000 [00:01<00:00, 1574.44it/s]warmup should be done:  56%|    | 1688/3000 [00:01<00:00, 1681.63it/s]warmup should be done:  56%|    | 1667/3000 [00:01<00:00, 1654.87it/s]warmup should be done:  56%|    | 1683/3000 [00:01<00:00, 1670.01it/s]warmup should be done:  56%|    | 1666/3000 [00:01<00:00, 1658.55it/s]warmup should be done:  55%|    | 1662/3000 [00:01<00:00, 1644.95it/s]warmup should be done:  55%|    | 1640/3000 [00:01<00:00, 1621.01it/s]warmup should be done:  61%|   | 1844/3000 [00:01<00:00, 1686.18it/s]warmup should be done:  58%|    | 1739/3000 [00:01<00:00, 1573.32it/s]warmup should be done:  62%|   | 1857/3000 [00:01<00:00, 1682.54it/s]warmup should be done:  61%|    | 1834/3000 [00:01<00:00, 1657.87it/s]warmup should be done:  62%|   | 1851/3000 [00:01<00:00, 1671.80it/s]warmup should be done:  61%|    | 1832/3000 [00:01<00:00, 1656.66it/s]warmup should be done:  61%|    | 1828/3000 [00:01<00:00, 1649.08it/s]warmup should be done:  60%|    | 1803/3000 [00:01<00:00, 1623.29it/s]warmup should be done:  67%|   | 2014/3000 [00:01<00:00, 1687.95it/s]warmup should be done:  63%|   | 1897/3000 [00:01<00:00, 1574.61it/s]warmup should be done:  68%|   | 2026/3000 [00:01<00:00, 1682.69it/s]warmup should be done:  67%|   | 2001/3000 [00:01<00:00, 1659.09it/s]warmup should be done:  67%|   | 2019/3000 [00:01<00:00, 1672.66it/s]warmup should be done:  67%|   | 1998/3000 [00:01<00:00, 1655.40it/s]warmup should be done:  66%|   | 1993/3000 [00:01<00:00, 1648.80it/s]warmup should be done:  66%|   | 1966/3000 [00:01<00:00, 1624.68it/s]warmup should be done:  73%|  | 2183/3000 [00:01<00:00, 1687.99it/s]warmup should be done:  68%|   | 2055/3000 [00:01<00:00, 1575.61it/s]warmup should be done:  73%|  | 2195/3000 [00:01<00:00, 1682.24it/s]warmup should be done:  72%|  | 2167/3000 [00:01<00:00, 1658.97it/s]warmup should be done:  72%|  | 2158/3000 [00:01<00:00, 1648.64it/s]warmup should be done:  73%|  | 2187/3000 [00:01<00:00, 1669.87it/s]warmup should be done:  72%|  | 2164/3000 [00:01<00:00, 1655.70it/s]warmup should be done:  71%|   | 2130/3000 [00:01<00:00, 1628.22it/s]warmup should be done:  78%|  | 2352/3000 [00:01<00:00, 1688.19it/s]warmup should be done:  74%|  | 2213/3000 [00:01<00:00, 1573.19it/s]warmup should be done:  78%|  | 2334/3000 [00:01<00:00, 1660.31it/s]warmup should be done:  79%|  | 2364/3000 [00:01<00:00, 1680.54it/s]warmup should be done:  78%|  | 2354/3000 [00:01<00:00, 1669.45it/s]warmup should be done:  78%|  | 2331/3000 [00:01<00:00, 1657.63it/s]warmup should be done:  77%|  | 2323/3000 [00:01<00:00, 1645.50it/s]warmup should be done:  76%|  | 2294/3000 [00:01<00:00, 1631.41it/s]warmup should be done:  79%|  | 2371/3000 [00:01<00:00, 1572.39it/s]warmup should be done:  84%| | 2521/3000 [00:01<00:00, 1676.03it/s]warmup should be done:  83%| | 2501/3000 [00:01<00:00, 1660.09it/s]warmup should be done:  84%| | 2521/3000 [00:01<00:00, 1669.40it/s]warmup should be done:  84%| | 2533/3000 [00:01<00:00, 1677.90it/s]warmup should be done:  83%| | 2488/3000 [00:01<00:00, 1639.91it/s]warmup should be done:  83%| | 2497/3000 [00:01<00:00, 1647.23it/s]warmup should be done:  82%| | 2458/3000 [00:01<00:00, 1628.17it/s]warmup should be done:  84%| | 2529/3000 [00:01<00:00, 1573.06it/s]warmup should be done:  90%| | 2701/3000 [00:01<00:00, 1678.31it/s]warmup should be done:  90%| | 2689/3000 [00:01<00:00, 1670.63it/s]warmup should be done:  89%| | 2668/3000 [00:01<00:00, 1658.75it/s]warmup should be done:  90%| | 2689/3000 [00:01<00:00, 1668.00it/s]warmup should be done:  88%| | 2653/3000 [00:01<00:00, 1641.43it/s]warmup should be done:  89%| | 2662/3000 [00:01<00:00, 1647.30it/s]warmup should be done:  87%| | 2621/3000 [00:01<00:00, 1604.69it/s]warmup should be done:  90%| | 2687/3000 [00:01<00:00, 1574.03it/s]warmup should be done:  96%|| 2869/3000 [00:01<00:00, 1678.03it/s]warmup should be done:  95%|| 2857/3000 [00:01<00:00, 1672.50it/s]warmup should be done:  94%|| 2834/3000 [00:01<00:00, 1657.12it/s]warmup should be done:  95%|| 2856/3000 [00:01<00:00, 1660.61it/s]warmup should be done:  94%|| 2819/3000 [00:01<00:00, 1644.23it/s]warmup should be done:  94%|| 2828/3000 [00:01<00:00, 1649.13it/s]warmup should be done:  93%|| 2782/3000 [00:01<00:00, 1600.51it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1680.27it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1673.94it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1670.95it/s]warmup should be done:  95%|| 2845/3000 [00:01<00:00, 1573.50it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1659.47it/s]warmup should be done: 100%|| 2985/3000 [00:01<00:00, 1647.80it/s]warmup should be done: 100%|| 2995/3000 [00:01<00:00, 1653.25it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1653.88it/s]warmup should be done:  98%|| 2945/3000 [00:01<00:00, 1608.15it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1648.78it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1620.80it/s]warmup should be done: 100%|| 3000/3000 [00:01<00:00, 1574.45it/s]2022-12-11 21:27:19.541410: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fd9f002e010 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 21:27:19.541468: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 21:27:20.805956: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fd91402a490 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 21:27:20.806025: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 21:27:20.934082: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7ff597832f70 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 21:27:20.934149: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 21:27:21.217157: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fd9bc02a490 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 21:27:21.217221: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 21:27:21.221956: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7ff59f832f90 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 21:27:21.222001: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 21:27:21.315769: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7fda3402d370 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 21:27:21.315834: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 21:27:21.337171: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7ff59b795bc0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 21:27:21.337229: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 21:27:21.337372: I tensorflow/compiler/xla/service/service.cc:170] XLA service 0x7ff59f830c60 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-12-11 21:27:21.337453: I tensorflow/compiler/xla/service/service.cc:178]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-12-11 21:27:21.755761: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 21:27:23.109981: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 21:27:23.165474: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 21:27:23.483855: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 21:27:23.492070: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 21:27:23.606822: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 21:27:23.628015: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 21:27:23.676857: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2022-12-11 21:27:24.578777: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 21:27:25.949221: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 21:27:26.021508: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 21:27:26.350734: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 21:27:26.351354: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 21:27:26.477546: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 21:27:26.492047: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-12-11 21:27:26.540719: I tensorflow/compiler/jit/xla_compilation_cache.cc:481] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[HCTR][21:28:06.620][ERROR][RK0][main]: replica 5 reaches 1000, calling init pre replica
[HCTR][21:28:06.620][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][21:28:06.625][ERROR][RK0][main]: coll ps creation done
[HCTR][21:28:06.625][ERROR][RK0][main]: replica 5 waits for coll ps creation barrier
[HCTR][21:28:06.638][ERROR][RK0][main]: replica 4 reaches 1000, calling init pre replica
[HCTR][21:28:06.638][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][21:28:06.645][ERROR][RK0][main]: coll ps creation done
[HCTR][21:28:06.645][ERROR][RK0][main]: replica 4 waits for coll ps creation barrier
[HCTR][21:28:06.691][ERROR][RK0][main]: replica 0 reaches 1000, calling init pre replica
[HCTR][21:28:06.691][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][21:28:06.699][ERROR][RK0][main]: coll ps creation done
[HCTR][21:28:06.699][ERROR][RK0][main]: replica 0 waits for coll ps creation barrier
[HCTR][21:28:06.709][ERROR][RK0][tid #140693733373696]: replica 1 reaches 1000, calling init pre replica
[HCTR][21:28:06.709][ERROR][RK0][tid #140693733373696]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][21:28:06.714][ERROR][RK0][tid #140693733373696]: coll ps creation done
[HCTR][21:28:06.714][ERROR][RK0][tid #140693733373696]: replica 1 waits for coll ps creation barrier
[HCTR][21:28:06.724][ERROR][RK0][main]: replica 7 reaches 1000, calling init pre replica
[HCTR][21:28:06.724][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][21:28:06.724][ERROR][RK0][tid #140693062285056]: replica 2 reaches 1000, calling init pre replica
[HCTR][21:28:06.725][ERROR][RK0][tid #140693062285056]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][21:28:06.729][ERROR][RK0][tid #140693062285056]: coll ps creation done
[HCTR][21:28:06.729][ERROR][RK0][tid #140693062285056]: replica 2 waits for coll ps creation barrier
[HCTR][21:28:06.729][ERROR][RK0][main]: coll ps creation done
[HCTR][21:28:06.729][ERROR][RK0][main]: replica 7 waits for coll ps creation barrier
[HCTR][21:28:06.740][ERROR][RK0][main]: replica 6 reaches 1000, calling init pre replica
[HCTR][21:28:06.740][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][21:28:06.740][ERROR][RK0][main]: replica 3 reaches 1000, calling init pre replica
[HCTR][21:28:06.740][ERROR][RK0][main]: coll ps creation, with 8 devices, using policy clique_part
[HCTR][21:28:06.744][ERROR][RK0][main]: coll ps creation done
[HCTR][21:28:06.744][ERROR][RK0][main]: replica 6 waits for coll ps creation barrier
[HCTR][21:28:06.745][ERROR][RK0][main]: coll ps creation done
[HCTR][21:28:06.745][ERROR][RK0][main]: replica 3 waits for coll ps creation barrier
[HCTR][21:28:06.745][ERROR][RK0][main]: replica 0 preparing frequency
[HCTR][21:28:07.588][ERROR][RK0][main]: replica 0 preparing frequency done
[HCTR][21:28:07.618][ERROR][RK0][main]: replica 0 calling init per replica
[HCTR][21:28:07.618][ERROR][RK0][main]: replica 6 calling init per replica
[HCTR][21:28:07.618][ERROR][RK0][main]: replica 3 calling init per replica
[HCTR][21:28:07.618][ERROR][RK0][tid #140693062285056]: replica 2 calling init per replica
[HCTR][21:28:07.618][ERROR][RK0][main]: replica 7 calling init per replica
[HCTR][21:28:07.618][ERROR][RK0][main]: replica 4 calling init per replica
[HCTR][21:28:07.618][ERROR][RK0][tid #140693733373696]: replica 1 calling init per replica
[HCTR][21:28:07.618][ERROR][RK0][main]: replica 5 calling init per replica
[HCTR][21:28:07.618][ERROR][RK0][main]: Calling build_v2
[HCTR][21:28:07.618][ERROR][RK0][main]: Calling build_v2
[HCTR][21:28:07.618][ERROR][RK0][main]: Calling build_v2
[HCTR][21:28:07.618][ERROR][RK0][tid #140693062285056]: Calling build_v2
[HCTR][21:28:07.618][ERROR][RK0][tid #140693733373696]: Calling build_v2
[HCTR][21:28:07.618][ERROR][RK0][main]: Calling build_v2
[HCTR][21:28:07.618][ERROR][RK0][main]: Calling build_v2
[HCTR][21:28:07.618][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][21:28:07.618][ERROR][RK0][main]: Calling build_v2
[HCTR][21:28:07.618][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][21:28:07.618][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][21:28:07.618][ERROR][RK0][tid #140693062285056]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][21:28:07.619][ERROR][RK0][tid #140693733373696]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][21:28:07.619][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][21:28:07.619][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[HCTR][21:28:07.619][ERROR][RK0][main]: cudaDevAttrCanUseHostPointerForRegisteredMem is 1
[2022-12-11 21:28:07.623024: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:[178] v100x8, slow pcie
2022-12-11 21:28:07.623067[: 2022-12-11 21:28:07E. 623104/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: [:E178 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccv100x8, slow pcie:2022-12-11 21:28:07
196.] 623108[assigning 0 to cpu: [
E2022-12-11 21:28:07 2022-12-11 21:28:07./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.623176:623159: 178: E] E v100x8, slow pcie /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc::[196178[] ] [2022-12-11 21:28:07assigning 0 to cpuv100x8, slow pcie2022-12-11 21:28:07.

.6232376232462022-12-11 21:28:07[: : .2022-12-11 21:28:07EE623209.[  : 623273/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE: :2022-12-11 21:28:07:[ E196.[2122022-12-11 21:28:07/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc ] 623263] .:/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccassigning 0 to cpu2022-12-11 21:28:07: build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8[623315178:
.E
: ] 1962022-12-11 21:28:07623310 Ev100x8, slow pcie] .: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[ 
assigning 0 to cpu623361E:[2022-12-11 21:28:07/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
:  178[2022-12-11 21:28:07.:E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] 2022-12-11 21:28:07.623439212 :v100x8, slow pcie.623457: ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc178
623474: [Ebuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8:] : E2022-12-11 21:28:07 [
178v100x8, slow pcieE ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 21:28:07] 
 /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc623535[:[.v100x8, slow pcie/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:: 2022-12-11 21:28:072132022-12-11 21:28:07623557
:212E.] .: 196]  [623601remote time is 8.68421623621E] build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 21:28:07: 
:  assigning 0 to cpu
:.EE/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[
212623664[  :2022-12-11 21:28:07] : 2022-12-11 21:28:07/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc196.build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8[E.::] 623732
2022-12-11 21:28:07 623748213196assigning 0 to cpu: ./hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: [] ] 
E623792:E2022-12-11 21:28:07remote time is 8.68421assigning 0 to cpu : 196 .

/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc623850[:[ assigning 0 to cpu:: 2022-12-11 21:28:072142022-12-11 21:28:07/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
213E.] .:]  [623924cpu time is 97.0588623935212remote time is 8.68421/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-11 21:28:07: 
: ] 
:.EEbuild asymm link desc with 8X Tesla V100-SXM2-32GB out of 8[213[623998  
2022-12-11 21:28:07] 2022-12-11 21:28:07: /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.remote time is 8.68421[.E::624046
2022-12-11 21:28:07624054 212214: .: [/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] ] E624100E2022-12-11 21:28:07:build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8cpu time is 97.0588 :  .212

/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc624148] [: :: build asymm link desc with 8X Tesla V100-SXM2-32GB out of 82022-12-11 21:28:07212/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc214E
.] :]  624241[build asymm link desc with 8X Tesla V100-SXM2-32GB out of 8213cpu time is 97.0588/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: 2022-12-11 21:28:07
] 
:E.remote time is 8.68421214[ 624309
] 2022-12-11 21:28:07/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: cpu time is 97.0588.[:E
6243432022-12-11 21:28:07213 : .] /hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE624375remote time is 8.68421: : 
213/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE ] :[/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccremote time is 8.684212132022-12-11 21:28:07:
] .214remote time is 8.68421624461] [
: cpu time is 97.05882022-12-11 21:28:07E[
. 2022-12-11 21:28:07624509/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc.: :624535E214:  ] E/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cccpu time is 97.0588 :
/hugectr_dev/third_party/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc214:] 214cpu time is 97.0588] 
cpu time is 97.0588
[2022-12-11 21:29:26.834307: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-11 21:29:26.874414: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
block 0 storage is 00010001
	access is	0	0	0	0	4	4	4	4	
block 1 storage is 00100010
	access is	1	1	1	1	5	5	5	5	
block 2 storage is 01000100
	access is	2	2	2	2	6	6	6	6	
block 3 storage is 10001000
	access is	3	3	3	3	7	7	7	7	
block 4 storage is 00000000
	access is	8	8	8	8	8	8	8	8	
[2022-12-11 21:29:26.986289: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-11 21:29:26.986351: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-11 21:29:27.126701: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-11 21:29:27.126757: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-11 21:29:27.127236: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 21:29:27.127300: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:29:27.128348: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:29:27.129141: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:29:27.141959: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 3 solved
[2022-12-11 21:29:27.142015: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 3 initing device 3
[2022-12-11 21:29:27.142180: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 5 solved
[2022-12-11 21:29:27.142237: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] [worker 0 thread 5 initing device 52022-12-11 21:29:27
.142244: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] 1 solved
[2022-12-11 21:29:27.142303: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 1 initing device 1
[2022-12-11 21:29:27.142444: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 21:29:27.142491: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:29:27.142662: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 21:29:27.142705: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] [eager alloc mem 381.47 MB2022-12-11 21:29:27
.142722: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 21:29:27.142767: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:29:27.144633: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:202] [2 solved2022-12-11 21:29:27
.[144664[2022-12-11 21:29:27: 2022-12-11 21:29:27.E.144663 144690: /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu: E:E 1980 /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc] /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:eager alloc mem 381.47 MB:[202
2052022-12-11 21:29:27] ] .7 solvedworker 0 thread 2 initing device 2144781

: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:[1980[2022-12-11 21:29:27] 2022-12-11 21:29:27.eager alloc mem 381.47 MB.144850
144847: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::2051980] ] worker 0 thread 7 initing device 7eager alloc mem 381.47 MB

[2022-12-11 21:29:27.145241: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[2022-12-11 21:29:27.145283: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:29:27.145354: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8
[[2022-12-11 21:29:272022-12-11 21:29:27..145393145409: [: E2022-12-11 21:29:27E . /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc145418/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:: :202E1980]  ] 4 solved/hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cceager alloc mem 381.47 MB
:
202] [2022-12-11 21:29:276 solved.
145541: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc[:2022-12-11 21:29:27205.] 145563worker 0 thread 4 initing device 4: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 6 initing device 6
[2022-12-11 21:29:27.145993: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] Building Coll Cache with ... num gpu device is 8[
2022-12-11 21:29:27.146016: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1815] [Building Coll Cache with ... num gpu device is 82022-12-11 21:29:27
.146042: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB[
2022-12-11 21:29:27.146076: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:29:27.148912: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:29:27.149172: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:29:27.149390: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:29:27.149943: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:29:27.149999: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:29:27.150063: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:29:27.150560: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:29:27.153944: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:29:27.153987: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:29:27.154051: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:29:27.154147: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 381.47 MB
[2022-12-11 21:29:27.206915: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 5.00 Bytes
[2022-12-11 21:29:27.214325: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-11 21:29:27.214462: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 21:29:27.215403: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 8.01 MB
[2022-12-11 21:29:27.216146: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:29:27.217134: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:29:27.218880: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 21:29:27.219624: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 21:29:27.219673: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 981.44 MB
[[[[[2022-12-11 21:29:272022-12-11 21:29:272022-12-11 21:29:272022-12-11 21:29:272022-12-11 21:29:27.....233356233341233341233341233341: : : : : EEEEE     /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:::::19801980198019801980] ] ] ] ] eager alloc mem 5.00 Byteseager alloc mem 5.00 Byteseager alloc mem 5.00 Byteseager alloc mem 5.00 Byteseager alloc mem 5.00 Bytes




[[2022-12-11 21:29:272022-12-11 21:29:27..239120239120: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::19801980] ] eager alloc mem 5.00 Byteseager alloc mem 5.00 Bytes

[2022-12-11 21:29:27.239741: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-11 21:29:27.239791: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5[
2022-12-11 21:29:27.239836: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 21:29:27.239880: E[ 2022-12-11 21:29:27/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.:239880638: ] Eeager release cuda mem 400000000 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-11 21:29:27[.2022-12-11 21:29:27239958.: 239977E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc :/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638:] 638eager release cuda mem 5] 
eager release cuda mem 400000000
[[2022-12-11 21:29:272022-12-11 21:29:27..240042240059: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::638638] ] eager release cuda mem 5eager release cuda mem 400000000

[2022-12-11 21:29:27.240150: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 21:29:27.240829: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 8.01 MB
[2022-12-11 21:29:27.241432: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 8.01 MB
[2022-12-11 21:29:27.242346: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 8.01 MB
[2022-12-11 21:29:27.242875: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 8.01 MB
[2022-12-11 21:29:27.243445: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 8.01 MB
[2022-12-11 21:29:27.244681: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:29:27.244769: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:29:27.245221: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:29:27.245268: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:29:27.245322: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:29:27.245654: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:29:27.245725: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:29:27.246188: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:29:27.246226: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:29:27.246288: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:29:27.246402: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 5
[2022-12-11 21:29:27.246485[: 2022-12-11 21:29:27E. 246481/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc: :E638 ] /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cceager release cuda mem 400000000:
638] eager release cuda mem 5
[2022-12-11 21:29:27.246576: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 400000000
[2022-12-11 21:29:27.247435: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 8.01 MB
[2022-12-11 21:29:27.248255: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 8.01 MB
[2022-12-11 21:29:27.251816: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 21:29:27.252500: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu[:2022-12-11 21:29:271980.] 252529eager alloc mem 25.25 KB: 
E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 21:29:27.252584: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 981.44 MB
[2022-12-11 21:29:27.252715: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 21:29:27.253058: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 21:29:27.253176: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 21:29:27.253220: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 21:29:27.253262: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 981.44 MB
[2022-12-11 21:29:27.253314: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 21:29:27.253356: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 981.44 MB
[2022-12-11 21:29:27.253767: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 21:29:27.253811: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 981.44 MB
[2022-12-11 21:29:27.253885: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 21:29:27.253930: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 981.44 MB
[2022-12-11 21:29:27.254210: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:29:27.255176: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:29:27.260045: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:29:27.260945: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:29:27.261770: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 21:29:27.262363: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 21:29:27.262414: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 981.44 MB
[2022-12-11 21:29:27.267617: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 25.25 KB
[2022-12-11 21:29:27.268209: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 25855
[2022-12-11 21:29:27.268250: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 981.44 MB
[[[[[[[[2022-12-11 21:29:272022-12-11 21:29:272022-12-11 21:29:272022-12-11 21:29:272022-12-11 21:29:272022-12-11 21:29:272022-12-11 21:29:272022-12-11 21:29:27........607318607318607318607320607319607318607318607319: : : : : : : : EEEEEEEE        /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu::::::::19261926192619261926192619261926] ] Device 4 init p2p of link 5] ] ] ] ] ] Device 2 init p2p of link 1
Device 5 init p2p of link 6Device 1 init p2p of link 7Device 0 init p2p of link 3Device 3 init p2p of link 2Device 6 init p2p of link 0Device 7 init p2p of link 4






[2022-12-11 21:29:27.607813: E[ 2022-12-11 21:29:27/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.[[:6078252022-12-11 21:29:27[2022-12-11 21:29:27[1980[: .2022-12-11 21:29:27[.2022-12-11 21:29:27] 2022-12-11 21:29:27E607831.2022-12-11 21:29:27607831.eager alloc mem 611.00 KB. : 607837.: 607839
607842/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cuE: 607847E: : : E:  EE1980/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu  ] :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cueager alloc mem 611.00 KB1980:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980::
] 1980:] 19801980eager alloc mem 611.00 KB] 1980eager alloc mem 611.00 KB] ] 
eager alloc mem 611.00 KB] 
eager alloc mem 611.00 KBeager alloc mem 611.00 KB
eager alloc mem 611.00 KB


[2022-12-11 21:29:27.608713: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:29:27.608829: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:29:27.608866[: 2022-12-11 21:29:27E[.[ 2022-12-11 21:29:276088732022-12-11 21:29:27/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc.: [.[:608879E2022-12-11 21:29:276088862022-12-11 21:29:27638:  .: .] E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc608905E608907eager release cuda mem 625663 ::  : 
/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638E/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.ccE:]  : 638eager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc638/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc] 
:] :eager release cuda mem 625663638eager release cuda mem 625663638
] 
] eager release cuda mem 625663eager release cuda mem 625663

[2022-12-11 21:29:27.622163: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 2
[2022-12-11 21:29:27.622296: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:29:27.622329: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 7
[2022-12-11 21:29:27.622479: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:29:27.622508: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 4
[2022-12-11 21:29:27.622604: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 5
[2022-12-11 21:29:27.622655: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:29:27[.2022-12-11 21:29:27622736.: 622752E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1926:] 1980Device 3 init p2p of link 0] 
eager alloc mem 611.00 KB
[2022-12-11 21:29:27.622938: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:29:27.623114: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:29:27.623204: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 3
[2022-12-11 21:29:27.623275: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:29:27.623372: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:29:27.623444: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:29:27.623617: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:29:27.623681: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 1
[2022-12-11 21:29:27.623748: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:29:27.623826: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:29:27.623859: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 6
[2022-12-11 21:29:27.624000: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:29:27.624158: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:29:27.624593: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:29:27.624765: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:29:27.636159: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 3
[2022-12-11 21:29:27.636270: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:29:27.636444: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 2
[2022-12-11 21:29:27.636562: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:29:27.636798: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 5 init p2p of link 7
[2022-12-11 21:29:27.636847: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 6
[2022-12-11 21:29:27.636915: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:29:27.636962: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[[2022-12-11 21:29:272022-12-11 21:29:27..637051637064: : [EE2022-12-11 21:29:27  ./hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc637083::: 1926638E] ]  Device 6 init p2p of link 4eager release cuda mem 625663/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu

:1926] Device 0 init p2p of link 1
[2022-12-11 21:29:27.637262: E[ 2022-12-11 21:29:27[/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu.2022-12-11 21:29:27:637272.1980: 637272] E: eager alloc mem 611.00 KB[ E
2022-12-11 21:29:27/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu .:/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu6373271980:: ] 1926Eeager alloc mem 611.00 KB]  
Device 3 init p2p of link 5/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc
:638] eager release cuda mem 625663
[2022-12-11 21:29:27[.2022-12-11 21:29:27637474.: 637469E:  E/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu :/hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu1980:] 1926eager alloc mem 611.00 KB] 
Device 2 init p2p of link 0
[2022-12-11 21:29:27.637606: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:29:27.637704: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:29:27.637755: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:29:27.638119: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:29:27.638154: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:29:27.638264: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:29:27.638392: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:29:27.651744: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 4 init p2p of link 6
[2022-12-11 21:29:27.651859: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:29:27.652649: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:29:27.652742: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 1 init p2p of link 0
[2022-12-11 21:29:27.652853: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:29:27.653335: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 3 init p2p of link 1
[2022-12-11 21:29:27.653454: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[[2022-12-11 21:29:272022-12-11 21:29:27..653631653645: : EE  /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu/hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc::1926638] ] Device 5 init p2p of link 3eager release cuda mem 625663

[2022-12-11 21:29:27.653775: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:29:27.653977: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 7 init p2p of link 5
[2022-12-11 21:29:27.654091: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:29:27.654258: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:29:27.654308: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 6 init p2p of link 7
[2022-12-11 21:29:27.654430: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:29:27.654543: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:29:27.654611: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 0 init p2p of link 2
[2022-12-11 21:29:27.654726: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:29:27.654876: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:29:27.655215: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:29:27.655234: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1926] Device 2 init p2p of link 4
[2022-12-11 21:29:27.655357: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1980] eager alloc mem 611.00 KB
[2022-12-11 21:29:27.655518: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:29:27.656118: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 625663
[2022-12-11 21:29:27.668692: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 8399996
[2022-12-11 21:29:27.668727: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 8399996
[2022-12-11 21:29:27.669432: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 8399996
[2022-12-11 21:29:27.669881: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 8399996
[2022-12-11 21:29:27.669953: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 8399996
[2022-12-11 21:29:27.670615: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 8399996
[2022-12-11 21:29:27.670895: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] eager release cuda mem 8399996
[2022-12-11 21:29:27.671177: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 1999999 / 100000000 nodes ( 2.00 %~2.00 %) | remote 5999997 / 100000000 nodes ( 6.00 %) | cpu 92000004 / 100000000 nodes ( 92.00 %) | 981.44 MB | 0.525141 secs 
[2022-12-11 21:29:27.671239: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/common.cc:638] [eager release cuda mem 83999962022-12-11 21:29:27
.671266: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 1999999 / 100000000 nodes ( 2.00 %~2.00 %) | remote 5999997 / 100000000 nodes ( 6.00 %) | cpu 92000004 / 100000000 nodes ( 92.00 %) | 981.44 MB | 0.528568 secs 
[2022-12-11 21:29:27.671354: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 1999999 / 100000000 nodes ( 2.00 %~2.00 %) | remote 5999997 / 100000000 nodes ( 6.00 %) | cpu 92000004 / 100000000 nodes ( 92.00 %) | 981.44 MB | 0.528593 secs 
[2022-12-11 21:29:27.671759: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 1999999 / 100000000 nodes ( 2.00 %~2.00 %) | remote 5999997 / 100000000 nodes ( 6.00 %) | cpu 92000004 / 100000000 nodes ( 92.00 %) | 981.44 MB | 0.529276 secs 
[2022-12-11 21:29:27.671952: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 1999999 / 100000000 nodes ( 2.00 %~2.00 %) | remote 5999997 / 100000000 nodes ( 6.00 %) | cpu 92000004 / 100000000 nodes ( 92.00 %) | 981.44 MB | 0.526676 secs 
[2022-12-11 21:29:27.672528: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 1999999 / 100000000 nodes ( 2.00 %~2.00 %) | remote 5999997 / 100000000 nodes ( 6.00 %) | cpu 92000004 / 100000000 nodes ( 92.00 %) | 981.44 MB | 0.52713 secs 
[2022-12-11 21:29:27.672613: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 1999999 / 100000000 nodes ( 2.00 %~2.00 %) | remote 5999997 / 100000000 nodes ( 6.00 %) | cpu 92000004 / 100000000 nodes ( 92.00 %) | 981.44 MB | 0.526546 secs 
[2022-12-11 21:29:27.673020: E /hugectr_dev/third_party/collcachelib/coll_cache_lib/cache_context.cu:1955] Asymm Coll cache (policy: clique_part) | local 1999999 / 100000000 nodes ( 2.00 %~2.00 %) | remote 5999997 / 100000000 nodes ( 6.00 %) | cpu 92000004 / 100000000 nodes ( 92.00 %) | 981.44 MB | 0.545733 secs 
[HCTR][21:29:27.673][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier
[HCTR][21:29:27.673][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier
[HCTR][21:29:27.673][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier
[HCTR][21:29:27.673][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier
[HCTR][21:29:27.673][ERROR][RK0][tid #140693733373696]: replica 1 calling init per replica done, doing barrier
[HCTR][21:29:27.673][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier
[HCTR][21:29:27.673][ERROR][RK0][tid #140693062285056]: replica 2 calling init per replica done, doing barrier
[HCTR][21:29:27.673][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier
[HCTR][21:29:27.673][ERROR][RK0][main]: replica 7 calling init per replica done, doing barrier done
[HCTR][21:29:27.673][ERROR][RK0][main]: replica 0 calling init per replica done, doing barrier done
[HCTR][21:29:27.673][ERROR][RK0][main]: replica 6 calling init per replica done, doing barrier done
[HCTR][21:29:27.673][ERROR][RK0][main]: replica 4 calling init per replica done, doing barrier done
[HCTR][21:29:27.673][ERROR][RK0][main]: replica 3 calling init per replica done, doing barrier done
[HCTR][21:29:27.673][ERROR][RK0][tid #140693062285056]: replica 2 calling init per replica done, doing barrier done
[HCTR][21:29:27.673][ERROR][RK0][tid #140693733373696]: replica 1 calling init per replica done, doing barrier done
[HCTR][21:29:27.673][ERROR][RK0][main]: replica 5 calling init per replica done, doing barrier done
[HCTR][21:29:27.673][ERROR][RK0][main]: init per replica done
[HCTR][21:29:27.673][ERROR][RK0][main]: init per replica done
[HCTR][21:29:27.673][ERROR][RK0][main]: init per replica done
[HCTR][21:29:27.673][ERROR][RK0][main]: init per replica done
[HCTR][21:29:27.673][ERROR][RK0][tid #140693062285056]: init per replica done
[HCTR][21:29:27.673][ERROR][RK0][tid #140693733373696]: init per replica done
[HCTR][21:29:27.673][ERROR][RK0][main]: init per replica done
[HCTR][21:29:27.675][ERROR][RK0][main]: init per replica done








