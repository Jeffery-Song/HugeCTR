{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4337359",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/notebooks/dlsw-notebooks/merlin_hugectr_hps-sok-to-dlrm-demo/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# SOK to HPS DLRM Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ac179f",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook demonstrates how to train a DLRM model with SparseOperationKit (SOK) and then make inference with HierarchicalParameterServer(HPS). It is recommended to run [sparse_operation_kit_demo.ipynb](https://github.com/NVIDIA-Merlin/HugeCTR/blob/master/sparse_operation_kit/notebooks/sparse_operation_kit_demo.ipynb) and [hierarchical_parameter_server_demo.ipynb](hierarchical_parameter_server_demo.ipynb) before diving into this notebook.\n",
    "\n",
    "For more details about SOK, please refer to [SOK Documentation](https://nvidia-merlin.github.io/HugeCTR/sparse_operation_kit/master/index.html). For more details about HPS APIs, please refer to [HPS APIs](https://nvidia-merlin.github.io/HugeCTR/master/hierarchical_parameter_server/api/index.html). For more details about HPS, please refer to [HugeCTR Hierarchical Parameter Server (HPS)](https://nvidia-merlin.github.io/HugeCTR/master/hierarchical_parameter_server/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202109ad",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "### Get SOK from NGC\n",
    "\n",
    "Both SOK and HPS Python modules are preinstalled in the 22.09 and later [Merlin TensorFlow Container](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/merlin/containers/merlin-tensorflow): `nvcr.io/nvidia/merlin/merlin-tensorflow:22.09`.\n",
    "\n",
    "You can check the existence of the required libraries by running the following Python code after launching this container.\n",
    "\n",
    "```bash\n",
    "$ python3 -c \"import sparse_operation_kit as sok\"\n",
    "$ python3 -c \"import hierarchical_parameter_server as hps\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c936f7",
   "metadata": {},
   "source": [
    "## Configurations\n",
    "\n",
    "First of all we specify the required configurations, e.g., the arguments needed for generating the dataset, the model parameters and the paths to save the model. We will use DLRM model which has one embedding table, bottom MLP layers, interaction layer and top MLP layers. Please note that the input to the embedding layer will be a sparse key tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "039e7d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: sparse_operation_kit is imported\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.12) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import sparse_operation_kit as sok\n",
    "import sys\n",
    "sys.path.append(\"/hugectr/sparse_operation_kit/unit_test/test_scripts/tf2/\")\n",
    "import utils\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import struct\n",
    "import yaml\n",
    "\n",
    "args = dict()\n",
    "\n",
    "args[\"gpu_num\"] = 4                               # the number of available GPUs\n",
    "args[\"iter_num\"] = 10                             # the number of training iteration\n",
    "args[\"slot_num\"] = 26                             # the number of feature fields in this embedding layer\n",
    "args[\"embed_vec_size\"] = 16                       # the dimension of embedding vectors\n",
    "args[\"dense_dim\"] = 13                            # the dimension of dense features\n",
    "args[\"global_batch_size\"] = 1024                  # the globally batchsize for all GPUs\n",
    "args[\"max_vocabulary_size\"] = 260000\n",
    "# args[\"vocabulary_range_per_slot\"] = [[i*10000, (i+1)*10000] for i in range(26)] \n",
    "args[\"max_nnz\"] = 10                # the max number of non-zeros for all slots\n",
    "args[\"combiner\"] = \"mean\"\n",
    "\n",
    "args[\"ps_config_file\"] = \"dlrm.json\"\n",
    "args[\"dense_model_path\"] = \"dlrm_dense.model\"\n",
    "args[\"embedding_table_path\"] = \"dlrm_sparse.model\"\n",
    "args[\"saved_path\"] = \"dlrm_tf_saved_model\"\n",
    "args[\"np_key_type\"] = np.int64\n",
    "args[\"np_vector_type\"] = np.float32\n",
    "args[\"tf_key_type\"] = tf.int64\n",
    "args[\"tf_vector_type\"] = tf.float32\n",
    "args[\"optimizer\"] = \"plugin_adam\"\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join(map(str, range(args[\"gpu_num\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f19157f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 39884406], [39884406, 39923449], [39923449, 39940738], [39940738, 39948158], [39948158, 39968421], [39968421, 39968424], [39968424, 39975544], [39975544, 39977087], [39977087, 39977150], [39977150, 78510101], [78510101, 81463647], [81463647, 81866993], [81866993, 81867003], [81867003, 81869211], [81869211, 81881149], [81881149, 81881304], [81881304, 81881308], [81881308, 81882284], [81882284, 81882298], [81882298, 121862069], [121862069, 147503364], [147503364, 187168348], [187168348, 187754283], [187754283, 187767255], [187767255, 187767363], [187767363, 187767399]]\n",
      "[[0, 10000], [10000, 20000], [20000, 30000], [30000, 40000], [40000, 50000], [50000, 60000], [60000, 70000], [70000, 80000], [80000, 90000], [90000, 100000], [100000, 110000], [110000, 120000], [120000, 130000], [130000, 140000], [140000, 150000], [150000, 160000], [160000, 170000], [170000, 180000], [180000, 190000], [190000, 200000], [200000, 210000], [210000, 220000], [220000, 230000], [230000, 240000], [240000, 250000], [250000, 260000]]\n"
     ]
    }
   ],
   "source": [
    "# load vocabulary_range_per_slot from yaml file\n",
    "file = open('criteo_full.yaml', 'r', encoding=\"utf-8\")\n",
    "file_data = file.read()\n",
    "file.close()\n",
    "feature_spec = yaml.load(file_data, yaml.Loader)['feature_spec']\n",
    "\n",
    "ranges = [[0, 0] for i in range(26)]\n",
    "max_range = 0\n",
    "for i in range(26):\n",
    "    feature_cat = feature_spec['cat_' + str(i) + '.bin']['cardinality']\n",
    "    ranges[i][0] = max_range\n",
    "    ranges[i][1] = max_range + feature_cat\n",
    "    max_range += feature_cat\n",
    "print(ranges)\n",
    "print([[i*10000, (i+1)*10000] for i in range(26)])\n",
    "args[\"vocabulary_range_per_slot\"] = ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cc6730a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_samples(num_samples, vocabulary_range_per_slot, max_nnz, dense_dim):\n",
    "    def generate_sparse_keys(num_samples, vocabulary_range_per_slot, max_nnz, key_dtype = args[\"np_key_type\"]):\n",
    "        slot_num = len(vocabulary_range_per_slot)\n",
    "        indices = []\n",
    "        values = []\n",
    "        for i in range(num_samples):\n",
    "            for j in range(slot_num):\n",
    "                vocab_range = vocabulary_range_per_slot[j]\n",
    "                nnz = np.random.randint(low=1, high=max_nnz+1)\n",
    "                entries = sorted(np.random.choice(max_nnz, nnz, replace=False))\n",
    "                for entry in entries:\n",
    "                    indices.append([i, j, entry])\n",
    "                values.extend(np.random.randint(low=vocab_range[0], high=vocab_range[1], size=(nnz, )))\n",
    "        values = np.array(values, dtype=key_dtype)\n",
    "        return tf.sparse.SparseTensor(indices = indices,\n",
    "                                    values = values,\n",
    "                                    dense_shape = (num_samples, slot_num, max_nnz))\n",
    "\n",
    "    \n",
    "    sparse_keys = generate_sparse_keys(num_samples, vocabulary_range_per_slot, max_nnz)\n",
    "    dense_features = np.random.random((num_samples, dense_dim)).astype(np.float32)\n",
    "    labels = np.random.randint(low=0, high=2, size=(num_samples, 1))\n",
    "    return sparse_keys, dense_features, labels\n",
    "\n",
    "def tf_dataset(sparse_keys, dense_features, labels, batchsize):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((sparse_keys, dense_features, labels))\n",
    "    dataset = dataset.batch(batchsize, drop_remainder=True)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe74fa65",
   "metadata": {},
   "source": [
    "## Train with SOK embedding layers\n",
    "\n",
    "We define the model graph for training with SOK embedding layers, i.e., `sok.DistributedEmbedding`. We can then train the model and save the trained weights of the embedding table into the formats required by HPS. As for the dense layers, they are saved as a separate model graph, which can be loaded directly during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1f685fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(tf.keras.layers.Layer):\n",
    "    def __init__(self,\n",
    "                arch,\n",
    "                activation='relu',\n",
    "                out_activation=None,\n",
    "                **kwargs):\n",
    "        super(MLP, self).__init__(**kwargs)\n",
    "        self.layers = []\n",
    "        index = 0\n",
    "        for units in arch[:-1]:\n",
    "            self.layers.append(tf.keras.layers.Dense(units, activation=activation, name=\"{}_{}\".format(kwargs['name'], index)))\n",
    "            index+=1\n",
    "        self.layers.append(tf.keras.layers.Dense(arch[-1], activation=out_activation, name=\"{}_{}\".format(kwargs['name'], index)))\n",
    "\n",
    "            \n",
    "    def call(self, inputs, training=True):\n",
    "        x = self.layers[0](inputs)\n",
    "        for layer in self.layers[1:]:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "class SecondOrderFeatureInteraction(tf.keras.layers.Layer):\n",
    "    def __init__(self, self_interaction=False):\n",
    "        super(SecondOrderFeatureInteraction, self).__init__()\n",
    "        self.self_interaction = self_interaction\n",
    "\n",
    "    def call(self, inputs):\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        num_feas = tf.shape(inputs)[1]\n",
    "\n",
    "        dot_products = tf.matmul(inputs, inputs, transpose_b=True)\n",
    "\n",
    "        ones = tf.ones_like(dot_products)\n",
    "        mask = tf.linalg.band_part(ones, 0, -1)\n",
    "        out_dim = num_feas * (num_feas + 1) // 2\n",
    "\n",
    "        if not self.self_interaction:\n",
    "            mask = mask - tf.linalg.band_part(ones, 0, 0)\n",
    "            out_dim = num_feas * (num_feas - 1) // 2\n",
    "        flat_interactions = tf.reshape(tf.boolean_mask(dot_products, mask), (batch_size, out_dim))\n",
    "        return flat_interactions\n",
    "\n",
    "class DLRM(tf.keras.models.Model):\n",
    "    def __init__(self,\n",
    "                 combiner,\n",
    "                 max_vocabulary_size_per_gpu,\n",
    "                 embed_vec_size,\n",
    "                 slot_num,\n",
    "                 max_nnz,\n",
    "                 dense_dim,\n",
    "                 arch_bot,\n",
    "                 arch_top,\n",
    "                 self_interaction,\n",
    "                 **kwargs):\n",
    "        super(DLRM, self).__init__(**kwargs)\n",
    "        \n",
    "        self.combiner = combiner\n",
    "        self.max_vocabulary_size_per_gpu = max_vocabulary_size_per_gpu\n",
    "        self.embed_vec_size = embed_vec_size\n",
    "        self.slot_num = slot_num\n",
    "        self.max_nnz = max_nnz\n",
    "        self.dense_dim = dense_dim\n",
    "        \n",
    "        self.embedding_layer = sok.DistributedEmbedding(combiner=self.combiner,\n",
    "                                                        max_vocabulary_size_per_gpu=self.max_vocabulary_size_per_gpu,\n",
    "                                                        embedding_vec_size=self.embed_vec_size,\n",
    "                                                        slot_num=self.slot_num,\n",
    "                                                        max_nnz=self.max_nnz)\n",
    "        self.bot_nn = MLP(arch_bot, name = \"bottom\", out_activation='relu')\n",
    "        self.top_nn = MLP(arch_top, name = \"top\", out_activation='sigmoid')\n",
    "        self.interaction_op = SecondOrderFeatureInteraction(self_interaction)\n",
    "        if self_interaction:\n",
    "            self.interaction_out_dim = (self.slot_num+1) * (self.slot_num+2) // 2\n",
    "        else:\n",
    "            self.interaction_out_dim = self.slot_num * (self.slot_num+1) // 2\n",
    "        self.reshape_layer1 = tf.keras.layers.Reshape((1, arch_bot[-1]), name = \"reshape1\")\n",
    "        self.concat1 = tf.keras.layers.Concatenate(axis=1, name = \"concat1\")\n",
    "        self.concat2 = tf.keras.layers.Concatenate(axis=1, name = \"concat2\")\n",
    "            \n",
    "    def call(self, inputs, training=True):\n",
    "        input_cat = inputs[0]\n",
    "        input_dense = inputs[1]\n",
    "        \n",
    "        embedding_vector = self.embedding_layer(input_cat, training=training)\n",
    "        dense_x = self.bot_nn(input_dense)\n",
    "        concat_features = self.concat1([embedding_vector, self.reshape_layer1(dense_x)])\n",
    "        \n",
    "        Z = self.interaction_op(concat_features)\n",
    "        z = self.concat2([dense_x, Z])\n",
    "        logit = self.top_nn(z)\n",
    "        return logit, embedding_vector\n",
    "\n",
    "    def summary(self):\n",
    "        inputs = [tf.keras.Input(shape=(self.max_nnz, ), sparse=True, dtype=args[\"tf_key_type\"]), \n",
    "                  tf.keras.Input(shape=(self.dense_dim, ), dtype=tf.float32)]\n",
    "        model = tf.keras.models.Model(inputs=inputs, outputs=self.call(inputs))\n",
    "        return model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4b516ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args):\n",
    "    dlrm = DLRM(combiner = \"mean\", \n",
    "                max_vocabulary_size_per_gpu = args[\"max_vocabulary_size\"] // args[\"gpu_num\"],\n",
    "                embed_vec_size = args[\"embed_vec_size\"],\n",
    "                slot_num = args[\"slot_num\"],\n",
    "                max_nnz = args[\"max_nnz\"],\n",
    "                dense_dim = args[\"dense_dim\"],\n",
    "                arch_bot = [256, 128, args[\"embed_vec_size\"]],\n",
    "                arch_top = [256, 128, 1],\n",
    "                self_interaction = False)\n",
    "\n",
    "    emb_opt = utils.get_embedding_optimizer(args[\"optimizer\"])(learning_rate=0.1)\n",
    "    dense_opt = utils.get_dense_optimizer(args[\"optimizer\"])(learning_rate=0.1)\n",
    "\n",
    "    init_tensors = np.ones(shape=[args[\"max_vocabulary_size\"], args[\"embed_vec_size\"]], dtype=args[\"np_vector_type\"])\n",
    "    embedding_saver = sok.Saver()\n",
    "    embedding_saver.load_embedding_values(dlrm.embedding_layer.embedding_variable, init_tensors)\n",
    "\n",
    "    loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "    @tf.function\n",
    "    def _train_step(inputs, labels):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logit, embedding_vector = dlrm(inputs, training=True)\n",
    "            loss = loss_fn(labels, logit)\n",
    "        embedding_variables, other_variable = sok.split_embedding_variable_from_others(dlrm.trainable_variables)\n",
    "        grads, emb_grads = tape.gradient(loss, [other_variable, embedding_variables])\n",
    "        if 'plugin' not in args[\"optimizer\"]:\n",
    "            with sok.OptimizerScope(embedding_variables):\n",
    "                emb_opt.apply_gradients(zip(emb_grads, embedding_variables),\n",
    "                                        experimental_aggregate_gradients=False)\n",
    "        else:\n",
    "            emb_opt.apply_gradients(zip(emb_grads, embedding_variables),\n",
    "                                    experimental_aggregate_gradients=False)\n",
    "        dense_opt.apply_gradients(zip(grads, other_variable))\n",
    "        return logit, embedding_vector, loss\n",
    "\n",
    "    sparse_keys, dense_features, labels = generate_random_samples(args[\"global_batch_size\"]  * args[\"iter_num\"], args[\"vocabulary_range_per_slot\"], args[\"max_nnz\"], args[\"dense_dim\"])\n",
    "    dataset = tf_dataset(sparse_keys, dense_features, labels, args[\"global_batch_size\"])\n",
    "    for i, (sparse_keys, dense_features, labels) in enumerate(dataset):\n",
    "        sparse_keys = tf.sparse.reshape(sparse_keys, [-1, sparse_keys.shape[-1]])\n",
    "        inputs = [sparse_keys, dense_features]\n",
    "        logit, embedding_vector, loss = _train_step(inputs, labels)\n",
    "        print(\"-\"*20, \"Step {}, loss: {}\".format(i, loss),  \"-\"*20)\n",
    "    return dlrm, embedding_saver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562fb969",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-09 15:16:54.858227: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-09 15:16:57.210554: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-10-09 15:16:57.210639: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14538 MB memory:  -> device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:18:00.0, compute capability: 7.0\n",
      "2022-10-09 15:16:57.212445: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-10-09 15:16:57.212482: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 14628 MB memory:  -> device: 1, name: Tesla V100-SXM2-16GB, pci bus id: 0000:3b:00.0, compute capability: 7.0\n",
      "2022-10-09 15:16:57.214150: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-10-09 15:16:57.214181: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 14624 MB memory:  -> device: 2, name: Tesla V100-SXM2-16GB, pci bus id: 0000:86:00.0, compute capability: 7.0\n",
      "2022-10-09 15:16:57.215815: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-10-09 15:16:57.215846: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 14522 MB memory:  -> device: 3, name: Tesla V100-SXM2-16GB, pci bus id: 0000:af:00.0, compute capability: 7.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-10-09 15:16:57.328617: I sparse_operation_kit/kit_cc/kit_cc_infra/src/resources/manager.cc:107] Mapping from local_replica_id to device_id:\n",
      "2022-10-09 15:16:57.328617: I sparse_operation_kit/kit_cc/kit_cc_infra/src/resources/manager.cc:109] 0 -> 0\n",
      "2022-10-09 15:16:57.328617: I sparse_operation_kit/kit_cc/kit_cc_infra/src/resources/manager.cc:109] 1 -> 1\n",
      "2022-10-09 15:16:57.328617: I sparse_operation_kit/kit_cc/kit_cc_infra/src/resources/manager.cc:109] 2 -> 2\n",
      "2022-10-09 15:16:57.328617: I sparse_operation_kit/kit_cc/kit_cc_infra/src/resources/manager.cc:109] 3 -> 3\n",
      "2022-10-09 15:16:57.328617: I sparse_operation_kit/kit_cc/kit_cc_infra/src/resources/manager.cc:84] Global seed is 229825253\n",
      "2022-10-09 15:16:57.328617: I sparse_operation_kit/kit_cc/kit_cc_infra/src/resources/manager.cc:85] Local GPU Count: 4\n",
      "2022-10-09 15:16:57.328617: I sparse_operation_kit/kit_cc/kit_cc_infra/src/resources/manager.cc:86] Global GPU Count: 1\n"
     ]
    }
   ],
   "source": [
    "sok.Init(global_batch_size=args[\"global_batch_size\"])\n",
    "trained_model, embedding_saver = train(args)\n",
    "trained_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38b0053",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_model = tf.keras.Model([trained_model.get_layer(\"distributed_embedding\").output,\n",
    "                             trained_model.get_layer(\"bottom\").input],\n",
    "                             trained_model.get_layer(\"top\").output)\n",
    "dense_model.summary()\n",
    "dense_model.save(args[\"dense_model_path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "285201f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-10-09 07:06:21.299181: I sparse_operation_kit/kit_cc/kit_cc_infra/src/parameters/raw_manager.cc:192] Saving EmbeddingVariable to dlrm_sparse.model..\n",
      "2022-10-09 07:06:21.299181: I sparse_operation_kit/kit_cc_impl/embedding/common/src/dumping_functions.cc:60] Worker: 0, GPU: 0 key-index count = 260000\n",
      "2022-10-09 07:06:21.299181: I sparse_operation_kit/kit_cc_impl/embedding/common/src/dumping_functions.cc:147] Worker: 0, GPU: 0: dumping parameters from hashtable..\n",
      "2022-10-09 07:06:21.299181: I sparse_operation_kit/kit_cc/kit_cc_infra/src/parameters/raw_manager.cc:200] Saved EmbeddingVariable to dlrm_sparse.model.\n",
      "total 18284\n",
      "-rw-r--r-- 1 root root 16640000 Oct  9 07:06 emb_vector\n",
      "-rw-r--r-- 1 root root  2080000 Oct  9 07:06 key\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p dlrm_sparse.model\n",
    "embedding_saver.dump_to_file(trained_model.embedding_layer.embedding_variable, args[\"embedding_table_path\"])\n",
    "!mv dlrm_sparse.model/EmbeddingVariable_keys.file dlrm_sparse.model/key\n",
    "!mv dlrm_sparse.model/EmbeddingVariable_values.file dlrm_sparse.model/emb_vector\n",
    "!ls -l dlrm_sparse.model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0712a55f",
   "metadata": {},
   "source": [
    "## Create the inference graph with HPS SparseLookupLayer\n",
    "In order to use HPS in the inference stage, we need to create a inference model graph which is almost the same as the train graph except that `sok.DistributedEmbedding` is replaced by `hps.SparseLookupLayer`. The trained dense model graph can be loaded directly, while the weights of the embedding table can be retrieved by HPS from the folder `dlrm_sparse.model`.\n",
    "\n",
    "We can then save the inference model graph, which will be ready to be loaded for inference deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9cd0042c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] hierarchical_parameter_server is imported\n"
     ]
    }
   ],
   "source": [
    "import hierarchical_parameter_server as hps\n",
    "\n",
    "class InferenceModel(tf.keras.models.Model):\n",
    "    def __init__(self,\n",
    "                 slot_num,\n",
    "                 embed_vec_size,\n",
    "                 max_nnz,\n",
    "                 dense_dim,\n",
    "                 dense_model_path,\n",
    "                 **kwargs):\n",
    "        super(InferenceModel, self).__init__(**kwargs)\n",
    "        \n",
    "        self.slot_num = slot_num\n",
    "        self.embed_vec_size = embed_vec_size\n",
    "        self.max_nnz = max_nnz\n",
    "        self.dense_dim = dense_dim\n",
    "        \n",
    "        self.sparse_lookup_layer = hps.SparseLookupLayer(model_name = \"dlrm\", \n",
    "                                            table_id = 0,\n",
    "                                            emb_vec_size = self.embed_vec_size,\n",
    "                                            emb_vec_dtype = args[\"tf_vector_type\"])\n",
    "        self.dense_model = tf.keras.models.load_model(dense_model_path, compile=False)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        input_cat = inputs[0]\n",
    "        input_dense = inputs[1]\n",
    "\n",
    "        embeddings = tf.reshape(self.sparse_lookup_layer(sp_ids=input_cat, sp_weights = None, combiner=\"mean\"),\n",
    "                                shape=[-1, self.slot_num, self.embed_vec_size])\n",
    "        logit = self.dense_model([embeddings, input_dense])\n",
    "        return logit, embeddings\n",
    "\n",
    "    def summary(self):\n",
    "        inputs = [tf.keras.Input(shape=(self.max_nnz, ), sparse=True, dtype=args[\"tf_key_type\"]), \n",
    "                  tf.keras.Input(shape=(self.dense_dim, ), dtype=tf.float32)]\n",
    "        model = tf.keras.models.Model(inputs=inputs, outputs=self.call(inputs))\n",
    "        return model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6209c43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_save_inference_graph(args): \n",
    "    model = InferenceModel(args[\"slot_num\"], args[\"embed_vec_size\"], args[\"max_nnz\"], args[\"dense_dim\"], args[\"dense_model_path\"])\n",
    "    model.summary()\n",
    "    inputs = [tf.keras.Input(shape=(args[\"max_nnz\"], ), sparse=True, dtype=args[\"tf_key_type\"]), \n",
    "              tf.keras.Input(shape=(args[\"dense_dim\"], ), dtype=tf.float32)]\n",
    "    _, _ = model(inputs)\n",
    "    model.save(args[\"saved_path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97783f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_4 (InputLayer)           [(None, 10)]         0           []                               \n",
      "                                                                                                  \n",
      " sparse_lookup_layer (SparseLoo  (None, 16)          0           ['input_4[0][0]']                \n",
      " kupLayer)                                                                                        \n",
      "                                                                                                  \n",
      " tf.reshape (TFOpLambda)        (None, 26, 16)       0           ['sparse_lookup_layer[0][0]']    \n",
      "                                                                                                  \n",
      " input_5 (InputLayer)           [(None, 13)]         0           []                               \n",
      "                                                                                                  \n",
      " model_1 (Functional)           (None, 1)            165777      ['tf.reshape[0][0]',             \n",
      "                                                                  'input_5[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 165,777\n",
      "Trainable params: 165,777\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) args_0 with unsupported characters which will be renamed to args_0_3 in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as bottom_0_layer_call_fn, bottom_0_layer_call_and_return_conditional_losses, bottom_1_layer_call_fn, bottom_1_layer_call_and_return_conditional_losses, bottom_2_layer_call_fn while saving (showing 5 of 12). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: dlrm_tf_saved_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: dlrm_tf_saved_model/assets\n"
     ]
    }
   ],
   "source": [
    "create_and_save_inference_graph(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b49dcd",
   "metadata": {},
   "source": [
    "## Inference with saved model graph\n",
    "\n",
    "In order to initialize the lookup service provided by HPS, we also need to create a JSON configuration file and specify the details of the embedding tables for the models to be deployed. We deploy the DLRM model that has one embedding table here, and it can support multiple models with multiple embedding tables actually. Please note how `maxnum_catfeature_query_per_table_per_sample` is specified for the embedding table: the `max_nnz` is 10 for all the slots and there are 26 slots, so this entry is configured as 260.\n",
    "\n",
    "We first call `hps.Init` to do the necessary initialization work, and then load the saved model graph to make inference. We peek at the keys and the embedding vectors for each table for the last inference batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fbe72621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting dlrm.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile dlrm.json\n",
    "{\n",
    "    \"supportlonglong\": true,\n",
    "    \"models\": [{\n",
    "        \"model\": \"dlrm\",\n",
    "        \"sparse_files\": [\"dlrm_sparse.model\"],\n",
    "        \"num_of_worker_buffer_in_pool\": 3,\n",
    "        \"embedding_table_names\":[\"sparse_embedding0\"],\n",
    "        \"embedding_vecsize_per_table\": [16],\n",
    "        \"maxnum_catfeature_query_per_table_per_sample\": [260],\n",
    "        \"default_value_for_each_table\": [1.0],\n",
    "        \"deployed_device_list\": [0],\n",
    "        \"max_batch_size\": 1024,\n",
    "        \"cache_refresh_percentage_per_iteration\": 0.2,\n",
    "        \"hit_rate_threshold\": 1.0,\n",
    "        \"gpucacheper\": 1.0,\n",
    "        \"gpucache\": true\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ff25901",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_with_saved_model(args):\n",
    "    hps.Init(global_batch_size = args[\"global_batch_size\"],\n",
    "             ps_config_file = args[\"ps_config_file\"])\n",
    "    model = tf.keras.models.load_model(args[\"saved_path\"])\n",
    "    model.summary()\n",
    "    def _infer_step(inputs, labels):\n",
    "        logit, embeddings = model(inputs)\n",
    "        return logit, embeddings\n",
    "    \n",
    "    embeddings_peek = list()\n",
    "    inputs_peek = list()\n",
    "    \n",
    "    sparse_keys, dense_features, labels = generate_random_samples(args[\"global_batch_size\"]  * args[\"iter_num\"], args[\"vocabulary_range_per_slot\"], args[\"max_nnz\"], args[\"dense_dim\"])\n",
    "    dataset = tf_dataset(sparse_keys, dense_features, labels, args[\"global_batch_size\"])\n",
    "    for i, (sparse_keys, dense_features, labels) in enumerate(dataset):\n",
    "#         print(sparse_keys)\n",
    "#         print(dense_features)\n",
    "#         print(labels)\n",
    "        sparse_keys = tf.sparse.reshape(sparse_keys, [-1, sparse_keys.shape[-1]])\n",
    "        inputs = [sparse_keys, dense_features]\n",
    "        logit, embeddings = _infer_step(inputs, labels)\n",
    "        embeddings_peek.append(embeddings)\n",
    "        inputs_peek.append(inputs)\n",
    "        print(\"-\"*20, \"Step {}\".format(i),  \"-\"*20)\n",
    "    return embeddings_peek, inputs_peek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83377160",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================HPS Parse====================================================\n",
      "[HCTR][07:12:13.763][INFO][RK0][main]: dense_file is not specified using default: \n",
      "[HCTR][07:12:13.763][INFO][RK0][main]: num_of_refresher_buffer_in_pool is not specified using default: 1\n",
      "[HCTR][07:12:13.763][INFO][RK0][main]: maxnum_des_feature_per_sample is not specified using default: 26\n",
      "[HCTR][07:12:13.763][INFO][RK0][main]: refresh_delay is not specified using default: 0\n",
      "[HCTR][07:12:13.763][INFO][RK0][main]: refresh_interval is not specified using default: 0\n",
      "====================================================HPS Create====================================================\n",
      "[HCTR][07:12:13.763][INFO][RK0][main]: Creating HashMap CPU database backend...\n",
      "[HCTR][07:12:13.763][DEBUG][RK0][main]: Created blank database backend in local memory!\n",
      "[HCTR][07:12:13.763][INFO][RK0][main]: Volatile DB: initial cache rate = 1\n",
      "[HCTR][07:12:13.763][INFO][RK0][main]: Volatile DB: cache missed embeddings = 0\n",
      "[HCTR][07:12:13.763][DEBUG][RK0][main]: Created raw model loader in local memory!\n",
      "[HCTR][07:12:13.992][INFO][RK0][main]: Table: hps_et.dlrm.sparse_embedding0; cached 260000 / 260000 embeddings in volatile database (HashMapBackend); load: 260000 / 18446744073709551615 (0.00%).\n",
      "[HCTR][07:12:13.992][DEBUG][RK0][main]: Real-time subscribers created!\n",
      "[HCTR][07:12:13.992][INFO][RK0][main]: Creating embedding cache in device 0.\n",
      "[HCTR][07:12:14.033][INFO][RK0][main]: Model name: dlrm\n",
      "[HCTR][07:12:14.033][INFO][RK0][main]: Number of embedding tables: 1\n",
      "[HCTR][07:12:14.033][INFO][RK0][main]: Use GPU embedding cache: True, cache size percentage: 1.000000\n",
      "[HCTR][07:12:14.033][INFO][RK0][main]: Use I64 input key: True\n",
      "[HCTR][07:12:14.033][INFO][RK0][main]: Configured cache hit rate threshold: 1.000000\n",
      "[HCTR][07:12:14.033][INFO][RK0][main]: The size of thread pool: 80\n",
      "[HCTR][07:12:14.033][INFO][RK0][main]: The size of worker memory pool: 3\n",
      "[HCTR][07:12:14.033][INFO][RK0][main]: The size of refresh memory pool: 1\n",
      "[HCTR][07:12:14.033][INFO][RK0][main]: The refresh percentage : 0.200000\n",
      "[HCTR][07:12:14.069][DEBUG][RK0][main]: Created raw model loader in local memory!\n",
      "[HCTR][07:12:14.077][INFO][RK0][main]: EC initialization for model: \"dlrm\", num_tables: 1\n",
      "[HCTR][07:12:14.078][INFO][RK0][main]: EC initialization on device: 0\n",
      "[HCTR][07:12:14.082][INFO][RK0][main]: Creating lookup session for dlrm on device: 0\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"inference_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " sparse_lookup_layer (Sparse  multiple                 0         \n",
      " LookupLayer)                                                    \n",
      "                                                                 \n",
      " model_1 (Functional)        (None, 1)                 165777    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 165,777\n",
      "Trainable params: 165,777\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "-------------------- Step 0 --------------------\n",
      "-------------------- Step 1 --------------------\n",
      "-------------------- Step 2 --------------------\n",
      "-------------------- Step 3 --------------------\n",
      "-------------------- Step 4 --------------------\n",
      "-------------------- Step 5 --------------------\n",
      "-------------------- Step 6 --------------------\n",
      "-------------------- Step 7 --------------------\n",
      "-------------------- Step 8 --------------------\n",
      "-------------------- Step 9 --------------------\n",
      "tf.Tensor([  9334   6739   8476 ... 256474 255693 253575], shape=(146484,), dtype=int64)\n",
      "tf.Tensor(\n",
      "[[[1.0303558  1.0303864  1.0303241  ... 1.0303041  1.0302438  1.0303174 ]\n",
      "  [0.6030799  0.60340446 0.60464424 ... 0.60118407 0.6096928  0.6030893 ]\n",
      "  [0.65922654 0.6585769  0.65791893 ... 0.659387   0.65735656 0.6587229 ]\n",
      "  ...\n",
      "  [0.68700653 0.68678975 0.68679315 ... 0.68705714 0.6859454  0.6867702 ]\n",
      "  [0.64445025 0.64428926 0.64457405 ... 0.6445771  0.64393765 0.64443654]\n",
      "  [0.6858526  0.68585634 0.68585575 ... 0.6858479  0.68586445 0.6858535 ]]\n",
      "\n",
      " [[0.8697153  0.86971337 0.8697013  ... 0.86969024 0.8697314  0.8697028 ]\n",
      "  [0.66645104 0.66888124 0.6700823  ... 0.6663506  0.6773426  0.6690721 ]\n",
      "  [0.75879836 0.75869125 0.7591771  ... 0.75898564 0.75854594 0.75893027]\n",
      "  ...\n",
      "  [0.5872361  0.5891948  0.59225774 ... 0.5872379  0.60974115 0.58926153]\n",
      "  [0.79607904 0.79606605 0.7960712  ... 0.79607975 0.796039   0.7960713 ]\n",
      "  [0.77932304 0.77895314 0.7787221  ... 0.77959496 0.774297   0.7785742 ]]\n",
      "\n",
      " [[0.766498   0.7661567  0.7662255  ... 0.7665265  0.7654758  0.76635045]\n",
      "  [0.72306126 0.7228346  0.7228236  ... 0.7230918  0.7226055  0.7229267 ]\n",
      "  [0.7213389  0.7213372  0.7213396  ... 0.7213396  0.7213414  0.7213387 ]\n",
      "  ...\n",
      "  [0.7209428  0.7210231  0.72109556 ... 0.7205387  0.72159827 0.72121716]\n",
      "  [0.72127426 0.72094184 0.72113645 ... 0.7213587  0.720437   0.7208954 ]\n",
      "  [0.6604779  0.6632467  0.66293377 ... 0.6604781  0.6678379  0.6616087 ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.70262486 0.70254624 0.7025745  ... 0.70262873 0.7025616  0.70260894]\n",
      "  [0.6704369  0.6705681  0.6715333  ... 0.6704384  0.6768364  0.6722785 ]\n",
      "  [0.7060472  0.7053683  0.7038146  ... 0.7067513  0.70339286 0.70589864]\n",
      "  ...\n",
      "  [0.7683074  0.7684458  0.7686661  ... 0.7683065  0.7698299  0.76844174]\n",
      "  [0.62969255 0.6291788  0.62944484 ... 0.62969345 0.628466   0.62923825]\n",
      "  [0.69377935 0.693787   0.69378436 ... 0.69377863 0.69379437 0.6937825 ]]\n",
      "\n",
      " [[0.7071889  0.70718247 0.7071691  ... 0.707153   0.70721996 0.70717126]\n",
      "  [0.69182056 0.692414   0.6913541  ... 0.6907724  0.6924854  0.6909436 ]\n",
      "  [0.54575616 0.545508   0.5462191  ... 0.5462434  0.5447614  0.5458104 ]\n",
      "  ...\n",
      "  [0.7455766  0.74550116 0.7454029  ... 0.7455754  0.7452085  0.74555004]\n",
      "  [0.6438957  0.64384174 0.6440111  ... 0.6437413  0.6436827  0.6438156 ]\n",
      "  [0.93420154 0.9281132  0.9348466  ... 0.9435567  0.8971756  0.93920296]]\n",
      "\n",
      " [[0.7082391  0.7082191  0.70823234 ... 0.70824397 0.7081414  0.70822024]\n",
      "  [0.626692   0.6278864  0.6267243  ... 0.6266403  0.6295388  0.6271619 ]\n",
      "  [0.66701114 0.6670188  0.6670102  ... 0.6670062  0.6670335  0.66701126]\n",
      "  ...\n",
      "  [0.5505822  0.5488095  0.5497763  ... 0.55103326 0.54736215 0.5503158 ]\n",
      "  [0.9100596  0.91305864 0.9134315  ... 0.9100593  0.9256331  0.9136089 ]\n",
      "  [0.8167601  0.8159869  0.8160305  ... 0.8167762  0.81549317 0.8162966 ]]], shape=(1024, 26, 16), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "embeddings_peek, inputs_peek = inference_with_saved_model(args)\n",
    "\n",
    "# embedding table, input keys are SparseTensor \n",
    "print(inputs_peek[-1][0].values)\n",
    "print(embeddings_peek[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ad23aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
